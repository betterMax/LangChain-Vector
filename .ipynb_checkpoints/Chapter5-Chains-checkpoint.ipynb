{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36cf86f-dcf1-49a0-b363-3ca5c784c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cbb96a-8d1d-4c7c-aca9-0211b8ce3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前windows\n",
    "# 获取当前的 Conda 环境路径\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "\n",
    "# \".env\" 文件的绝对路径\n",
    "dotenv_path = os.path.join(conda_env_path, '.env')\n",
    "\n",
    "# 加载 \".env\" 文件\n",
    "_ = load_dotenv(dotenv_path, verbose=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e10f80-4951-4027-9f8c-87612384a8db",
   "metadata": {},
   "source": [
    "# Introduction to Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cf509-b0a1-4574-a31c-8d43faa07f1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Chains and Why They Are Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282491a-3ed3-4d3b-81a7-c801f3e83756",
   "metadata": {},
   "source": [
    "The chains are responsible for creating an end-to-end pipeline for using the language models. They will join the model, prompt, memory, parsing output, and debugging capability and provide an easy-to-use interface. A chain will 1) receive the user’s query as an input, 2) process the LLM’s response, and lastly, 3) return the output to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8af2a-7f82-4b28-aedc-1f98789513ca",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c40763c-2649-4c79-a9ff-38e5501e58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762300af-1082-46fa-87fd-357520760874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42489e86-2afa-4d17-80f8-8b00c5b83fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n\\nSynthetic'}, {'text': '\\n\\nWisdom'}, {'text': '\\n\\nAutomaton'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89142dcc-277f-42d9-9437-217b013cba9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWisdom', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAutomaton', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 46, 'completion_tokens': 13, 'prompt_tokens': 33}, 'model_name': 'text-davinci-003'}, run=RunInfo(run_id=UUID('6e8e83d4-5352-429f-b390-42bd0d139643')))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96926ae-f8a4-403f-8079-907981b95ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nventilator'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Looking at the context of '{context}'. What is an appropriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"fan\", context=\"object\")\n",
    "# or llm_chain.run(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b136edcd-16ad-41e0-9fd6-18ffece9d70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAdmirer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"humans\")\n",
    "# or llm_chain.run(word=\"fan\", context=\"humans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d7f73-dcc9-4f83-ab75-855feb193cad",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c14e041-0176-4352-9cc5-651621fc7a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic',\n",
       " 'Manufactured',\n",
       " 'Imitation',\n",
       " 'Fabricated',\n",
       " 'Fake',\n",
       " 'Simulated',\n",
       " 'Artificial Intelligence',\n",
       " 'Automated',\n",
       " 'Constructed',\n",
       " 'Programmed',\n",
       " 'Mechanical',\n",
       " 'Processed',\n",
       " 'Algorithmic',\n",
       " 'Generated.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, output_parser=output_parser, input_variables=[]),\n",
    "    output_parser=output_parser)\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d82a8c-cfc2-4496-9dac-c76c02018cc6",
   "metadata": {},
   "source": [
    "## Conversational Chain (Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce778b5b-7b9a-4905-b329-313b355f72b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Synthetic, robotic, manufactured, simulated, computerized, programmed, man-made, fabricated, contrived, and artificial.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"List all possible words as substitute for 'artificial' as comma separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaaa7cf1-5d25-4d68-af5a-a007f806d4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Automated, cybernetic, mechanized, and engineered.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17347a80-d003-49a7-976a-33c0cfc97543",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20b9ca-6fca-49b9-9b2e-0bfccb3b5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b1693-7ccf-4ccf-89e1-00879abeca43",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0487d246-894b-4aaf-9912-7eb02d9ad07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Synthetic, Imitation, Manufactured, Fabricated, Simulated, Fake, Artificial, Constructed, Computerized, Programmed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29ccd1-7bc7-45c8-9e99-b215c81cc089",
   "metadata": {},
   "source": [
    "## Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f24fca7c-011f-4ec0-8453-fb436dc22e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db90831-2317-4cae-abcd-27b4fe2a3fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Artificial means something that is not natural or made by humans, but rather created or produced by artificial means.\n",
      "\n",
      "Synthetic\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377112fc-c837-4ff6-8ca5-088504b2e9b2",
   "metadata": {},
   "source": [
    "# Create a YouTube Video Summarizer Using Whisper and LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331567aa-d7da-44ba-8957-1967f25fc60a",
   "metadata": {},
   "source": [
    "![图片描述](pics/youtube_video_summarizer.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e821f11-51ec-4e4a-93b6-8ff491b20eef",
   "metadata": {},
   "source": [
    "## Installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c3fe00-7019-4d2a-bea0-da7a6617cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q yt_dlp\n",
    "# !pip install -q git+https://github.com/openai/whisper.git # 该方法失败，下面的方法可以成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb0b8a0-ee6c-44c1-9975-0672f91929ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global http.proxy http://127.0.0.1:7890\n",
    "!git config --global https.proxy http://127.0.0.1:7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0dadeec-a041-49d2-920c-177dfd1c9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'E:\\LocalWork\\PyProjects\\LangChain-Vector\\whisper'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/whisper.git E:\\LocalWork\\PyProjects\\LangChain-Vector\\whisper\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4eeed4-f1e6-45d7-96d2-5775e51d868d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Processing e:\\localwork\\pyprojects\\resource\\whisper\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numba in c:\\users\\administrator\\appdata\\roaming\\python\\python39\\site-packages (from openai-whisper==20230314) (0.57.1)\n",
      "Requirement already satisfied: numpy in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from openai-whisper==20230314) (1.24.4)\n",
      "Requirement already satisfied: torch in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from openai-whisper==20230314) (2.0.1)\n",
      "Requirement already satisfied: tqdm in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from openai-whisper==20230314) (4.65.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\administrator\\appdata\\roaming\\python\\python39\\site-packages (from openai-whisper==20230314) (10.1.0)\n",
      "Requirement already satisfied: tiktoken==0.3.3 in c:\\users\\administrator\\appdata\\roaming\\python\\python39\\site-packages (from openai-whisper==20230314) (0.3.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.31.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\administrator\\appdata\\roaming\\python\\python39\\site-packages (from numba->openai-whisper==20230314) (0.40.1)\n",
      "Requirement already satisfied: filelock in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from torch->openai-whisper==20230314) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from torch->openai-whisper==20230314) (4.7.1)\n",
      "Requirement already satisfied: sympy in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from torch->openai-whisper==20230314) (1.12)\n",
      "Requirement already satisfied: networkx in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from torch->openai-whisper==20230314) (3.1)\n",
      "Requirement already satisfied: jinja2 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
      "Requirement already satisfied: colorama in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from tqdm->openai-whisper==20230314) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from jinja2->torch->openai-whisper==20230314) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\programfiles\\anaconda\\envs\\c_lcvd\\lib\\site-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=807707 sha256=73fded196742882a7aab4a8accf7c8272f617c956af5fdd709345b686fb119f7\n",
      "  Stored in directory: C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-303hutak\\wheels\\66\\e7\\1d\\2a7d9e2cebf9d7bacf6cd1b79087b5f7904b9c0b649f08c21b\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: openai-whisper\n",
      "  Attempting uninstall: openai-whisper\n",
      "    Found existing installation: openai-whisper 20230314\n",
      "    Uninstalling openai-whisper-20230314:\n",
      "      Successfully uninstalled openai-whisper-20230314\n",
      "Successfully installed openai-whisper-20230314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script whisper.exe is installed in 'C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install ../Resource/whisper --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f240d87-ed33-4198-ad95-c45264f1404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(url):\n",
    "    # Set the options for the download\n",
    "    filename = 'lecuninterview.mp4'\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': filename,\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=05w4fbYqbmU&ab_channel=EyeonAI\"\n",
    "download_mp4_from_youtube(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00d936-5f7f-4b5b-adbd-3b6595df4fca",
   "metadata": {},
   "source": [
    "## Now it’s time for Whisper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e52620d-cc69-45b8-bfd1-d9626c1fa862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, I'm Craig Smith and this is I on AI. This week I speak with Bratine Saaha, the head of Amazon's machine learning services. We talked about Amazon's growing dominance in model building and deploying AI about the company's SageMaker platform and whether anyone can compete with the behemoth. Before we begin, I want to thank our sponsor ClearML, the MLOP solution. You can check them out at clear.ml.com. In the meantime, I hope you find my conversation with Bratine as interesting as I did. Maybe start by describing your background when you came to Amazon and what you're doing at Amazon now. At Amazon, I now lead all of our AI and machine learning services, one of which is H. What we have the broadest and deepest set of capabilities. And then Brad, I was at NVIDIA, VP of Software Infrastructure, I did my PhD at Yale University and then also went to Harvard Business School. Been in drugs, been on the product side, been on the business side and now at Amazon leading all of the AI and machine learning services. And the services are platforms like SageMaker with various tools. We think of our customer persona in three broad categories. So there's one set of customers who say, give us just the optimized infrastructure and we will build a machine learning infrastructure and we'll build the machine learning models and we'll deploy it ourselves. So for them, we have what we call ML engines. These are deep learning containers, deep learning armies, frameworks like the core software components. We optimize them for a continuous infrastructure and then we provide them to customers to use. Then there's the next set of customers, which is where most customers are today who say building the ML infrastructure is undifferentiated heavy lifting. We would rather have you build infrastructure so we can focus on just doing the machine learning models. These are data science teams. And so that is where most of our customers are. And that is what Sage makes. It gives you an end to end platform, the ML infrastructure so that you can build a train and deploy machine learning models. And then we have the Mars who we give the pre-trained machine learning models as well. So you can just use API's. You can infuse intelligence into every app that you're building by just making calls to these functions, doing things like natural language processing, doing things like document processing, doing things like computer vision, image recognition kind of stuff. And we have solutions as well. So if you look at monitor on which is one of our solutions for preventive maintenance, that uses machine learning to predict when your equipment may be due for maintenance or may actually have downtime. We have things like panorama, which are solutions using computer vision for things like quality inspection and so on. So we think of them as three broad tiers. One, there are people are building their own machine learning infrastructure in the own models. The second is we build infrastructure. They build a model and the third is we build a model. So as well, they just invoke and they take the models and train it on their data set. They take the models and train. So for example, you can have pre-trained models. So you can start with some of the models. We provide this is the SageMaker layer and then the train it using the data set. So you either build models from scratch or you start with some models that we provide or you get models from the open source and then train it on the data set. I've heard from a lot of people that one of the attractions of working with Amazon SageMaker and tools like that is if you're on the Amazon Cloud, you're within that ecosystem. Is that important that kind of interoperability that happens within an ecosystem or can you be building on TensorFlow and using? We have customers today who do all of their machine learning on SageMaker, but we also have customers who do part of it and then do the remaining thing somewhere else. So for example, you could build your machine learning models on Prem and then deploy it on the cloud. You could on the other hand train your models in the cloud and then deploy them on Prem. So we have customers of all kinds and it's important for us to enable that so that customers have the choice and our container formats are open source. So you know exactly how you need to build your stuff. So customers can do part of the machine learning in the cloud on Adeson SageMaker and then do the remaining thing on Prem or somewhere they have the full freedom to do that. What is the difference between the Amazon ecosystem and say the Google ecosystem or the Microsoft ecosystem where you have in each case you have frameworks and then platforms and cloud integrations and all of the things that you can do. So you can do all of that and then do the same thing. So we have a lot of questions and all of that. It's hard from outside to understand whether one is pointed more towards industry one is pointed more towards search or whether they're fairly interchangeable at this point. So that's the reason why we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we have a lot of questions and we can learn more delivered了吧 עَنَّ الشِلْالْ الَ GanzHer AS There's a lot of solutions toшَفَукَّعَنْ مَزَنْ وَرْعَنْ مَزَنْросَنْ فَبُغْكَلَّن So what we have done at OS is taken that deep expertise and made it available to our customers. And then when you look at performance, when you look at the performance of open source frameworks like TensorFlow and PyTorch, these frameworks run fastest on AWS. So if you look at third party sites and third party analysts reports and all that, they will tell you that in terms of the breadth of features, in terms of the breadth of capabilities, we have a lot more. And that is what attracts a lot more customers and our machine learning services, AI services like SageMaker, they're one of the fastest doing services in AWS history. And so the key factors are the breadth and depth of our machine learning services. Second machine learning doesn't exist in isolation. It requires this foundation of compute and storage and database and analytics where we have the most capable services. And then the fact that we are talking to the most customers because most customers users allows us to iterate very quickly and get more capabilities out and the speed of innovation. So I think these are the most important factors. But a more machine learning happens in AWS than anywhere else. There is an explosion of ML ops startups right now who are, they all seem to claim to be ended, but they typically take one part of the ML ops pipeline and focus on that. Is there room for smaller startups to develop platforms and tools when there's something like SageMaker that already exists? And one of the things I hear from the ML ops people is, yes, SageMakers there. People use SageMaker because they're on AWS, but SageMaker is a broad brush tool. It doesn't have a lot of the granularity for specific use cases that our ML ops platform has. Not to get into the, he said she said, but is there some merit to that? Is there room for these new ML ops companies? This is a very broad space, right? Now that said, if you look at the breadth of customers that are using machine learning on AWS and on SageMaker, as I said, it's one of the fastest growing services in AWS history. We have more than 100,000 customers using these services and they're being used in every domain in financial services in healthcare, media entertainment and so on. Co-industries, they use a service monitor on that uses machine learning to detect vibrations and predict equipment failures and they have been able to save a lot of downtime. Geology of pacific uses it to improve paper quality. They use it to predict the pace at which the paper roles should be operating and they have been able to reduce paper tests by 40%. Then you have healthcare companies like AstraZeneca and Philips that are using our machine learning services. Then you have into it that uses a machine learning services for apps like Mint and DoubleTax. And so given the breadth of customers that are using our machine learning services, they're being used for pretty much every use case you can think about. And that is why having an end-to-end platform is so important because you want to have end-to-end traceability. You want to automate the end-to-end process. So sports, media, software, financial industry, healthcare, all of them are now doing machine learning on AWS using SageMaker or AI services. And when you say machine learning, how much of that is deep learning? Pretty good amount of that is deep learning and then amount of deep learning keeps on increasing. We are seeing customers moving more and more towards using deep learning and more sophisticated models. It has been increasing over the last several years. It's primarily supervised learning. It's a combination of both. We do see learn supervised learning as well. Little bit of reinforcement learning. But I think more people use supervised learning today on our services. SageMaker, the infrastructure product, it has a data prep end and it has deployment and monitoring. And I was talking earlier about these ML ops companies. There's one I'm familiar with, label box. And when I've spoken to label box, SageMaker always comes up. And their argument is that they focus entirely on the labeling process, the iterative process, the discovery of bias and mitigation and all of that to improve data quality. Any machine learning process starts with data. You can have structured data or you can have unstructured data. And for structured data, we have data right now. And that is the tabular data that you have in a databases that you want to be able to analyze for machine learning. Then there's unstructured data. And unstructured data could be images, could be text, could be audio and videos and so on. That is where data labeling comes in. And so for that, we have ground truth and ground truth provides a fully turnkey experience. You as a customer just come in and hand the data, ground truth does all of the data labeling and then hands the label data back to you. And we often use machine learning models to automate that data labeling. Now as part of that, we have quality checks to make sure that the labeling has been done well. And then we have tools like SageMaker Clarify that lets you do bias detection and let you do bias detection. Not just in your data, it lets you do bias detection even in your models. So we have a very holistic suite between structured data and unstructured data. One structure that is where you're looking at data labeling. We do the data labeling. We do quality checks. Then you can do bias analysis. And then once your data is prepared, then you send this on for model building and model deployment and so on. And then in model deployment, we have model monitoring. So once your models have been deployed, you do that. Now the benefit of having this end-to-end platform, which is very comprehensive data labeling platform included as part of this whole thing, is that once you have deployed the model into production, you want to monitor it. And then if there are some mispredictions, you want to be able to come back and retrain your model. Now because it's all in a single platform, you can look at where did the model go wrong, pick up those kinds of data, relabel it and then retrain it. And that is why customers say you want to be in a platform where I can get the data labeling through the model deployment. And in data labeling, we actually have lots of modalities. So we have audio, we have video, we have text, we have LIDAR. Customers can build their own data labeling workflows. They can choose from three different kinds of workforces. So we have our own public crowd workforce. Then they can bring their own workforce. And then we have a number of vendors so they can contract out to those vendors. So it's a very comprehensive data labeling solution that includes not just labeling, but also quality checks so people get good label data. And then of course, doing bias analysis and so on with SageMaker clarifying your data. The other thing I'll also say is we see customers do a lot more what we would call multi-modal data analysis. For example, in the financial industry, they want to be able to combine both tabular data and text data. For example, financial industry has been traditionally using tabular data, but now they want to analyze things like SEC forms and so on. And so this ability to be able to process both tabular data and unstructured data. I do data labeling and this other one is something that's very valuable to customers. Is there a cottage industry of people that are experts at SageMaker who are hired out as consultants or does Amazon have that service so that they don't have to start from scratch and do tutorials and figure out how SageMaker works and as you have a team of consultants that come in and help. So we have a variety of resources. We have solution architects that help customers. We have ML solutions lab where we have data scientists work together with customers to bring their models to production. We have a lot of documentation available. We have a number of other customers, SIs and GSI system indicators, global system indicators that help customers get started. And also we are constantly focusing on making it easier and easier to get started. So we recently launched a part of SageMaker that we call SageMaker Canvass. That's a no code solution for doing machine learning. You don't have to write a single line of code. You bring your data and the system automatically does the data preparation for you automatically creates models for you and then it'll deploy the models for you. So we've been very pleased to see how many people have started using that. So there are a number of things we are doing on the one hand making it easier to use our services at the product level through all of these innovations. Then having a lot more documentation and collateral, lot of tutorials, lot of examples, use cases, then our own solution architects, ML solutions labs. We have the machine learning university where all of the machine learning courses that we use for training Amazon engineers are now available for free to everyone. Then we partner with online education sites like Coursera where we have the practical applications of data science course with Andrew and we use that to help customers get started on SageMaker. So there's multiple things that we are doing from the product education collateral, ML solutions lab, solution architects to help customers get started. In your job, you describe the three layers. Where do you spend most of your time? Where is the most innovation happening? Is it at the infrastructure level We have a lot of innovation happening at all three levels and a lot of growth happening across all of these services. The AI services are being used by customers in very interesting ways. So there you don't need a lot of machine learning expertise to get started. So these are more like solutions. So we have customers like Anthem who have been able to automate almost 80% of the claims insurance process. We have customers like Discovery who are using our Amazon personalize to provide a much more personalized experience. What kind of shows you would like? We have co-op industries using monitron for industrial maintenance. Then we are into it that's using transcribe for call center intelligence. So we are seeing broad growth across all the three layers. Now SageMaker is one of the fastest growing services in the history that's what most of customers have used but we have seen growth in all three areas and very robust growth in all three layers. Okay, my last question is on the models. In the APIs available you have computer vision and natural language processing and things like that. What's the newest model that you're making available? We are adding a lot of machine learning for DevOps. If you're running cloud services automatically detecting anomalies. So that is new DevOps guru. We have services for forecasting. So we have a very broad collection and we are constantly innovating. We're looking for what customers want, looking for what customers need and just constantly iterating on those. That's it for this episode. I want to thank Broughton for his time and invite you to read a transcript of the conversation on our website iOnAI. That's EYE-ON.AI. I want also to thank our sponsor clear ml the ml-opt solution that you can try out for free at cl-e-a-r.ml. And remember the singularity may not be near but AI is about to change your world so pay attention.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"lecuninterview.mp4\", fp16=False)\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "030c182f-4afa-40f2-acd7-3b2c74e2e7a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, I'm Craig Smith and this is I on AI. This week I speak with Bratine Saaha, the head of Amazon's machine learning services. We talked about Amazon's growing dominance in model building and deploying AI about the company's SageMaker platform and whether anyone can compete with the behemoth. Before we begin, I want to thank our sponsor ClearML, the MLOP solution. You can check them out at clear.ml.com. In the meantime, I hope you find my conversation with Bratine as interesting as I did. Maybe start by describing your background when you came to Amazon and what you're doing at Amazon now. At Amazon, I now lead all of our AI and machine learning services, one of which is H. What we have the broadest and deepest set of capabilities. And then Brad, I was at NVIDIA, VP of Software Infrastructure, I did my PhD at Yale University and then also went to Harvard Business School. Been in drugs, been on the product side, been on the business side and now at Amazon leading all of the AI and machine learning services. And the services are platforms like SageMaker with various tools. We think of our customer persona in three broad categories. So there's one set of customers who say, give us just the optimized infrastructure and we will build a machine learning infrastructure and we'll build the machine learning models and we'll deploy it ourselves. So for them, we have what we call ML engines. These are deep learning containers, deep learning armies, frameworks like the core software components. We optimize them for a continuous infrastructure and then we provide them to customers to use. Then there's the next set of customers, which is where most customers are today who say building the ML infrastructure is undifferentiated heavy lifting. We would rather have you build infrastructure so we can focus on just doing the machine learning models. These are data science teams. And so that is where most of our customers are. And that is what Sage makes. It gives you an end to end platform, the ML infrastructure so that you can build a train and deploy machine learning models. And then we have the Mars who we give the pre-trained machine learning models as well. So you can just use API's. You can infuse intelligence into every app that you're building by just making calls to these functions, doing things like natural language processing, doing things like document processing, doing things like computer vision, image recognition kind of stuff. And we have solutions as well. So if you look at monitor on which is one of our solutions for preventive maintenance, that uses machine learning to predict when your equipment may be due for maintenance or may actually have downtime. We have things like panorama, which are solutions using computer vision for things like quality inspection and so on. So we think of them as three broad tiers. One, there are people are building their own machine learning infrastructure in the own models. The second is we build infrastructure. They build a model and the third is we build a model. So as well, they just invoke and they take the models and train it on their data set. They take the models and train. So for example, you can have pre-trained models. So you can start with some of the models. We provide this is the SageMaker layer and then the train it using the data set. So you either build models from scratch or you start with some models that we provide or you get models from the open source and then train it on the data set. I've heard from a lot of people that one of the attractions of working with Amazon SageMaker and tools like that is if you're on the Amazon Cloud, you're within that ecosystem. Is that important that kind of interoperability that happens within an ecosystem or can you be building on TensorFlow and using? We have customers today who do all of their machine learning on SageMaker, but we also have customers who do part of it and then do the remaining thing somewhere else. So for example, you could build your machine learning models on Prem and then deploy it on the cloud. You could on the other hand train your models in the cloud and then deploy them on Prem. So we have customers of all kinds and it's important for us to enable that so that customers have the choice and our container formats are open source. So you know exactly how you need to build your stuff. So customers can do part of the machine learning in the cloud on Adeson SageMaker and then do the remaining thing on Prem or somewhere they have the full freedom to do that. What is the difference between the Amazon ecosystem and say the Google ecosystem or the Microsoft ecosystem where you have in each case you have frameworks and then platforms and cloud integrations and all of the things that you can do. So you can have a lot of questions and all of that. It's hard from outside to understand whether one is pointed more towards industry, one is pointed more towards search or whether they're fairly interchangeable at this point. The vast majority of machine learning in the cloud happens on the west, the vast majority. And so the next question is why does the vast majority happen? And that is because of a variety of reasons one is machine learning does not exist in isolation machine learning is built on the compute and storage and data services that we have. And when you look at compute storage and database and analytic services, we have by far the most advanced most broad set of services includes city and all of that. So that is one reason. The second is when you again look at machine learning services that depth and breadth of our capabilities from model building to natural language processing to computer vision to industrial monitoring and all of that. We have the broadest and deepest set of services by far that enables customers to get their work done. And then the other thing I would also say is if you look at Amazon as a company at our investments in machine learning, we have been investing in machine learning for more than 20 years. In fact, you look at Amazon Alexa and Amazon go Amazon.com with recommendations. We have been deploying machine learning at scale for a long time. And so what we have done at was is taken that deep expertise and made it available to our customers. And then when you look at performance when you look at the performance of open source frameworks like TensorFlow and PyTorch, these frameworks run fastest on AWS. So if you look at third party sites and third party analysts reports and all that, they will tell you that in terms of the breadth of features in terms of the breadth of capabilities, we have a lot more. And that is what attracts a lot more customers and our machine learning services AI services like Sage maker, they're one of the fastest growing services in AWS history. And so the key factors are the breadth and depth of our machine learning services. Second machine learning doesn't exist in isolation. It requires this foundation of compute and storage and database and analytics where we have the most capable services. And then the fact that we are talking to the most customers because most customers users allows us to iterate very quickly and get more capabilities out and the speed of innovation. So I think these are the most important factors, but a more machine learning happens in AWS than any of it is. There is an explosion of ML ops startups right now who are they all seem to claim to be ended, but they typically take one part of the ML ops pipeline and focus on that. Is there room for smaller startups to develop platforms and tools when there's something like Sage maker that already exists and one of the things I hear from the ML ops people is yes, Sage makers there, people use Sage maker because they're on AWS, but Sage maker is a broad brush tool. It doesn't have a lot of the granularity for specific use cases that our ML ops platform has not to get into the he said she said, but is there some merit to that is there room for these new ML ops companies. This is a very broad space right now that said if you look at the breadth of customers that are using machine learning on AWS and on Sage maker as I said, it's one of the fastest growing services in AWS history. We have more than 100,000 customers using these services and they're being used in every domain in financial services in healthcare media entertainment and so on coke industries they use a service monitor on that uses machine learning to detect. vibrations and predict equipment failures and they have been able to save a lot of downtime geogia pacific uses it to improve paper quality they use it to predict the pace at which the paper rolls should be operating and they have been able to reduce paper test by 40% then you have healthcare companies like AstraZeneca and Philips that are using our machine learning services then you've into it that uses a machine learning services for apps like mint and double tax and so on. So given the breadth of customers that are using our machine learning services they're being used for pretty much every use case you can think about and that is why having an end to end platform is so important because you want to have into the disability you want to convey to the entire end to end process so sports media software financial industry healthcare all of them are now doing machine learning on AWS using Sage maker or AI services and when you say machine learning how much of that is deep learning pretty good amount of that is deep learning and then amount of deep learning keeps on increasing we are seeing customers moving more and more towards using deep learning and most sophisticated models it has been increasing over the last. It's primarily supervised learning it's it's a combination of both we do see on supervised learning as well little bit of reinforcement learning but I think more people use supervised learning today on our services. Sage maker the infrastructure product it has a data prep end and it has deployment and monitoring and I was talking earlier about these ML ops companies there's one of them layer with label box and when I've spoken to label box Sage maker always comes up and their argument is that they focus entirely on the labeling process the iterative process the discovery of bias and mitigation and all of that to improve data quality. The structure data and unstructured data could be images could be text could be audio and videos and so on that is the data labeling comes in and so for that we have ground rule and ground road provides a fully turn key experience you as a customer just come in hand the data ground through does all of the data labeling and then hands the label data back to you and we often use machine learning models to automate the data labeling now as part of that we have quality checks to make sure that the labeling has been done well and then we have tools like Sage maker clarify that lets you do bias detection and let you do bias detection not just in your data it lets you do bias detection even in your models. So we have a very holistic suite between structured data and unstructured data one structure that is where you're looking at data labeling we do the data labeling we do quality checks then you can do bias analysis and then once your data is prepared then you send this on for model building and model deployment and so on. And then in model deployment we have model monitoring so once your models have been deployed you do that now the benefit of having this end to end platform which is very comprehensive data labeling platform included as part of this whole thing is that once you have deployed the model into production you want to monitor it and then if there are some mis predictions you want to be able to come back and retrain your model now because it's all in a single platform you can look at where did the model go wrong pick up those kinds of data. Relabel it and then retrain it and that is why customers say want to be in a platform where I can get the data labeling through the model deployment and in data labeling we actually have a lots of modalities so we have audio we have video we have text we have LIDAR customers can build their own data labeling workflows they can choose from three different kinds of workforces so we have our own public crowd workforce then they can bring their own workforce and then we have a lot of work for us. So we have a lot of work for us and then we have a number of vendors so they can contract out to those vendors so it's a very comprehensive data labeling solution that includes not just labeling but also quality checks so people get good label data and then of course doing bias analysis and so on with Sage maker clarifying your data. So what we are also saying is we see customers do a lot more what we would call multi model data analysis for example in the financial industry they want to be able to combine both tabular data and text data for example financial industry has been traditionally using tabular data but now they want to analyze things like a CC forms and so on and so this ability to be able to process both tabular data and unstructured data do data labeling and this other one is something that's very valuable to customers. So is there a cottage industry of people that are experts at Sage maker who are hired out as consultants or does Amazon have that service so that they don't have to start from scratch and do tutorials and figure out how Sage is doing. Sage maker works and as you have a team of consultants to come in and help. So we have a variety of resources so we have solution architects that help customers we have ML solutions lab where we have data scientists work together with customers to bring their models to production. We have a lot of documentation available. We have a number of other customers size and GSI system indicators global system indicators that help customers get started and also we are constantly focusing on making it easy to do. So we recently launched a part of Sage maker that we call Sage maker can was that's a no code solution for doing machine learning you don't have to write a single line of code. You bring your data and the system automatically does the data preparation for you automatically create models for you and then it will deploy the models for you. So we've been very pleased to see how many people have started using that so there are a number of things we are doing on the one hand making it easier to use a services at the product level through all of these innovations then having a lot more documentation and collateral lot of tutorials lot of examples use cases then our own solution architects ML solutions labs we have the machine learning university where all of the machine learning courses that we use for training Amazon engineers are now available for free to everyone then we partner with online education sites like Coursera where we have the practical applications of data science course with Andrew and we use that to help customers get started on Sage maker. So there's multiple things that we are doing from the product education collateral ML solutions lab solution architects to help customers get started. In your job you describe the three layers where do you spend most of your time where is the most innovation happening is it at the infrastructure level we have a lot of innovation happening at all three levels and a lot of growth happening across all of these services the AI services are being used by customers in very interesting ways so there you don't need lot of machine learning expertise to get started so these are more like solutions. So we have customers like Anthem who have been able to automate almost 80% of the claims insurance process we have customers like discovery were using our Amazon personalized to provide a much more personalized experience what kind of shows you would like we have coke industries using monitron for industrial maintenance then we have into it that's using transcribe for call center intelligence. So we are seeing broad growth across all the three layers now Sage maker is one of the fastest growing services in the history that's what most of customers have used but we are seeing growth in all three areas and very robust growth in all three layers. Okay, my last question is on the models in the APIs available you have computer vision and natural language processing and things like that what's the newest model that you're making available we are adding a lot of machine learning for DevOps if you're running cloud services automatically detecting anomalies so that is new DevOps guru we have services for forecasting so we have a very broad collection and we are constantly innovating we're looking for what we are doing. What customers want looking for what customers need and just constantly it's reading on those. That's it for this episode I want to thank Broughton for his time and invite you to read a transcript of the conversation on our website I on AI that's EY E hyphen O N dot AI. I want also to thank our sponsor clear ml the ml up solution that you can try out for free at cllear dot ml and remember the singularity may not be near but AI is about to change your world so pay attention. .\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"lecuninterview.mp4\", fp16=False)\n",
    "print(result['text'])\n",
    "\n",
    "# options = whisper.DecodingOptions(language= 'en', fp16=False)\n",
    "# result = whisper.decode(model, \"lecuninterview.mp4\", options)\n",
    "# print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263c887c-0fd8-4bbe-895c-1cc51e34912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('text.txt', 'w') as file:  \n",
    "    file.write(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c8ea1-9c9d-4ff6-8d5b-6c26d9a8e112",
   "metadata": {},
   "source": [
    "## Summarization with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916de804-9cf7-4a76-a76d-652a6f5187d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.22) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028cfc52-f70a-48f7-ab53-2e097709d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa3904c-dfd9-4989-9b3d-a2cb52fde40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open('text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63b41b0-b360-4b2c-96de-aa522c10ef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Craig Smith interviews Bratine Saaha, the head of Amazon's machine learning services, about\n",
      "Amazon's dominance in model building and deploying AI, their SageMaker platform, and whether anyone\n",
      "can compete with them. Amazon SageMaker provides pre-trained models and the ability to build models\n",
      "from scratch or from open source. It is possible to do all machine learning on SageMaker, or to do\n",
      "part of it on SageMaker and the rest elsewhere. It is important to be within the Amazon Cloud\n",
      "ecosystem for interoperability.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab435b-19ce-44cf-b150-81dadd3fa34c",
   "metadata": {},
   "source": [
    "The `textwrap` library in Python provides a convenient way to wrap and format plain text by adjusting line breaks in an input paragraph. It is particularly useful when displaying text within a limited width, such as in console outputs, emails, or other formatted text displays. The library includes convenience functions like `wrap`, `fill`, and `horten`, as well as the `TextWrapper` class that handles most of the work. If you’re curious, I encourage you to follow this link and find out more, as there are other functions in the `textwrap` library that can be useful depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc5f103-1dd3-4e18-9caf-4676a1caffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17326f2-1de5-4b9f-8246-cfd39f18d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7918bd7-1676-44f5-8e04-8c5356ce1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Craig Smith speaks with Bratine Saaha, head of Amazon's machine learning services\n",
      "- Discussion focuses on Amazon's dominance in model building and deploying AI, and the SageMaker platform\n",
      "- Bratine's background includes VP of Software Infrastructure at NVIDIA, PhD from Yale University, and Harvard Business School\n",
      "- Amazon's services are divided into three tiers: customers building their own ML infrastructure and models, customers using Amazon's ML infrastructure to build models, and customers using pre-trained models\n",
      "- Interoperability within Amazon's ecosystem is important, but customers can also use other tools like TensorFlow\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d52dbc-2bc6-4727-bc55-2a7fcdff3139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Craig Smith interviews Bratine Saaha, the head of Amazon's machine learning services, about\n",
      "Amazon's dominance in model building and deploying AI, their SageMaker platform, and whether anyone\n",
      "can compete with them. Bratine has a background in software infrastructure, having done his PhD at\n",
      "Yale University and Harvard Business School, and now leads all of Amazon's AI and machine learning\n",
      "services. Amazon's machine learning services are platforms like SageMaker with various tools, which\n",
      "are tailored to three customer personas. The first set of customers are given optimized\n",
      "infrastructure to build and deploy their own machine learning models. The second set of customers\n",
      "are given the ML infrastructure to focus on building the machine learning models, which is what\n",
      "SageMaker provides. The third set of customers are given pre-trained machine learning models as well\n",
      "as APIs to infuse intelligence into applications such as natural language processing, document\n",
      "processing, and computer vision. Amazon also provides solutions such as Monitor for preventive\n",
      "maintenance and Panorama for quality inspection. Customers can either build models from scratch,\n",
      "start with pre-trained models provided by Amazon, or use open source models and train them on their\n",
      "own data set. Amazon's ecosystem allows customers to do all of their machine learning on SageMaker,\n",
      "or part of it and the remaining part\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a4613-2b76-48a5-86d6-60216b636f56",
   "metadata": {},
   "source": [
    "The `'refine'` summarization chain in LangChain provides a flexible and iterative approach to generating summaries, allowing you to customize prompts and provide additional context for refining the output. This method can result in more accurate and context-aware summaries compared to other chain types like `'stuff'` and `'map_reduce'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb287d51-a936-42b4-8892-b521de18d1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96e896-38f1-4f63-848f-f1f81dcab471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8ff42-3614-4f6c-a11f-14ad19f03f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e026358-e679-4520-a10f-2c979d135123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c7f8b-4d9b-4b19-a066-302a3b6d7580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4c01e-6bb0-4898-929c-c2832ebb5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
