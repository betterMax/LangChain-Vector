{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6672558-bd7d-479d-b8ab-26ee86e1030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba670af0-4bb4-44ed-b29d-db64ecb61b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前windows\n",
    "# 获取当前的 Conda 环境路径\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "\n",
    "# \".env\" 文件的绝对路径\n",
    "dotenv_path = os.path.join(conda_env_path, '.env')\n",
    "\n",
    "# 加载 \".env\" 文件\n",
    "_ = load_dotenv(dotenv_path, verbose=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d11a87-1716-4545-be86-2b1774bc223a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Quick Intro to Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693b283-12d9-4b58-81ff-7c57f5a8b30e",
   "metadata": {},
   "source": [
    "## LLMs in general:\n",
    "### Maximum number of tokens\n",
    "`split_text_into_chunks` and `combine_resultsCopy` are custom functions that you would need to implement based on your specific requirements, and we will cover them in later lessons. The key takeaway is to ensure that the input text does not exceed the maximum number of tokens supported by the model.\r\n",
    "\r\n",
    "Note that splitting into multiple chunks can hurt the coherence of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f179da-f5f7-4133-bba8-b4bf8a1eecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"your_long_input_text\"\n",
    "\n",
    "# Determine the maximum number of tokens from documentation\n",
    "max_tokens = 4097\n",
    "\n",
    "# Split the input text into chunks based on the max tokens\n",
    "text_chunks = split_text_into_chunks(input_text, max_tokens)\n",
    "\n",
    "# Process each chunk separately\n",
    "results = []\n",
    "for chunk in text_chunks:\n",
    "    result = llm.process(chunk)\n",
    "    results.append(result)\n",
    "\n",
    "# Combine the results as needed\n",
    "final_result = combine_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44b53a-778c-4167-a47d-b73e5f059d4e",
   "metadata": {},
   "source": [
    "### Tokens Distributions and Predicting the Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a50c8a2-3a9f-47e9-b292-96fe1dbc2b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.17) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Socks Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7aba08-3387-4702-a049-2a61ab50fba5",
   "metadata": {},
   "source": [
    "#### Tracking Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864fa82a-7e01-48fc-a2d4-bbe65f7b1972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 45\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 41\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0009000000000000001\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b374755-26f5-47c7-8295-1b306a676edc",
   "metadata": {},
   "source": [
    "### Few-shot learning\n",
    "Few-shot learning is a remarkable ability that allows LLMs to learn and generalize from limited examples.\n",
    "\n",
    "This approach involves using the `FewShotPromptTemplate` class, which takes in a `PromptTemplate` and a list of a few shot examples. The class formats the prompt template with a few shot examples, which helps the language model generate a better response. We can streamline this process by utilizing LangChain's FewShotPromptTemplate to structure the approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ee33c0-03c1-4a4a-82c8-2c1307447862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37fd955-0b60-4f2c-a8d6-fa34138ed70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The meaning of life is to find your own purpose and live it to the fullest!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "# load the model\n",
    "# chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "chat = OpenAI(model_name=\"text-davinci-003\", temperature=0.0)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca5bb9e-e58f-4406-9478-1f498b603447",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85062e-55c0-4164-8f96-de30c92fe090",
   "metadata": {},
   "source": [
    "### Examples with Easy Prompts: Text Summarization, Text Translation, and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4a475-d69a-496a-9433-fca73c880ef0",
   "metadata": {},
   "source": [
    "#### Creating a Question-Answering Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04312ee-804d-4384-b19d-d9a47001dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7303a265-819d-4558-82a9-b4821bf61e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b3185-297b-41e6-b702-b8e0ae20d53c",
   "metadata": {},
   "source": [
    "#### Asking Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57e75a1-18a6-4efe-8947-0ce7f69535c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('a294f472-188c-4053-9d37-6876c120a3ef'))\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09c60e7f-7d2a-418a-a7ab-404440c915c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm #hub_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58a60152-583d-4fff-a86c-f61ee962e6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The capital city of France is Paris.\\nThe largest mammal on Earth is the blue whale.\\nThe gas most abundant in Earth's atmosphere is nitrogen.\\nA ripe banana is yellow.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6790978-f33d-40e1-a93e-baf4c3eb7893",
   "metadata": {},
   "source": [
    "#### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b2ed93-c6e1-4c24-a4ee-647790feaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c223be-1fd0-42a8-b08a-2d55626281ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e60882-a174-498d-a8a6-3ae1e02801a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain offers various modules for building language model applications, allowing users to combine them for more complex applications or use them individually for simpler ones, with the basic building block being calling an LLM on input, as demonstrated in the example of creating a company name based on its product.\n"
     ]
    }
   ],
   "source": [
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)\n",
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c9d05-1adf-4d7d-b417-62e68c03e47d",
   "metadata": {},
   "source": [
    "#### Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eeef284-d758-4468-9578-3ce2cb0ecb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\",\"target_language\",\"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d34f3d5-7bdf-4d85-9efd-846e940770db",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"Chinese\"\n",
    "text = \"Your text here\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8cf7c3f-38bf-41c6-a44c-56d2d62d1fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的文本在这里\n"
     ]
    }
   ],
   "source": [
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8607d4-0959-445c-9573-1da4ad6b2378",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Understanding Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69361ae-5af9-4dad-acde-ab5c5f10ae80",
   "metadata": {},
   "source": [
    "1. Character Level: Consider each character in a text as a token.\n",
    "2. Word Level: Encoding each word in the corpus as one token.\n",
    "3. Subword Level: Breaking down a word into smaller chunks when possible. For example, we can encode the word “basketball” to the combination of two tokens as “basket” + “ball”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05e5b2-9eb6-410f-a3ae-492d66df49cf",
   "metadata": {},
   "source": [
    "<img src=\"pics/encoding approaches.webp\" alt=\"图片描述\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5c3a8-0ab8-421d-8412-b2ff6dda6d21",
   "metadata": {},
   "source": [
    "Therefore, our sole focus will be on one of its specific variants, known as Byte Pair Encoding (BPE). It is worth mentioning that other subword level algorithms exist, such as WordPiece and SentencePiece.\n",
    "\n",
    "## Byte Pair Encoding (BPE)\n",
    "Due to the fact that neural networks only accept numerical inputs, we can utilize the vocabulary to establish a mapping between tokens and their corresponding IDs, like a lookup table. We have to save the vocabulary for future use cases to be able to decode the model's output from the IDs to words. This is known as a pre-trained vocabulary, an essential component accompanying published pre-trained models. Without the vocabulary, understanding the model's output (the IDs) would be impossible. For smaller models like BERT, the dictionary can consist of as few as 30K tokens, while larger models like GPT-3 can expand to encompass up to 50K tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a25da6e4-1601-481f-b7ef-1d27c0a7bd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████| 665/665 [00:00<00:00, 328kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 1.21MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███████████████████████████████████████████| 456k/456k [00:00<00:00, 467kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████| 1.36M/1.36M [00:01<00:00, 1.17MB/s]\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download and load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19ef702e-0b38-4219-a5db-617be983bac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2116873-b5f7-4bd8-97de-9b8d9ca58f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:    ['This', 'Ġis', 'Ġa', 'Ġsample', 'Ġtext', 'Ġto', 'Ġtest', 'Ġthe', 'Ġtoken', 'izer', '.']\n",
      "Token IDs: [1212, 318, 257, 6291, 2420, 284, 1332, 262, 11241, 7509, 13]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\"This is a sample text to test the tokenizer.\")\n",
    "\n",
    "print(\"Tokens:   \", tokenizer.convert_ids_to_tokens( token_ids ))\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319e31a-0cfe-4870-91c4-d61bdcc5dfd6",
   "metadata": {},
   "source": [
    "## Tokenizers Shortcomings\n",
    "Several issues with the present tokenization methods are worth mentioning.\n",
    "\n",
    "- **Uppercase/Lowercase Words**: The tokenizer will treat the the same word differently based on cases. For example, a word like “hello” will result in token id `31373`, while the word “HELLO” will be represented by three tokens as `[13909, 3069, 46]` which translates to `[“HE”, “LL”, “O”]`.\n",
    "- **Dealing with Numbers**: You might have heard that transformers are not naturally proficient in handling mathematical tasks. One reason for this is the tokenizer's inconsistency in representing each number, leading to unpredictable variations. For instance, the number `200` might be represented as one token, while the number 201 will be represented as two tokens like `[20, 1]`.\n",
    "- **Trailing whitespace**: The tokenizer will identify some tokens with trailing whitespace. For example a word like “last” could be represented as “ last” as one tokens instead of `[\" \", \"last\"]`. This will impact the probability of predicting the next word if you finish your prompt with a whitespace or not. As evident from the sample output above, you may observe that certain tokens begin with a special character (Ġ) representing whitespace, while others lack this feature.\n",
    "- **Model-specific**: Even though most language models are using BPE method for tokenization, they still train a new tokenizer for their own models. GPT-4, LLaMA, OpenAssistant, and similar models all develop their separate tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ec91a-58a7-4eae-accd-4e4a16a60e9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Applications Powered by LLMs with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eee828cd-92df-46a6-90a6-e2d511ebafe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Inception\" is a science fiction action film directed by Christopher Nolan. It was released in 2010 and stars Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page, Tom Hardy, and Marion Cotillard. The film follows a professional thief who steals information by infiltrating the subconscious of his targets through their dreams. \n",
      "\n",
      "The story revolves around Dom Cobb (played by DiCaprio), who is offered a chance to have his criminal history erased in exchange for performing the act of \"inception\" - planting an idea in someone's mind rather than stealing it. As Cobb and his team delve deeper into the layers of dreams, they face various challenges and encounter unexpected twists.\n",
      "\n",
      "\"Inception\" received critical acclaim for its originality, visual effects, and complex narrative. It won four Academy Awards, including Best Cinematography, Best Sound Editing, Best Sound Mixing, and Best Visual Effects. The film was also a commercial success, grossing over $828 million worldwide.\n",
      "\n",
      "If you are interested in watching \"Inception,\" it is available on various streaming platforms and can be rented or purchased on digital platforms.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"You are an assistant that helps users find information about movies.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Find information about the movie {movie_title}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "response = chat(chat_prompt.format_prompt(movie_title=\"Inception\").to_messages())\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8da551-bcfb-4af8-905a-f4b058e6adf9",
   "metadata": {},
   "source": [
    "## Summarization chain example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87a24e17-7f71-411c-9982-02b62ad1c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf==3.10.0\n",
    "# Import necessary modules\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize language model\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Load the summarization chain\n",
    "summarize_chain = load_summarize_chain(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af280064-ee51-434d-af92-00e43e8aa43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the document using PyPDFLoader\n",
    "document_loader = PyPDFLoader(file_path=\"source/GeoMan论文.pdf\")\n",
    "document = document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1804d326-b5d7-4181-a55e-4ae0dc3d03ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49022640-cfca-4412-8fea-e28b334222d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This paper proposes a Multi-level Attention Network (GeoMAN) to predict the readings of a geo-sensor over several future hours. The model consists of two parts: a multi-level attention mechanism to model dynamic spatio-temporal correlations and a general fusion module to incorporate external factors from different domains. Experiments on two real-world datasets demonstrate that GeoMAN outperforms nine baseline methods.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the document\n",
    "summary = summarize_chain(document[:2])\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b96490-4d7c-4aff-a534-cfa1fe1b703e",
   "metadata": {},
   "source": [
    "## QA chain example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e17e334-69e5-4922-acea-e62e537288af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bf90e41-1c2c-4c0d-8935-b2d6c53989a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The meaning of life is subjective and can vary from person to person. For some, it may be to find happiness and fulfillment, while for others it may be to make a difference in the world. Ultimately, the meaning of life is up to each individual to decide.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e213ff2-2e7a-4186-bae3-5d68903b7e3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring the World of Language Models: LLMs vs Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558fd0f-711a-4bce-bb54-74a769e4aabe",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "LLMs, such as GPT-3, Bloom, PaLM, and Aurora genAI, take a text string as input and return a text string as output. They are trained on language modeling tasks and can generate human-like text, perform complex reasoning, and even write code. LLMs are powerful and flexible, capable of generating text for a wide range of tasks. However, they can sometimes produce incorrect or nonsensical answers, and their API is less structured compared to Chat Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac487ca-d4fd-44b6-981b-1540fc173fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wireless Audio Solutions\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"product\"],\n",
    "  template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print( chain.run(\"wireless headphones\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbd448-2b0d-47f1-9a7f-98781ca78a08",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "Chat Models are the most popular models in LangChain, such as ChatGPT that can incorporate GPT-3 or GPT-4 at its core. They have gained significant attention due to their ability to learn from human feedback and their user-friendly chat interface.\n",
    "\n",
    "Chat Models, such as ChatGPT, take a list of messages as input and return an AIMessageCopy. They typically use LLMs as their underlying technology, but their APIs are more structured. Chat Models are designed to remember previous exchanges with the user in a session and use that context to generate more relevant responses. They also benefit from reinforcement learning from human feedback, which helps improve their responses. However, they may still have limitations in reasoning and require careful handling to avoid generating inappropriate content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5748b970-1577-4c1e-8395-161a311649bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "  HumanMessage,\n",
    "  SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "\tSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "\tHumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82e08eb-52de-44dc-9d49-abdfcba939f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text='I like programming.', generation_info=None, message=AIMessage(content='I like programming.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 65, 'completion_tokens': 12, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'} run=RunInfo(run_id=UUID('ad048bbe-8a5f-4016-964d-4190c24ea1cc'))\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "  [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "  ],\n",
    "  [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates French to English.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: J'aime la programmation.\")\n",
    "  ],\n",
    "]\n",
    "print( chat.generate(batch_messages) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1857f22-7fe2-4a78-8b03-44ab5bc8b28b",
   "metadata": {},
   "source": [
    "As a comparison, here's what LLM and Chat Model APIs look like in LangChain.\n",
    "```\n",
    "llm_output:  {'product': 'Translate the following text from English to French: Hello, how are you?', 'text': '\\n\\nBonjour, comment allez-vous?'}\r\n",
    "\r\n",
    "chat_output:  content='Bonjour, comment ça va ?' additional_kwargs={} example=False\n",
    "`\n",
    "\n",
    "LLMs and Chat Models each have their advantages and disadvantages. LLMs are powerful and flexible, capable of generating text for a wide range of tasks. However, their API is less structured compared to Chat Models.\r\n",
    "\r\n",
    "On the other hand, Chat Models offer a more structured API and are better suited for conversational tasks. Also, they can remember previous exchanges with the user, making them more suitable for engaging in meaningful conversations. Additionally, they benefit from reinforcement learning from human feedback, which helps improve their responses. They still have some limitations in reasoning and may require careful handling to avoid hallucinations and generating inappropriate content.``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f5567-f240-4598-bee6-c64e3c6de020",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring Conversational Capabilities with GPT-4 and ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e055065-a2a3-4ff6-91f0-9674d20971a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a8b87e-aac5-4630-91b0-10e0f631251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Paris is not only the capital of France but also one of the most famous cities in the world. It is located in the north-central part of the country, along the banks of the River Seine. Paris is known for its rich history, stunning architecture, art, fashion, and cuisine.\\n\\nThe city is divided into 20 administrative districts, known as arrondissements, each with its own unique character and attractions. Some of the most iconic landmarks in Paris include the Eiffel Tower, Notre-Dame Cathedral, Louvre Museum, Champs-Élysées, and the Arc de Triomphe.\\n\\nParis is often called the \"City of Love\" due to its romantic atmosphere, charming streets, and beautiful parks and gardens. It is also renowned as a global center for art and culture, with numerous museums, art galleries, and theaters.\\n\\nThe city is known for its vibrant café culture, where locals and tourists alike can enjoy a leisurely coffee or a meal while people-watching. Parisian cuisine is famous worldwide, with delicious pastries, bread, cheese, and wine being just a few highlights.\\n\\nParis is well-connected with an extensive public transportation system, including the metro, buses, and trains, making it easy to explore the city and its surroundings.\\n\\nOverall, Paris offers a unique blend of history, culture, and romance, making it a popular destination for travelers from around the world.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"I'd like to know more about the city you just mentioned.\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = llm(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5521b3-7b66-4762-ab1a-fb25dec4498a",
   "metadata": {},
   "source": [
    "# Build a News Articles Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9235bbd6-020d-4fa7-9abf-d6c4ac029fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d36a2-804a-4862-98f1-949044e374fe",
   "metadata": {},
   "source": [
    "And here are the steps described in more detail:\n",
    "\n",
    "1. Install required libraries: To get started, ensure you have the necessary libraries installed: `requests`, `newspaper3k`, and `langchain`.\n",
    "2. Scrape articles: Use the `requests` library to scrape the content of the target news articles from their respective URLs.\n",
    "3. Extract titles and text: Employ the `newspaper` library to parse the scraped HTML and extract the titles and text of the articles.\n",
    "4. Preprocess the text: Clean and preprocess the extracted texts to make them suitable for input to ChatGPT.\n",
    "5. Generate summaries: Utilize ChatGPT to summarize the extracted articles' text concisely.\n",
    "6. Output the results: Present the summaries along with the original titles, allowing users to grasp the main points of each article quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb47edd-fff9-4266-a896-ed73682d5307",
   "metadata": {},
   "source": [
    "![图片描述](pics/Building%20a%20News%20Articles%20Summarizer.avif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1049ee6-23a7-400d-bd87-95a23ae7d3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Meta claims its new AI supercomputer will set records\n",
      "Text: Ryan is a senior editor at TechForge Media with over a decade of experience covering the latest technology and interviewing leading industry figures. He can often be sighted at tech conferences with a strong coffee in one hand and a laptop in the other. If it's geeky, he’s probably into it. Find him on Twitter (@Gadget_Ry) or Mastodon (@gadgetry@techhub.social)\n",
      "\n",
      "Meta (formerly Facebook) has unveiled an AI supercomputer that it claims will be the world’s fastest.\n",
      "\n",
      "The supercomputer is called the AI Research SuperCluster (RSC) and is yet to be fully complete. However, Meta’s researchers have already begun using it for training large natural language processing (NLP) and computer vision models.\n",
      "\n",
      "RSC is set to be fully built in mid-2022. Meta says that it will be the fastest in the world once complete and the aim is for it to be capable of training models with trillions of parameters.\n",
      "\n",
      "“We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together,” wrote Meta in a blog post.\n",
      "\n",
      "“Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.”\n",
      "\n",
      "For production, Meta expects RSC will be 20x faster than Meta’s current V100-based clusters. RSC is also estimated to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.\n",
      "\n",
      "A model with tens of billions of parameters can finish training in three weeks compared with nine weeks prior to RSC.\n",
      "\n",
      "Meta says that its previous AI research infrastructure only leveraged open source and other publicly-available datasets. RSC was designed with the security and privacy controls in mind to allow Meta to use real-world examples from its production systems in production training.\n",
      "\n",
      "What this means in practice is that Meta can use RSC to advance research for vital tasks such as identifying harmful content on its platforms—using real data from them.\n",
      "\n",
      "“We believe this is the first time performance, reliability, security, and privacy have been tackled at such a scale,” says Meta.\n",
      "\n",
      "(Image Credit: Meta)\n",
      "\n",
      "Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022.\n",
      "\n",
      "Explore other upcoming enterprise technology events and webinars powered by TechForge here.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_url = \"https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "try:\n",
    "    response = session.get(article_url, headers=headers, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Text: {article.text}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to fetch article at {article_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08e2616-8f4f-4ff0-b75c-b3ae973d950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77cce2c7-a806-4ba3-9bdc-605647b87d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a222939-ca46-44db-862b-b6bfb37efbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta, formerly known as Facebook, has introduced its AI Research SuperCluster (RSC), an AI supercomputer that is expected to be the fastest in the world once completed in mid-2022. The RSC will be capable of training models with trillions of parameters and aims to enable the development of AI systems for real-time voice translations and collaborative experiences in the metaverse. Meta anticipates that the RSC will be 20 times faster than its current clusters, significantly improving training times for large-scale NLP workflows. The supercomputer was designed with security and privacy controls to allow Meta to use real-world examples from its production systems for research purposes.\n"
     ]
    }
   ],
   "source": [
    "# generate summary\n",
    "summary = chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "589a0812-705d-47b5-ab4d-5193a4ca9c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Meta (formerly Facebook) has unveiled an AI supercomputer called the AI Research SuperCluster (RSC) that it claims will be the world's fastest.\n",
      "- The RSC is currently being used for training large natural language processing (NLP) and computer vision models.\n",
      "- Once fully built in mid-2022, Meta aims for the RSC to be capable of training models with trillions of parameters and to be the fastest in the world.\n",
      "- Meta hopes that the RSC will enable the development of new AI systems for real-time voice translations and collaboration in the metaverse.\n",
      "- The RSC is expected to be 20x faster than Meta's current clusters, 9x faster at running the NVIDIA Collective Communication Library (NCCL), and 3x faster at training large-scale NLP workflows.\n",
      "- With the RSC, Meta can train models with tens of billions of parameters in three weeks compared to nine weeks previously.\n",
      "- The RSC was designed with security and privacy controls to allow Meta to use real-world examples from its production systems for research, such as identifying harmful content on its platforms.\n",
      "- Meta believes that the RSC is the first infrastructure to tackle performance, reliability, security, and privacy at such a scale.\n"
     ]
    }
   ],
   "source": [
    "# prepare template for prompt\n",
    "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists.\n",
    "\n",
    "Here's the article you need to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Now, provide a summarized version of the article in a bulleted list format.\n",
    "\"\"\"\n",
    "\n",
    "# format prompt\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "# generate summary\n",
    "summary = chat([HumanMessage(content=prompt)])\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe1388e4-f0e9-4492-a8ae-923cb51937c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "- Meta (formerly Facebook) has unveiled an AI supercomputer called the AI Research SuperCluster (RSC).\n",
      "- The RSC is expected to be the world's fastest supercomputer once complete.\n",
      "- Meta's researchers have already started using the RSC for training large natural language processing (NLP) and computer vision models.\n",
      "- The aim is for the RSC to be capable of training models with trillions of parameters.\n",
      "- Meta hopes that the RSC will enable the development of new AI systems for real-time voice translations and collaboration in the metaverse.\n",
      "- The RSC is estimated to be 20x faster than Meta's current clusters for production.\n",
      "- It is also expected to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.\n",
      "- Meta designed the RSC with security and privacy controls to use real-world examples from its production systems for research.\n",
      "- The RSC will be used to advance research in tasks such as identifying harmful content on Meta's platforms.\n",
      "\n",
      "French:\n",
      "- Meta (anciennement Facebook) a dévoilé un superordinateur d'IA appelé le SuperCluster de Recherche en IA (RSC).\n",
      "- Le RSC devrait être le superordinateur le plus rapide au monde une fois terminé.\n",
      "- Les chercheurs de Meta ont déjà commencé à utiliser le RSC pour former de grands modèles de traitement du langage naturel (NLP) et de vision par ordinateur.\n",
      "- L'objectif est que le RSC soit capable de former des modèles avec des milliards de paramètres.\n",
      "- Meta espère que le RSC permettra le développement de nouveaux systèmes d'IA pour les traductions vocales en temps réel et la collaboration dans le métaverse.\n",
      "- Le RSC devrait être 20 fois plus rapide que les clusters actuels de Meta pour la production.\n",
      "- On estime également qu'il sera 9 fois plus rapide pour exécuter la bibliothèque de communication collective NVIDIA (NCCL) et 3 fois plus rapide pour former des flux de travail NLP à grande échelle.\n",
      "- Meta a conçu le RSC avec des contrôles de sécurité et de confidentialité pour utiliser des exemples du monde réel de ses systèmes de production pour la recherche.\n",
      "- Le RSC sera utilisé pour faire avancer la recherche dans des tâches telles que l'identification de contenus nuisibles sur les plateformes de Meta.\n"
     ]
    }
   ],
   "source": [
    "# prepare template for prompt\n",
    "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists in French.\n",
    "\n",
    "Here's the article you need to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Now, provide a summarized version of the article in a bulleted list format, in both English and Chinese.\n",
    "\"\"\"\n",
    "\n",
    "# format prompt\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "# generate summary\n",
    "summary = chat([HumanMessage(content=prompt)])\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc917f-bafd-46ff-848c-1ce0dbb5f988",
   "metadata": {},
   "source": [
    "# Using the Open-Source GPT4All Model Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e45c4-7303-439a-bcbd-fb71f3d61ddb",
   "metadata": {},
   "source": [
    "## 1. Convert the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b83e72-8eb8-47c0-8c32-dd688a8ea8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#pip install -q langchain==0.0.152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ff983-0b38-473c-96a5-d28f617f0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "local_path = './models/gpt4all-lora-quantized-ggml.bin'\n",
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
    "\n",
    "# send a GET request to the URL to download the file.\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# open the file in binary mode and write the contents of the response\n",
    "# to it in chunks.\n",
    "with open(local_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c8119-9891-48c5-aa9d-a6692c1527da",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "cd llama.cpp && git checkout 2b26469\n",
    "python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d83fbc-9333-4ce4-8586-6df299a6acc2",
   "metadata": {},
   "source": [
    "## 2. Load the Model and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ce201-0f32-4b6a-b5b2-499629fd5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb6253-9e25-4f5b-941a-1212ab06a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d8db3-64a0-4285-88d3-d87fe82b21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb743fa-f388-445e-bafe-b00827688149",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c5ae6-d22f-45ae-9648-ad27e3eb9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16719aed-b5c1-4d6f-a402-c0c71b4621cd",
   "metadata": {},
   "source": [
    "# What other models can we use?  Popular LLM models compared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037eb0b-def2-4d0a-8eee-dee102bdb4cb",
   "metadata": {},
   "source": [
    "## Popular LLM models accessible to LangChain via API\n",
    "\n",
    "### GPT-3.5\n",
    "GPT-3.5 is a language model developed by OpenAI. Its turbo version (recommended by OpenAI over other variants) offers a more affordable option for generating human-like text through an API accessible via OpenAI endpoints. The model is optimized for chat applications while remaining powerful on other generative tasks and can process 96 languages. GPT-3.5-turbo has a 4096 tokens context length and is the most cost-effective option from the OpenAI collection with only $0.002 per 1000 tokens. It is possible to access this model’s API by using the gpt-3.5-turbo key while initializing either ChatOpenAI or OpenAI classes.\n",
    "\n",
    "\n",
    "### GPT-4 (Limited Beta)\n",
    "OpenAI's GPT-4 is a competent multimodal model with an undisclosed number of parameters or training procedures. It is the latest and most powerful model published by OpenAI, and the multi-modality enables the model to process both text and image as input. Unfortunately, It is not publicly available; however, it can be accessed by submitting your early access request through the OpenAI platform. The two variants of the model are gpt-4 and gpt-4-32k with different context lengths, 8192 and 32768 tokens, respectively.\n",
    "\n",
    "\n",
    "### Cohere Command \n",
    "The Cohere service provides a variety of models such as Command (command) for dialogue-like interactions, Generation (base) for generative tasks, Summarize (summarize-xlarge) for generating summaries, and more. The models are more expensive compared to OpenAI APIs, and different for each task—for example, $2.5 for generating 1K tokens. However, creating customized models for each task could lead to a more targeted model and improved outcomes in downstream tasks. The LangChain’s Cohere class makes it easy to access these models. Cohere(model=\"<MODEL_NAME>\", cohere_api_key=\"<API_KEY>\")\n",
    "\n",
    "💡\n",
    "You might see deprecated model names in the LangChain documentation. (like command-xlarge-20221108) Please refer to the Cohere documentation for the latest naming convention.\n",
    "\n",
    "### Jurassic-2\n",
    "The AI21’s Jurassic-2 is a language model with three sizes and different price points: Jumbo, Grande, and Large. The model sizes are not publicly available, but their documentation marks the Jumbo version as the most powerful model. They describe the models as general-purpose with excellent capability on every generative task. Their J2 model understands seven languages and can be fine-tuned on custom datasets. Getting your API key from the AI21 platform and using the AI21()class to access these models is possible.\n",
    "\n",
    "\n",
    "### StableLM\n",
    "StableLM Alpha is a language model developed by Stable Diffusion, which can be accessed via HuggingFace Hub (with the following id stabilityai/stablelm-tuned-alpha-3b) to host locally or Replicate API with a rate from $0.0002 to $0.0023 per second. So far, it comes in two sizes, 3 billion and 7 billion parameters. The weights for StableLM Alpha are available under CC BY-SA 4.0 license with commercial use access. The context length of StableLM is 4096 tokens.\n",
    "\n",
    "### Dolly-v2-12B\n",
    "Dolly-v2-12B is a language model created by Databricks, which can be accessed via HuggingFace Hub (with the following id databricks/dolly-v2-3b) to host locally or Replicate API with the same price range as mentioned in the previous subsection. It has 12 billion parameters and is available under an open source license for commercial use. The base model used for Dolly-v2-12B is Pythia-12B. \n",
    "\n",
    "### GPT4ALL\n",
    "GPT4ALL is based on meta’s LLaMA model with 7B parameters. It is a language model developed by Nomic-AI that can be accessed through GPT4ALL and Hugging Face Local Pipelines. The model is published with a GPL 3.0 open-source license. However, it is not free to use for commercial applications. It is available for researchers to use for their projects and experiments. We went through this model’s capability and usage process in the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296ec5e-4aba-482c-a500-4cf4a1e9b69e",
   "metadata": {},
   "source": [
    "## LLM Platforms that can integrate into LangChain\n",
    "\n",
    "### OpenAI\n",
    "OpenAI platform is one of the biggest companies focusing on large language models. By introducing their conversational model, ChatGPT, they were the first service to catch mainstream media attention on the potency of LLMs. They also provide a large variety of API endpoints for different NLP tasks with different price points. The LangChain library provides multiple classes for convenient access, examples of which we saw in previous lessons, like ChatGPT and GPT4 classes.\n",
    "\n",
    "### Hugging Face Hub\n",
    "Hugging Face is a company that develops natural language processing (NLP) technologies, including pre-trained language models, and offers a platform for developing and deploying NLP models. The platform hosts over 120k models and 20k datasets. They offer the Spaces service for researchers and developers to create a demo and showcase their model’s capabilities quickly. The platform hosts large-scale models such as StableLM by Stability AI, Dolly by DataBricks, or Camel by Writer. The HuggingFaceHub class takes care of downloading and initializing the models.\n",
    "\n",
    "This integration provides access to many models that are optimized for Intel CPUs using Intel® Extension for PyTorch library. The mentioned package can be applied to models with minimal code change. It enables the networks to take advantage of Intel®’s advanced architectural designs to significantly enhance CPU and GPU lines' performance. For example, the reports reveal a 3.8 speed up while running the BLOOMZ model (text-to-image) on the Intel® Xeon® 4s CPU compared to the previous generation with no changes in architecture/weights. When the mentioned optimization library was used alongside the 4th generation of Intel® Xeon® CPU, the inference speed rate increased nearly twofold to 6.5 times its original value. (online demo) Whisper and GPT-J are two other examples of widely recognized models that leverage these efficiency gains.\n",
    "\n",
    "### Amazon SageMakerEndpoint\n",
    "The Amazon SageMaker infrastructure enables users to train and host their machine-learning models easily. It is a high-performance and low-cost environment for experimenting and using large-scale models. The LangChain library provides a simple-to-use interface that simplifies the process of querying the deployed models. So, There is no need to write API codes for accessing the model. It is possible to load a model by using the endpoint_name which is the model’s unique name from SageMaker, followed by credentials_profile_name which is the name of the profile you want to use for authentication.\n",
    "\n",
    "### Hugging Face Local Pipelines\n",
    "Hugging Face Local Pipelines is a powerful tool that allows users to run Hugging Face models locally using the HuggingFacePipeline class. The Hugging Face Model Hub is home to an impressive collection of more than 120,000 models, 20,000 datasets, and 50,000 demo apps (Spaces) that are all publicly available and open source, making it easy for individuals to collaborate and build machine learning models together. To access these models, users can either utilize the local pipeline wrapper or call the hosted inference endpoints via the HuggingFaceHub class. Before getting started, the Transformers Python package must be installed. Once installed, users can load their desired model using the model_id and task and any additional model arguments. Finally, the model can be integrated into an LLMChain by creating a PromptTemplate and LLMChain object and running the input through it.\n",
    "\n",
    "### Azure OpenAI\n",
    "OpenAI’s models can also be accessed via Microsoft’s Azure platform. \n",
    "\n",
    "### AI21\n",
    "AI21 is a company that offers access to their powerful Jurassic-2 large language models through their API. The API provides access to their Jurassic-2 model, which has an impressive 178 billion parameters. The API comes at quite a reasonable cost of only $0.01 for every 1k tokens. Developers can easily interact with the AI21 models by creating prompts with LangChain that incorporate input variables. With this simple process, developers can take advantage of their powerful language processing capabilities.\n",
    "\n",
    "Cohere\n",
    "\n",
    "Cohere is a Canadian-based startup that specializes in natural language processing models that help companies enhance human-machine interactions. Cohere provides access to their Cohere xlarge model through API, which has 52 billion parameters. Their API pricing is based on embeddings and is set at $1 for every 1000 embeddings. Cohere provides an easy-to-follow installation process for their package, which is required to access their API. Using LangChain, developers can easily interact with Cohere models by creating prompts incorporating input variables, which can then be passed to the Cohere API to generate responses. \n",
    "\n",
    "Aleph Alpha\n",
    "\n",
    "Aleph Alpha is a company that offers a family of large language models known as the Luminous series. The Luminous family includes three models, namely Luminous-base, Luminous-extended, and Luminous-supreme, which vary in terms of complexity and capabilities. Aleph Alpha's pricing model is token-based, and the table provides the base prices per model for every 1000 input tokens. The Luminous-base model costs 0.03€ per 1000 input tokens, Luminous-extended costs 0.045€ per 1000 input tokens, Luminous-supreme costs 0.175€ per 1000 input tokens, and Luminous-supreme-control costs 0.21875€ per 1000 input tokens.\n",
    "\n",
    "Banana\n",
    "\n",
    "Banana is a machine learning infrastructure-focused company that provides developers with the tools to build machine learning models. Using LangChain, one can interact with Banana models by installing the Banana package, including an SDK for Python. Next, two following tokens are required: the BANANA_API_KEY and the YOUR_MODEL_KEY, which can be obtained from their platform. After setting the keys, we can create an object by providing the YOUR_MODEL_KEY. It is then possible to integrate the Banana model into an LLMChain by creating a PromptTemplate and LLMChain object and running the desired input through it.\n",
    "\n",
    "CerebriumAI\n",
    "\n",
    "CerebriumAI is an excellent alternative to AWS Sagemaker, providing access to several LLM models through its API. The available pre-trained LLM models include Whisper, MT0, FlanT5, GPT-Neo, Roberta, Pygmalion, Tortoise, and GPT4All. Developers create an instance of CerebriumAI by providing the endpoint URL and other relevant parameters such as max length, temperature, etc.\n",
    "\n",
    "DeepInfra\n",
    "\n",
    "DeepInfra is a unique API that offers a range of LLMs, such as distilbert-base-multilingual-cased, bert-base, whisper-large, gpt2, dolly-v2-12b, and more. It is connected to LangChain via API and runs on A100 GPUs that are optimized for inference performance and low latency. Compared to Replicate, DeepInfra's pricing is much more affordable, at $0.0005/second and $0.03/minute. With DeepInfra, we are given a 1-hour free trial of serverless GPU computing to experiment with different models.\n",
    "\n",
    "ForefrontAI\n",
    "\n",
    "ForefrontAI is a platform that allows users to fine-tune and utilize various open-source large language models like GPT-J, GPT-NeoX, T5, and more. The platform offers different pricing plans, including the Starter plan for $29/month, which comes with 5 million serverless tokens, 5 fine-tuned models, 1 user, and Discord support. With ForefrontAI, developers have access to various models that can be fine-tuned to suit our specific needs.\n",
    "\n",
    "GooseAI\n",
    "\n",
    "GooseAI is a fully managed NLP-as-a-Service platform that offers access to various models, including GPT-Neo, Fairseq, and GPT-J. The pricing for GooseAI is based on different model sizes and usage. For the 125M model, the base price for up to 25 tokens is $0.000035 per request, with an additional fee of $0.000001. To use GooseAI with LangChain, you need to install the openai package and set the Environment API Key, which can be obtained from GooseAI. Once you have the API key, you can create a GooseAI instance and define a Prompt Template for Question and Answer. The LLMChain can then be initiated, and you can provide a question to run the LLMChain.\n",
    "\n",
    "Llama-cpp\n",
    "\n",
    "Llama-cpp, a Python binding for llama.cpp, has been seamlessly integrated into the LangChain framework. This integration allows users to access a variety of LLM (Large Language Model) models offered by Llama-cpp, including LLaMA 🦙, Alpaca, GPT4All, Chinese LLaMA / Alpaca, Vigogne (French), Vicuna, Koala, OpenBuddy 🐶 (Multilingual), Pygmalion 7B, and Metharme 7B. With this integration, users have a wide range of options to choose from based on their specific language processing needs. By integrating Llama-cpp into LangChain, users can benefit from the powerful language models and generate humanistic and step-by-step responses to their input questions.\n",
    "\n",
    "Manifest\n",
    "\n",
    "Manifest is an integration tool that enhances the capabilities of LangChain, making it more powerful and user-friendly for language processing tasks. It acts as a bridge between LangChain and local Hugging Face models, allowing users to access and utilize these models within LangChain easily. Manifest has been seamlessly integrated into LangChain, providing users with enhanced capabilities for language processing tasks. To utilize Manifest within LangChain, users can follow the provided instructions, which involve installing the manifest-ml package and configuring the connection settings. Once integrated, users can leverage Manifest's functionalities alongside LangChain for a comprehensive language processing experience.\n",
    "\n",
    "Modal\n",
    "\n",
    "Modal is seamlessly integrated into LangChain, adding powerful cloud computing capabilities to the language processing workflow. While Modal does not provide any specific language models (LLMs), it serves as the infrastructure enabling LangChain to leverage serverless cloud computing. By integrating Modal into LangChain, users can directly harness the benefits of on-demand access to cloud resources from their Python scripts on their local computers. By installing the Modal client library and generating a new token, users can authenticate and establish a connection to the Modal server. In the LangChain example, a Modal LLM is instantiated using the endpoint URL, and a PromptTemplate is defined to structure the input. LangChain then executes the LLMChain with the specified prompt and runs a language processing task, such as answering a question.\n",
    "\n",
    "NLP Cloud\n",
    "\n",
    "NLP Cloud seamlessly integrates with LangChain, providing a comprehensive suite of high-performance pre-trained and custom models for a wide range of natural language processing (NLP) tasks. These models are designed for production use and can be accessed through a REST API. By executing the LLMChain with the specified prompt, users can seamlessly perform NLP tasks like answering questions.\n",
    "\n",
    "Petals\n",
    "\n",
    "Petals are seamlessly integrated into LangChain, enabling the utilization of over 100 billion language models within a decentralized architecture similar to BitTorrent. This notebook provides guidance on incorporating Petals into the LangChain workflow. Petals offer a diverse range of language models, and its integration with LangChain enhances natural language understanding and generation capabilities. Petals operate under a decentralized model, providing users with powerful language processing capabilities in a distributed environment.\n",
    "\n",
    "PipelineAI\n",
    "\n",
    "PipelineAI is seamlessly integrated into LangChain, allowing users to scale their machine-learning models in the cloud. Additionally, PipelineAI offers API access to a range of LLM (Large Language Model) models. It includes GPT-J, Stable Diffusion, ESRGAN, DALL·E, GPT-2, and GPT-Neo, each with its own specific model parameters and capabilities. PipelineAI empowers users to leverage the scalability and power of the cloud for their machine-learning workflows within the LangChain ecosystem.\n",
    "\n",
    "PredictionGuard\n",
    "\n",
    "PredictionGuard is seamlessly integrated into LangChain, providing users with a powerful wrapper for their language model usage. To begin using PredictionGuard within the LangChain framework, the predictionguard and LangChain libraries need to be installed. PredictionGuard can also be seamlessly integrated into LangChain's LLMChain for more advanced tasks. PredictionGuard enhances the LangChain experience by providing an additional layer of control and safety to language model outputs.\n",
    "\n",
    "PromptLayer OpenAI\n",
    "\n",
    "PredictionGuard is seamlessly integrated into LangChain, offering users enhanced control and management of their GPT prompt engineering. PromptLayer acts as a middleware between users' code and OpenAI's Python library, enabling the recording, tracking, and exploration of OpenAI API requests through the PromptLayer dashboard. To utilize PromptLayer with OpenAI, the 'promptlayer' package needs to be installed. Users can attach templates to requests, enabling the evaluation of different templates and models within the PromptLayer dashboard.\n",
    "\n",
    "Replicate\n",
    "\n",
    "Replicate is seamlessly integrated into LangChain, providing a wide range of LLM models for various applications. Some of the LLM models offered by Replicate include vicuna-13b, bark, speaker-transcription, stablelm-tuned-alpha-7b, Kandinsky-2, and stable-diffusion. These models cover diverse areas such as language generation, generative audio, speaker transcription, language modeling, and text-to-image generation. Each model has specific parameters and capabilities, enabling users to choose the most suitable model for their needs. Replicate provides flexible pricing options based on the computational resources required for running the models. Replication simplifies the deployment of custom machine-learning models at scale. Users can integrate Replicate into LangChain to interact with these models effectively.\n",
    "\n",
    "Runhouse\n",
    "\n",
    "Runhouse is seamlessly integrated into LangChain, providing powerful remote compute and data management capabilities across different environments and users. Runhouse offers the flexibility to host models on your own GPU infrastructure or leverage on-demand GPUs from cloud providers such as AWS, GCP, and Azure. Runhouse provides several LLM models that can be utilized within LangChain, such as gpt2 and google/flan-t5-small. Users can specify the desired hardware configuration. By combining Runhouse and LangChain, users can easily create advanced language model workflows, enabling efficient model execution and collaboration across different environments and users.\n",
    "\n",
    "StochasticAI\n",
    "\n",
    "StochasticAI aims to simplify the workflow of deep learning models within LangChain, providing users with an efficient and user-friendly environment for model interaction and deployment. It provides a streamlined process for the lifecycle management of Deep Learning models. StochasticAI's Acceleration Platform simplifies tasks such as model uploading, versioning, training, compression, and acceleration, ultimately facilitating the deployment of models into production. Within LangChain, users can interact with StochasticAI models effortlessly. The available LLM models from StochasticAI include FLAN-T5, GPT-J, Stable Diffusion 1, and Stable Diffusion 2. These models offer diverse capabilities for various language-related tasks.\n",
    "\n",
    "Writer\n",
    "\n",
    "The writer is seamlessly integrated into LangChain, providing users with a powerful platform for generating diverse language content. With Writer integration, LangChain users can effortlessly interact with a range of LLM models to meet their language generation needs. The available LLM models provided by Writer include Palmyra Small (128m), Palmyra 3B (3B), Palmyra Base (5B), Camel 🐪 (5B), Palmyra Large (20B), InstructPalmyra (30B), Palmyra-R (30B), Palmyra-E (30B), and Silk Road. These models offer different capacities for improving language understanding, generative pre-training, following instructions, and retrieval-augmented generation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5042f-929b-44ef-a0f8-334224e5789a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e966d-42f9-41e3-a4e0-1bd256340b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e62a0-9464-4e05-9813-2dd02de2eda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c152024-86b5-4199-87ed-b77ccfae545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe3559-0315-4cc1-9072-8997e47d2f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451248ad-2c9f-4732-a776-4db6ba066b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc7647-10df-4cc2-a859-f95b8bc78570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952d38e-97bd-41a9-96b6-83336bf8aefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105fe2d-6802-45e5-b7da-75edcc9c99a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf0d44-5077-4f5f-be82-26b59443ca63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
