{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36cf86f-dcf1-49a0-b363-3ca5c784c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cbb96a-8d1d-4c7c-aca9-0211b8ce3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前windows\n",
    "# 获取当前的 Conda 环境路径\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "\n",
    "# \".env\" 文件的绝对路径\n",
    "dotenv_path = os.path.join(conda_env_path, '.env')\n",
    "\n",
    "# 加载 \".env\" 文件\n",
    "_ = load_dotenv(dotenv_path, verbose=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e10f80-4951-4027-9f8c-87612384a8db",
   "metadata": {},
   "source": [
    "# Introduction to Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cf509-b0a1-4574-a31c-8d43faa07f1c",
   "metadata": {},
   "source": [
    "# Chains and Why They Are Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282491a-3ed3-4d3b-81a7-c801f3e83756",
   "metadata": {},
   "source": [
    "The chains are responsible for creating an end-to-end pipeline for using the language models. They will join the model, prompt, memory, parsing output, and debugging capability and provide an easy-to-use interface. A chain will 1) receive the user’s query as an input, 2) process the LLM’s response, and lastly, 3) return the output to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8af2a-7f82-4b28-aedc-1f98789513ca",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c40763c-2649-4c79-a9ff-38e5501e58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762300af-1082-46fa-87fd-357520760874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42489e86-2afa-4d17-80f8-8b00c5b83fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n\\nSynthetic'}, {'text': '\\n\\nWisdom'}, {'text': '\\n\\nAutomaton'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89142dcc-277f-42d9-9437-217b013cba9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWisdom', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nAutomaton', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 46, 'completion_tokens': 13, 'prompt_tokens': 33}, 'model_name': 'text-davinci-003'}, run=RunInfo(run_id=UUID('6e8e83d4-5352-429f-b390-42bd0d139643')))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96926ae-f8a4-403f-8079-907981b95ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nventilator'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Looking at the context of '{context}'. What is an appropriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"fan\", context=\"object\")\n",
    "# or llm_chain.run(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b136edcd-16ad-41e0-9fd6-18ffece9d70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAdmirer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"humans\")\n",
    "# or llm_chain.run(word=\"fan\", context=\"humans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d7f73-dcc9-4f83-ab75-855feb193cad",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c14e041-0176-4352-9cc5-651621fc7a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthetic',\n",
       " 'Manufactured',\n",
       " 'Imitation',\n",
       " 'Fabricated',\n",
       " 'Fake',\n",
       " 'Simulated',\n",
       " 'Artificial Intelligence',\n",
       " 'Automated',\n",
       " 'Constructed',\n",
       " 'Programmed',\n",
       " 'Mechanical',\n",
       " 'Processed',\n",
       " 'Algorithmic',\n",
       " 'Generated.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, output_parser=output_parser, input_variables=[]),\n",
    "    output_parser=output_parser)\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d82a8c-cfc2-4496-9dac-c76c02018cc6",
   "metadata": {},
   "source": [
    "## Conversational Chain (Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce778b5b-7b9a-4905-b329-313b355f72b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Synthetic, robotic, manufactured, simulated, computerized, programmed, man-made, fabricated, contrived, and artificial.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"List all possible words as substitute for 'artificial' as comma separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaaa7cf1-5d25-4d68-af5a-a007f806d4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Automated, cybernetic, mechanized, and engineered.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17347a80-d003-49a7-976a-33c0cfc97543",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20b9ca-6fca-49b9-9b2e-0bfccb3b5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b1693-7ccf-4ccf-89e1-00879abeca43",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0487d246-894b-4aaf-9912-7eb02d9ad07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Synthetic, Imitation, Manufactured, Fabricated, Simulated, Fake, Artificial, Constructed, Computerized, Programmed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29ccd1-7bc7-45c8-9e99-b215c81cc089",
   "metadata": {},
   "source": [
    "## Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f24fca7c-011f-4ec0-8453-fb436dc22e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db90831-2317-4cae-abcd-27b4fe2a3fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Artificial means something that is not natural or made by humans, but rather created or produced by artificial means.\n",
      "\n",
      "Synthetic\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377112fc-c837-4ff6-8ca5-088504b2e9b2",
   "metadata": {},
   "source": [
    "# Create a YouTube Video Summarizer Using Whisper and LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331567aa-d7da-44ba-8957-1967f25fc60a",
   "metadata": {},
   "source": [
    "![图片描述](pics/youtube_video_summarizer.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c3fe00-7019-4d2a-bea0-da7a6617cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-req-build-a6zw0uj_' did not run successfully.\n",
      "  exit code: 128\n",
      "  \n",
      "  [1 lines of output]\n",
      "  fatal: unable to access 'https://github.com/openai/whisper.git/': Recv failure: Connection was reset\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-req-build-a6zw0uj_' did not run successfully.\n",
      "exit code: 128\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q yt_dlp\n",
    "!pip install -q git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dadeec-a041-49d2-920c-177dfd1c9665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0b8a0-ee6c-44c1-9975-0672f91929ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4eeed4-f1e6-45d7-96d2-5775e51d868d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f240d87-ed33-4198-ad95-c45264f1404f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52620d-cc69-45b8-bfd1-d9626c1fa862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee2e75-6b93-467a-9069-cae58d57b360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cf80e-e32d-4c5b-a80d-57653c3c0e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c182f-4afa-40f2-acd7-3b2c74e2e7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c887c-0fd8-4bbe-895c-1cc51e34912b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17326f2-1de5-4b9f-8246-cfd39f18d20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4c01e-6bb0-4898-929c-c2832ebb5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
