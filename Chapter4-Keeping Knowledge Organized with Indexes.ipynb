{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a0d31d-2a90-4ac0-a6ef-9a07a28c9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89309a2-cdde-460d-89c1-215e3c807137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前windows\n",
    "# 获取当前的 Conda 环境路径\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "\n",
    "# \".env\" 文件的绝对路径\n",
    "dotenv_path = os.path.join(conda_env_path, '.env')\n",
    "\n",
    "# 加载 \".env\" 文件\n",
    "_ = load_dotenv(dotenv_path, verbose=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e0859-b083-4559-8efb-35428f11a707",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intro to Keeping Knowledge Organized with Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c6861-adaa-476e-9d88-2981a4c5a92d",
   "metadata": {},
   "source": [
    "- Exploring The Role of LangChain's Indexes and Retrievers: To kick off the module, we introduce the Deep Lake database and its seamless integration with the LangChain library. This lesson highlights the benefits of utilizing Deep Lake, including the ability to retrieve pertinent documents for contextual use. Additionally, we delve into the limitations of this approach and present solutions to overcome them.\n",
    "- Streamlined Data Ingestion: Text, PyPDF, Selenium URL Loaders, and Google Drive Sync: The LangChain library offers a variety of helper classes designed to facilitate data loading and extraction from diverse sources. Regardless of whether the information originates from a PDF file or website content, these classes streamline the process of handling different data formats.\n",
    "- What are Text Splitters and Why They are Useful: The length of the contents may vary depending on their source. For instance, a PDF file containing a book may exceed the input window size of the model, making it incompatible with direct processing. However, splitting the large text into smaller segments will allow us to use the most relevant chunk as the context instead of expecting the model to comprehend the whole book and answer a question. This lesson will thoroughly explore different approaches that enable us to accomplish this objective.\n",
    "- Exploring the World of Embeddings: Embeddings are high-dimensional vectors that capture semantic information. Large language models can transform textual data into embedding space, allowing for versatile representations across languages. These embeddings serve as valuable tools to identify relevant information by quantifying the distance between data points, thereby indicating closer semantic meaning for points closer together. The LangChain integration provides necessary functions for both transforming and calculating similarities.\n",
    "- Build a Customer Support Question Answering Chatbot: This practical example demonstrates the utilization of a website's content as supplementary context for a chatbot to respond to user queries effectively. The code implementation involves employing the mentioned data loaders, storing the corresponding embeddings in the Deep Lake dataset, and ultimately retrieving the most pertinent documents based on the user's question.\n",
    "- Conversation Intelligence: Gong.io Open-Source Alternative AI Sales Assistant: In this lesson, we will explore how LangChain, Deep Lake, and GPT-4 can be used to develop a sales assistant able to give advice to salesman, taking into considerations internal guidelines.\n",
    "- FableForge: Creating Picture Books with OpenAI, Replicate, and Deep Lake: In this final lesson, we are going to delve into a use case of AI technology in the creative domain of children's picture book creation in a project called \"FableForge\", leveraging both OpenAI GPT-3.5 LLM for writing the story and Stable Diffusion for generating images for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197051f-3d61-4b5e-857c-d436cd4ca06b",
   "metadata": {},
   "source": [
    "# Exploring the Role of Langchain's Indexes and Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00dc1634-f0c7-4aa9-b11c-f199a7fd18c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# text to write to a local file\n",
    "# taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
    "text = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
    "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
    "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
    "it says will help businesses “generate text, images, code, videos, audio, and more from\n",
    "simple natural language prompts.”\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
    "Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\n",
    "PaLM is a flexible system that can potentially carry out all sorts of text generation and\n",
    "editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\n",
    "example, or you could use it for tasks like summarizing text or even writing code.\n",
    "(It’s similar to features Google also announced today for its Workspace apps like Google\n",
    "Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# write text to local file\n",
    "with open(\"source/my_file.txt\", \"w\") as file:\n",
    "    file.write(text)\n",
    "\n",
    "# use TextLoader to load text from local file\n",
    "loader = TextLoader(\"source/my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fcde22-16aa-4d46-8c37-dbfecf4bf4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# create a text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "# split documents into chunks\n",
    "docs = text_splitter.split_documents(docs_from_file)\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d67c0807-b7dc-480f-979f-8ae4ff3b780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d875ef3-c9e8-44ea-bb6c-6dc6349f4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bff5ffd-4f56-4a6b-8dc3-5e63361b664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://bettermaxfeng/langchain_course_indexers_retrievers', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (2, 1536)  float32   None   \n",
      "    id        text      (2, 1)      str     None   \n",
      " metadata     json      (2, 1)      str     None   \n",
      "   text       text      (2, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e9ff15b0-407b-11ee-b60b-d89c6787905c',\n",
       " 'e9ff15b1-407b-11ee-98a7-d89c6787905c']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "# Before executing the following code, make sure to have your\n",
    "# Activeloop key saved in the “ACTIVELOOP_TOKEN” environment variable.\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = os.environ[\"ACTIVELOOP-ORG-ID\"]\n",
    "my_activeloop_dataset_name = \"langchain_course_indexers_retrievers\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c9f406-a571-49ab-a5cb-f6ff248b00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dda5d3a-5aab-4df7-8b18-c2824314c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "\tllm=OpenAI(model=\"text-davinci-003\"),\n",
    "\tchain_type=\"stuff\",\n",
    "\tretriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bb6171-a462-4fce-86bc-5c8dedbcdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google plans to challenge OpenAI by offering developers access to one of its most advanced AI language models, called PaLM.\n"
     ]
    }
   ],
   "source": [
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e8ecf-cf9c-46dc-818b-a2be0b38681c",
   "metadata": {},
   "source": [
    "A `DocumentCompressor` abstraction has been introduced to address this issue, allowing compress_documents on the retrieved documents.\n",
    "\n",
    "The `ContextualCompressionRetriever` is a wrapper around another retriever in LangChain. It takes a base retriever and a `DocumentCompressor` and automatically compresses the retrieved documents from the base retriever. This means that only the most relevant parts of the retrieved documents are returned, given a specific query.\n",
    "\n",
    "A popular compressor choice is the `LLMChainExtractor`, which uses an LLMChain to extract only the statements relevant to the query from the documents. To improve the retrieval process, a ContextualCompressionRetriever is used, wrapping the base retriever with an LLMChainExtractor. The LLMChainExtractor iterates over the initially returned documents and extracts only the content relevant to the query. \n",
    "\n",
    "Here's an example of how to use `ContextualCompressionRetriever` with `LLMChainExtractor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbc5ee8f-4d65-453e-9545-f92eb78c03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# create GPT3 wrapper\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# create compressor for the retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "\tbase_compressor=compressor,\n",
    "\tbase_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "282cdd42-4992-4292-ad7e-d0e5102c79ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chains\\llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google is offering developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses “generate text, images, code, videos, audio, and more from simple natural language prompts.”\n"
     ]
    }
   ],
   "source": [
    "# retrieving compressed documents\n",
    "retrieved_docs = compression_retriever.get_relevant_documents(\n",
    "\t\"How Google plans to challenge OpenAI?\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75739202-77d1-4b80-b05d-e81ef042600d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Streamlined Data Ingestion: Text, PyPDF,  Selenium URL Loaders, and Google Drive Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ec61e-0e9d-4a91-bef1-48eb9f03402f",
   "metadata": {},
   "source": [
    "## TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c37bf5-a209-4146-a790-46ba3abadd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('file_path.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98885ff-edac-4ee9-9d4e-ebf820f58259",
   "metadata": {},
   "source": [
    "You can use the `encoding` argument to change the encoding type. (For example:  `encoding=\"ISO-8859-1\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d664be-307b-4f71-a595-0963f93cb148",
   "metadata": {},
   "source": [
    "## PyPDFLoader (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b10f13b-bfa4-4d9d-adc5-37bf52d8e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebe9845-3479-48da-a521-879668717447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction\\nYuxuan Liang1;2, Songyu Ke3;2, Junbo Zhang2;4, Xiuwen Yi4;2, Yu Zheng2;1;3;4\\n1School of Computer Science and Technology, Xidian University, Xi’an, China\\n2Urban Computing Business Unit, JD Finance, Beijing, China\\n3Zhiyuan College, Shanghai Jiao Tong University, Shanghai, China\\n4School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China\\nfyuxliang, songyu-ke, msjunbozhang, xiuwyi, msyuzhengg@outlook.com\\nAbstract\\nNumerous sensors have been deployed in different\\ngeospatial locations to continuously and coopera-\\ntively monitor the surrounding environment, such\\nas the air quality. These sensors generate multiple\\ngeo-sensory time series, with spatial correlations\\nbetween their readings. Forecasting geo-sensory\\ntime series is of great importance yet very chal-\\nlenging as it is affected by many complex factors,\\ni.e., dynamic spatio-temporal correlations and ex-\\nternal factors. In this paper, we predict the read-\\nings of a geo-sensor over several future hours by\\nusing a multi-level attention-based recurrent neural\\nnetwork that considers multiple sensors’ readings,\\nmeteorological data, and spatial data. More specif-\\nically, our model consists of two major parts: 1) a\\nmulti-level attention mechanism to model the dy-\\nnamic spatio-temporal dependencies. 2) a general\\nfusion module to incorporate the external factors\\nfrom different domains. Experiments on two types\\nof real-world datasets, viz., air quality data and wa-\\nter quality data, demonstrate that our method out-\\nperforms nine baseline methods.\\n1 Introduction\\nThere are massive sensors, such as meteorological sites, that\\nhave been deployed in the physical world. Each of them has\\na unique geospatial location, constantly generating time se-\\nries readings. A group of sensors collectively monitor the\\nenvironment of a spatial region, with the spatial correlation\\nbetween their readings. We call such sensors’ readings geo-\\nsensory time series. Additionally, it is common that one sen-\\nsor generates multiple kinds of geo-sensory time series as it\\nmonitors different target conditions simultaneously. For ex-\\nample, as shown in Figure 1(a), the loop detectors in roads\\nreport timely readings about the vehicles passing by as well\\nas their travel speed. Figure 1(b) presents that the sensors\\ngenerate three different chemical indexes about water quality\\nevery 5 minutes. Besides monitoring, there is a rising demand\\nfor geo-sensory time series prediction, e.g., trafﬁc prediction.\\nHowever, forecasting geo-sensory time series is very chal-\\nlenging, affected by the two following complex factors:\\nSensors\\nRoads\\n(a) Loop detectors over road networks\\nVolume: 32\\nSpeed: 50km/h\\n(b) Urban water monitoring system (c) Illustration of dynamic spatio-\\ntemporal correlationSensors\\nPipelinesS4 S2S3Time\\nS1Spatial \\ncorrelationTemporal \\ncorrelation\\nt1t3t4\\nt2\\nt1t2t3t4\\nt1t2\\nt1t2t3t5Sudden change\\nRC: 0.84\\npH: 7.1 \\nTurbidity: 0.54Figure 1: (a)-(b) Examples of geo-sensory time series. (c) The future\\nreadings of a sensor depends on its past readings and that of nearby\\nsensors, where the weights are changing over the location and time.\\n1)Dynamic spatio-temporal correlations.\\n\\x0fComplex inter-sensor correlations. Figure 1(c) shows\\nthe spatial correlation between different sensors’ time\\nseries is highly dynamic, changing over time. Moreover,\\ngeo-sensory time series varies by locations non-linearly.\\nWhen modeling dynamic pairwise correlation, classi-\\ncal methods (e.g., probabilistic graphical models [Koller\\nand Friedman, 2009 ]) have extremely heavy computa-\\ntional cost due to their massive parameters.\\n\\x0fDynamic intra-sensor correlations. First, a geo-sensory\\ntime series usually follows a periodic pattern (e.g., S1in\\nFigure 1(c)), which changes over time and varies geo-\\ngraphically [Zhang et al., 2017 ]. Second, sensors’ read-\\nings sometimes ﬂuctuate tremendously and suddenly\\nchange, quickly decreasing the impact of their previous' metadata={'source': 'source/GeoMan论文.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"source/GeoMan论文.pdf\") \n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ff27f-c235-4c99-9a57-24d3c8353286",
   "metadata": {},
   "source": [
    "## SeleniumURLLoader (URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "525e1049-80f9-4f73-acf8-6decd9ffc318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q unstructured selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f783139-69cb-4801-8720-33bd1b0ecc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"OPENASSISTANT TAKES ON CHATGPT!\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nWatch Later\\n\\nShare\\n\\nCopy link\\n\\nTap to unmute\\n\\nIf playback doesn't begin shortly, try restarting your device.\\n\\nUp next\\n\\nLiveUpcoming\\n\\nPlay now\\n\\nYou're signed out\\n\\nVideos that you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\\n\\nMachine Learning Street Talk\\n\\nSubscribe\\n\\nSubscribed\\n\\nSwitch camera\\n\\nShare\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\n2:19\\n\\n2:19 / 59:51\\n\\nWatch full video\\n\\n•\\n\\nScroll for details\\n\\nNaN / NaN\\n\\nSearch\" metadata={'source': 'https://www.youtube.com/watch?v=TFa539R09EQ&t=139s'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=TFa539R09EQ&t=139s\",\n",
    "    \"https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66713556-a405-4462-afeb-35635825b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SeleniumURLLoader(urls=urls, browser=\"firefox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05621c8d-3555-4209-8523-42bbb48adfd4",
   "metadata": {},
   "source": [
    "## Google Drive loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc9788ee-6a1f-46e0-93bf-16d718275a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38299bdc-8462-4779-a351-3140e12c4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GoogleDriveLoader(\n",
    "    folder_id=\"your_folder_id\",\n",
    "    recursive=False  # Optional: Fetch files from subfolders recursively. Defaults to False.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d124dd-c7ee-4261-8a2b-05d903b63f65",
   "metadata": {},
   "source": [
    "- Folder: `https://drive.google.com/drive/u/0/folders/{folder_id}`\n",
    "- Document: `https://docs.google.com/document/d/{document_id}/edit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9ce42-8849-4170-ac79-fa86a9d9b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ab711-38fd-4659-abd6-6e328f933fc4",
   "metadata": {},
   "source": [
    "## What are Text Splitters and Why They are Useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4d0b8-3561-42c1-ac40-3b1be5f2ea97",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "- Reduced hallucination: By providing a source document, the LLM is more likely to generate content based on the given information, reducing the chances of creating false or irrelevant information.\n",
    "- Increased accuracy: With a reliable source document, the LLM can generate more accurate answers, especially in use cases where accuracy is crucial.\n",
    "- Verifiable information: Users can cross-check the generated content with the source document to ensure the information is accurate and reliable.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Limited scope: Relying on a single document may limit the scope of the generated content, as the LLM will only have access to the information provided in the document.\n",
    "- Dependence on document quality: The accuracy of the generated content heavily depends on the quality and reliability of the source document. The LLM will likely generate incorrect or misleading content if the document contains inaccurate or biased information.\n",
    "- Inability to eliminate hallucination completely: Although providing a document as a base reduces the chances of hallucination, it does not guarantee that the LLM will never generate false or irrelevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2542fe-cbde-4d98-835c-e51aa7470fef",
   "metadata": {},
   "source": [
    "## Customizing Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02626b-382b-4ee7-b8af-8b9296674d8f",
   "metadata": {},
   "source": [
    "At a high level, text splitters follow these steps:\n",
    "\n",
    "- Divide the text into small, semantically meaningful chunks (often sentences).\n",
    "- Combine these small chunks into a larger one until a specific size is reached (determined by a particular function).\n",
    "- Once the desired size is attained, separate that chunk as an individual piece of text, then start forming a new chunk with some overlap to maintain context between segments.\n",
    "\n",
    "Consequently, there are two primary dimensions to consider when customizing your text splitter:\n",
    "\n",
    "- The method used to split the text\n",
    "- The approach for measuring chunk size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d302a18-db1a-4c46-9cee-2e8c90ba5a34",
   "metadata": {},
   "source": [
    "### Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd7f957-6ee6-4973-a7f3-452598ec1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"source/The One Page Linux Manual.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a4dda31-1d6d-4250-bab8-2b0b9e69be97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 2, first text: page_content='THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\\nVersion 3.0 May 1999 squadron@powerup.com.au\\nStarting & Stopping\\nshutdown -h now Shutdown the system now and do not\\nreboot\\nhalt Stop all processes - same as above\\nshutdown -r 5 Shutdown the system in 5 minutes and\\nreboot\\nshutdown -r now Shutdown the system now and reboot\\nreboot Stop all processes and then reboot - same\\nas above\\nstartx Start the X system\\nAccessing & mounting file systems\\nmount -t iso9660 /dev/cdrom\\n/mnt/cdromMount the device cdrom\\nand call it cdrom under the\\n/mnt directory\\nmount -t msdos /dev/hdd\\n/mnt/ddriveMount hard disk “d” as a\\nmsdos file system and call\\nit ddrive under the /mnt\\ndirectory\\nmount -t vfat /dev/hda1\\n/mnt/cdriveMount hard disk “a” as a\\nVFAT file system and call it\\ncdrive under the /mnt\\ndirectory\\numount /mnt/cdrom Unmount the cdrom\\nFinding files and text within files\\nfind / -name  fname Starting with the root directory, look\\nfor the file called fname\\nfind / -name ”*fname* ” Starting with the root directory, look\\nfor the file containing the string fname\\nlocate missingfilename Find a file called missingfilename\\nusing the locate command - this\\nassumes you have already used the\\ncommand updatedb (see next)\\nupdatedb Create or update the database of files\\non all file systems attached to the linux\\nroot directory\\nwhich missingfilename Show the subdirectory containing the\\nexecutable file  called missingfilename\\ngrep textstringtofind\\n/dirStarting with the directory called dir ,\\nlook for and list all files containing\\ntextstringtofind\\nThe X Window System\\nxvidtune Run the X graphics tuning utility\\nXF86Setup Run the X configuration menu with\\nautomatic probing of graphics cards\\nXconfigurator Run another X configuration menu with\\nautomatic probing of graphics cards\\nxf86config Run a text based X configuration menu\\nMoving, copying, deleting & viewing files\\nls -l List files in current directory using\\nlong format\\nls -F List files in current directory and\\nindicate the file type\\nls -laC List all files in current directory in\\nlong format and display in columnsrm name Remove a file or directory called\\nname\\nrm -rf name Kill off an entire directory and all it’s\\nincludes files and subdirectories\\ncp filename\\n/home/dirnameCopy the file called filename to the\\n/home/dirname directory\\nmv filename\\n/home/dirnameMove the file called filename to the\\n/home/dirname directory\\ncat filetoview Display the file called filetoview\\nman -k keyword Display man pages containing\\nkeyword\\nmore filetoview Display the file called filetoview one\\npage at a time, proceed to next page\\nusing the spacebar\\nhead filetoview Display the first 10 lines of the file\\ncalled filetoview\\nhead -20 filetoview Display the first 20 lines of the file\\ncalled filetoview\\ntail filetoview Display the last 10 lines of the file\\ncalled filetoview\\ntail -20 filetoview Display the last 20 lines of the file\\ncalled filetoview\\nInstalling software for Linux\\nrpm -ihv name.rpm Install the rpm package called name\\nrpm -Uhv name.rpm Upgrade the rpm package called\\nname\\nrpm -e package Delete the rpm package called\\npackage\\nrpm -l package List the files in the package called\\npackage\\nrpm -ql package List the files and state the installed\\nversion of the package called\\npackage\\nrpm -i --force package Reinstall the rpm package called\\nname having deleted parts of it (not\\ndeleting using rpm -e)\\ntar -zxvf archive.tar.gz or\\ntar -zxvf archive.tgzDecompress the files contained in\\nthe zipped and tarred archive called\\narchive\\n./configure Execute the script preparing the\\ninstalled files for compiling\\nUser Administration\\nadduser accountname Create a new user call accountname\\npasswd accountname Give accountname a new password\\nsu Log in as superuser from current login\\nexit Stop being superuser and revert to\\nnormal user\\nLittle known tips and tricks\\nifconfig List ip addresses for all devices on\\nthe machine\\napropos subject List manual pages for subject\\nusermount Executes graphical application for\\nmounting and unmounting file\\nsystems' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "Preview:\n",
      "THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\n",
      "Version 3.0 May 1999 squadron@powerup.com.au\n",
      "Starting & Stopping\n",
      "shutdown -h now Shutdown the system now and do not\n",
      "reboot\n",
      "halt Stop all processes - same as above\n",
      "shutdown -r 5 Shutdown the system in 5 minutes and\n",
      "reboot\n",
      "shutdown -r now Shutdown the system now and reboot\n",
      "reboot Stop all processes and then reboot - same\n",
      "as above\n",
      "startx Start the X system\n",
      "Accessing & mounting file systems\n",
      "mount -t iso9660 /dev/cdrom\n",
      "/mnt/cdromMount the device cdrom\n",
      "and call it cdrom under the\n",
      "/mnt directory\n",
      "mount -t msdos /dev/hdd\n",
      "/mnt/ddriveMount hard disk “d” as a\n",
      "msdos file system and call\n",
      "it ddrive under the /mnt\n",
      "directory\n",
      "mount -t vfat /dev/hda1\n",
      "/mnt/cdriveMount hard disk “a” as a\n",
      "VFAT file system and call it\n",
      "cdrive under the /mnt\n",
      "directory\n",
      "umount /mnt/cdrom Unmount the cdrom\n",
      "Finding files and text within files\n",
      "find / -name  fname Starting with the root directory, look\n",
      "for the file called fname\n",
      "find / -name ”*fname* ” Starting with the root directory, look\n",
      "for the file containing the string fname\n",
      "locate missingfilename Find a file called missingfilename\n",
      "using the locate command - this\n",
      "assumes you have already used the\n",
      "command updatedb (see next)\n",
      "updatedb Create or update the database of files\n",
      "on all file systems attached to the linux\n",
      "root directory\n",
      "which missingfilename Show the subdirectory containing the\n",
      "executable file  called missingfilename\n",
      "grep textstringtofind\n",
      "/dirStarting with the directory called dir ,\n",
      "look for and list all files containing\n",
      "textstringtofind\n",
      "The X Window System\n",
      "xvidtune Run the X graphics tuning utility\n",
      "XF86Setup Run the X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "Xconfigurator Run another X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "xf86config Run a text based X configuration menu\n",
      "Moving, copying, deleting & viewing files\n",
      "ls -l List files in current directory using\n",
      "long format\n",
      "ls -F List files in current directory and\n",
      "indicate the file type\n",
      "ls -laC List all files in current directory in\n",
      "long format and display in columnsrm name Remove a file or directory called\n",
      "name\n",
      "rm -rf name Kill off an entire directory and all it’s\n",
      "includes files and subdirectories\n",
      "cp filename\n",
      "/home/dirnameCopy the file called filename to the\n",
      "/home/dirname directory\n",
      "mv filename\n",
      "/home/dirnameMove the file called filename to the\n",
      "/home/dirname directory\n",
      "cat filetoview Display the file called filetoview\n",
      "man -k keyword Display man pages containing\n",
      "keyword\n",
      "more filetoview Display the file called filetoview one\n",
      "page at a time, proceed to next page\n",
      "using the spacebar\n",
      "head filetoview Display the first 10 lines of the file\n",
      "called filetoview\n",
      "head -20 filetoview Display the first 20 lines of the file\n",
      "called filetoview\n",
      "tail filetoview Display the last 10 lines of the file\n",
      "called filetoview\n",
      "tail -20 filetoview Display the last 20 lines of the file\n",
      "called filetoview\n",
      "Installing software for Linux\n",
      "rpm -ihv name.rpm Install the rpm package called name\n",
      "rpm -Uhv name.rpm Upgrade the rpm package called\n",
      "name\n",
      "rpm -e package Delete the rpm package called\n",
      "package\n",
      "rpm -l package List the files in the package called\n",
      "package\n",
      "rpm -ql package List the files and state the installed\n",
      "version of the package called\n",
      "package\n",
      "rpm -i --force package Reinstall the rpm package called\n",
      "name having deleted parts of it (not\n",
      "deleting using rpm -e)\n",
      "tar -zxvf archive.tar.gz or\n",
      "tar -zxvf archive.tgzDecompress the files contained in\n",
      "the zipped and tarred archive called\n",
      "archive\n",
      "./configure Execute the script preparing the\n",
      "installed files for compiling\n",
      "User Administration\n",
      "adduser accountname Create a new user call accountname\n",
      "passwd accountname Give accountname a new password\n",
      "su Log in as superuser from current login\n",
      "exit Stop being superuser and revert to\n",
      "normal user\n",
      "Little known tips and tricks\n",
      "ifconfig List ip addresses for all devices on\n",
      "the machine\n",
      "apropos subject List manual pages for subject\n",
      "usermount Executes graphical application for\n",
      "mounting and unmounting file\n",
      "systems\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f'length: {len(texts)}, first text: {texts[0]}')\n",
    "\n",
    "# print (f\"You have {len(texts)} documents\")\n",
    "print (\"Preview:\")\n",
    "print (texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d16d0-f975-40b6-b516-b305fe32cc6b",
   "metadata": {},
   "source": [
    "Finding the best chunk size for your project means going through a few steps. \n",
    "- First, clean up your data by getting rid of anything that's not needed, like HTML tags from websites.\n",
    "- Then, pick a few different chunk sizes to test. The best size will depend on what kind of data you're working with and the model you're using.\n",
    "- Finally, test out how well each size works by running some queries and comparing the results.\n",
    "- You might need to try a few different sizes before finding the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b35858-5b74-4311-8c80-d8508bb5f134",
   "metadata": {},
   "source": [
    "### Recursive Character Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81a62d-3988-4df6-9378-9a1f5a79d334",
   "metadata": {},
   "source": [
    "- chunk_size : The maximum size of the chunks, as measured by the length_function (default is 100).\n",
    "- chunk_overlap: The maximum overlap between chunks to maintain continuity between them (default is 20).\n",
    "- length_function: parameter is used to calculate the length of the chunks. By default, it is set to len, which counts the number of characters in a chunk. However, you can also pass a token counter or any other function that calculates the length of a chunk based on your specific requirements.\n",
    "\n",
    "Using a token counter instead of the default `len` function can benefit specific scenarios, such as when working with language models with token limits. For example, OpenAI's GPT-3 has a token limit of 4096 tokens per request, so you might want to count tokens instead of characters to better manage and optimize your requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aeefc16-e38d-4d6f-ad50-b1dbc8eb229e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='THE ONE     PAGE LINUX MANUALA summary of useful' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='of useful Linux commands' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Version 3.0 May 1999 squadron@powerup.com.au' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Starting & Stopping' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='shutdown -h now Shutdown the system now and do' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='and do not' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='reboot\\nhalt Stop all processes - same as above' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='shutdown -r 5 Shutdown the system in 5 minutes' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='5 minutes and' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='reboot' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='shutdown -r now Shutdown the system now and' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='now and reboot' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='reboot Stop all processes and then reboot - same' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='as above\\nstartx Start the X system' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Accessing & mounting file systems' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='mount -t iso9660 /dev/cdrom' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/mnt/cdromMount the device cdrom' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='and call it cdrom under the\\n/mnt directory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='mount -t msdos /dev/hdd' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/mnt/ddriveMount hard disk “d” as a' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='msdos file system and call' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='it ddrive under the /mnt\\ndirectory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='directory\\nmount -t vfat /dev/hda1' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/mnt/cdriveMount hard disk “a” as a' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='VFAT file system and call it' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='cdrive under the /mnt\\ndirectory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='directory\\numount /mnt/cdrom Unmount the cdrom' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Finding files and text within files' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='find / -name  fname Starting with the root' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the root directory, look' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='for the file called fname' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='find / -name ”*fname* ” Starting with the root' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the root directory, look' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='for the file containing the string fname' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='locate missingfilename Find a file called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called missingfilename' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='using the locate command - this' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='assumes you have already used the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='command updatedb (see next)' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='updatedb Create or update the database of files' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='on all file systems attached to the linux' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='root directory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='which missingfilename Show the subdirectory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='containing the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='executable file  called missingfilename' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='grep textstringtofind' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/dirStarting with the directory called dir ,' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='look for and list all files containing' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='textstringtofind\\nThe X Window System' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='xvidtune Run the X graphics tuning utility' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='XF86Setup Run the X configuration menu with' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='automatic probing of graphics cards' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Xconfigurator Run another X configuration menu' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='menu with' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='automatic probing of graphics cards' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='xf86config Run a text based X configuration menu' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Moving, copying, deleting & viewing files' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ls -l List files in current directory using' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='long format' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ls -F List files in current directory and' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='indicate the file type' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ls -laC List all files in current directory in' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='long format and display in columnsrm name Remove' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Remove a file or directory called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='name' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rm -rf name Kill off an entire directory and all' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='and all it’s' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='includes files and subdirectories\\ncp filename' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirnameCopy the file called filename to the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirname directory\\nmv filename' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirnameMove the file called filename to the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirname directory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='cat filetoview Display the file called filetoview' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='man -k keyword Display man pages containing' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='keyword' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='more filetoview Display the file called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview one' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='page at a time, proceed to next page' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='using the spacebar' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='head filetoview Display the first 10 lines of the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='of the file' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='head -20 filetoview Display the first 20 lines of' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='lines of the file' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tail filetoview Display the last 10 lines of the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='of the file' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tail -20 filetoview Display the last 20 lines of' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='lines of the file' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview\\nInstalling software for Linux' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -ihv name.rpm Install the rpm package called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called name' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -Uhv name.rpm Upgrade the rpm package called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='name\\nrpm -e package Delete the rpm package called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -l package List the files in the package' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -ql package List the files and state the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='state the installed' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='version of the package called\\npackage' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -i --force package Reinstall the rpm package' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='name having deleted parts of it (not' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='deleting using rpm -e)' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tar -zxvf archive.tar.gz or' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tar -zxvf archive.tgzDecompress the files' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the files contained in' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the zipped and tarred archive called\\narchive' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='./configure Execute the script preparing the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='installed files for compiling\\nUser Administration' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='adduser accountname Create a new user call' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='user call accountname' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='passwd accountname Give accountname a new' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='a new password' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='su Log in as superuser from current login' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='exit Stop being superuser and revert to' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='normal user\\nLittle known tips and tricks' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ifconfig List ip addresses for all devices on' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the machine' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='apropos subject List manual pages for subject' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='usermount Executes graphical application for' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='mounting and unmounting file\\nsystems' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/sbin/e2fsck hda5 Execute the filesystem check' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='check utility' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='on partition hda5' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='fdformat /dev/fd0H1440 Format the floppy disk in' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='disk in device fd0' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='tar -cMf /dev/fd0 Backup the contents of the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='of the current' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='directory and subdirectories to' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='multiple floppy disks' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='tail -f /var/log/messages Display the last 10' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='last 10 lines of the system' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='log.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='cat /var/log/dmesg Display the file containing' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='the boot' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='time messages - useful for locating' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='problems. Alternatively, use the\\ndmesg  command.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='* wildcard - represents everything. eg.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='cp from/* to  will copy all files in the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='from directory to the to directory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='? Single character wildcard. eg.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='cp config.? /configs will copy all files' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='beginning with the name config. in' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='the current directory to the directory' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='named configs.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='[xyz] Choice of character wildcards. eg.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='ls [xyz]* will list all files in the current' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='directory starting with the letter x, y,\\nor z.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='linux single At the lilo prompt, start in single' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='in single user' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='mode. This is useful if you have' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='forgotten your password. Boot in' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='single user mode, then run the\\npasswd  command.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='ps List current processes' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='kill 123 Kill a specific process eg. kill 123' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Configuration files and what they do' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/profile System wide environment variables' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='variables for' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='all users.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/fstab List of devices and their associated' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='mount' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='points. Edit this file to add cdroms, DOS' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='partitions and floppy drives at startup.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/motd Message of the day broadcast to all' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='to all users' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='at login.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='etc/rc.d/rc.local Bash script that is executed at' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='at the end of' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='login process. Similar to autoexec.bat in\\nDOS.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/HOSTNAME Conatins full hostname including' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='including domain.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/cron.* There are 4 directories that' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='that automatically' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='execute all scripts within the directory at' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='intervals of hour, day, week or month.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/hosts A list of all know host names and IP' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='addresses on the machine.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/httpd/conf Paramters for the Apache web' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='web server' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/inittab Specifies the run level that the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='that the machine' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='should boot into.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/resolv.conf Defines IP addresses of DNS' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='of DNS servers.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/smb.conf Config file for the SAMBA server.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='server. Allows' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='file and print sharing with Microsoft\\nclients.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='clients.\\n/etc/X11/XF86Confi' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='gConfig file for X -Windows.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='~/.xinitrc Defines the windows manager loaded by' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='X. ~ refers to user’s home directory.File' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='permissions' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='If the command ls -l is given, a long list of' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='list of file names is' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='displayed. The first column in this list details' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='details the permissions' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='applying to the file. If a permission is missing' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='missing for a owner,' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='group of other, it is represented by - eg.  drwxr' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='drwxr -x—x' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Read = 4\\nWrite = 2' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Execute = 1File permissions are altered by giving' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='by giving the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod command and the appropriate' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='octal code for each user type. eg' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod 7 6 4 filename will make the file' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='called filename R+W+X for the owner,' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='R+W for the group and R for others.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod 7 5 5 Full permission for the owner, read' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='read and' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='execute access for the group and others.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod +x filename Make the file called filename' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='filename executable' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='to all users.\\nX Shortcuts - (mainly for Redhat)' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Control|Alt  + or - Increase or decrease the' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='the screen' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='resolution. eg. from 640x480 to\\n800x600' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Alt | escape Display list of active windows' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Shift|Control F8 Resize the selected window' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Right click on desktop\\nbackgroundDisplay menu' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Shift|Control Altr Refresh the screen' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Shift|Control Altx Start an xterm session' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Printing' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/rc.d/init.d/lpd start Start the print daemon' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/rc.d/init.d/lpd stop Stop the print daemon' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/rc.d/init.d/lpd' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='statusDisplay status of the print daemon' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='lpq Display jobs in print queue' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='lprm Remove jobs from queue\\nlpr Print a file' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='lpc Printer control tool' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='man subject | lpr Print the manual page called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='called subject' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='as plain text' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='man -t subject | lpr Print the manual page called' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='called subject' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='as Postscript output' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='printtool Start X printer setup' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='setup interface~/.Xdefaults Define configuration' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='for some X -' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='applications. ~ refers to user’s home\\ndirectory.' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Get your own Official Linux Pocket Protector -' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='- includes' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='handy command summary. Visit:' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='www.powerup.com.au/~squadron' metadata={'source': 'source/The One Page Linux Manual.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"source/The One Page Linux Manual.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852c638-6a7d-4e12-b854-ee174c63422f",
   "metadata": {},
   "source": [
    "We created an instance of the `RecursiveCharacterTextSplitter` class with the desired parameters. The default list of characters to split by is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n",
    "\n",
    "The text is first split by two new-line characters `(\\n\\n)`. Then, since the chunks are still larger than the desired chunk size (50), the class tries to split the output by a single new-line character `(\\n)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20d335-a98f-4295-9b75-fff5a1efee2b",
   "metadata": {},
   "source": [
    "### NLTK Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbaee7fc-5147-43c9-9aba-a75dfe815fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20477892-76ad-43a0-9863-915a51d75ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd16b164-5baf-4ad1-a32f-ab47877ebd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20f252de-9705-4fdd-9642-25dc8a66249b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NLTKTextSplitter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load a long document\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43municode_escape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     sample_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      7\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m NLTKTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt'"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f68033-c711-4c01-b206-f2735d81534c",
   "metadata": {},
   "source": [
    "However, as mentioned in your context, the NLTKTextSplitter is not specifically designed to handle word segmentation in English sentences without spaces. For this purpose, you can use alternative libraries like pyenchant or word segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b7362-75eb-409e-9491-b85f1d94d5c0",
   "metadata": {},
   "source": [
    "### SpacyTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3023b6-424c-447e-ba29-2e262e40df93",
   "metadata": {},
   "source": [
    "The SpacyTextSplitter helps split large text documents into smaller chunks based on a specified size. This is useful for better management of large text inputs. It's important to note that the SpacyTextSplitter is an alternative to NLTK-based sentence splitting. You can create a SpacyTextSplitter object by specifying the chunk_size parameter, measured by a length function passed to it, which defaults to the number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f3fe8-fc21-498a-970f-c2a2e552c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# Split the text using SpacyTextSplitter\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "\n",
    "# Print the first chunk\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677406f-5ec6-460a-b89c-01cfc5d1256b",
   "metadata": {},
   "source": [
    "### MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea9ed2-f9ea-4ec9-8e32-a5f008bc5e30",
   "metadata": {},
   "source": [
    "The MarkdownTextSplitter is designed to split text written using Markdown languages like headers, code blocks, or dividers. It is implemented as a simple subclass of RecursiveCharacterSplitter with Markdown-specific separators. By default, these separators are determined by the Markdown syntax, but they can be customized by providing a list of characters during the initialization of the MarkdownTextSplitter instance. The chunk size, which is initially set to the number of characters, is measured by the length function passed in. To customize the chunk size, provide an integer value when initializing an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bffe52e-386f-4a6e-96a9-dea8223382c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# \\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='## Introduction', metadata={}), Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}), Document(page_content='Java, and JavaScript.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='## About this Blog', metadata={}), Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}), Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}), Document(page_content='```\\n\\nStay tuned for more updates!', metadata={}), Document(page_content='## Contact Me', metadata={}), Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}), Document(page_content='johndoe@email.com.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# \n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ccb8b6-2c7c-49f2-aa73-ba9516057c33",
   "metadata": {},
   "source": [
    "### TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f07264c-2a04-4519-8303-bcddd5589323",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d1aa8-43c5-49a1-b448-898d88961368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db823c5a-5842-494b-9d7a-4fdb113c7615",
   "metadata": {},
   "source": [
    "- CharacterTextSplitter is an example that helps balance manageable pieces and semantic context preservation. Experimenting with different chunk sizes and overlaps tailor the results for specific use cases.\n",
    "- RecursiveCharacterTextSplitter focuses on preserving semantic relationships while offering customizable chunk sizes and overlaps.\n",
    "- NLTKTextSplitter utilizes the Natural Language Toolkit library for more accurate text segmentation.\n",
    "- SpacyTextSplitter leverages the popular SpaCy library to split texts based on linguistic features.\n",
    "- MarkdownTextSplitter is tailored for Markdown-formatted texts, ensuring content is split meaningfully according to the syntax.\n",
    "- Lastly, TokenTextSplitter employs BPE tokens for splitting, offering a fine-grained approach to text segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc19c3-7b28-4667-9c3c-a97606829be1",
   "metadata": {},
   "source": [
    "# Exploring the World of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82aa6c-c783-4afe-85a2-8631b6735afd",
   "metadata": {},
   "source": [
    "## Similarity search and vector embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "490742ee-ab83-420d-ba9d-c928ef2b7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e0e41bd-37bf-43a4-abcd-248f9707707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to the query 'A cat is sitting on a mat.':\n",
      "The cat is on the mat.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "# Perform a similarity search for a given query\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa86d7-dd26-4ab9-ac37-af7a6095bee8",
   "metadata": {},
   "source": [
    "## Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "979f2f4c-1292-447e-a2d4-64c8e2d58bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers===2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "992d786b-3636-42ed-b9a7-4ee849c31de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)a8e1d/.gitattributes: 100%|█████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 393kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|████████████████████████████████████████████| 190/190 [00:00<00:00, 31.9kB/s]\n",
      "Downloading (…)b20bca8e1d/README.md: 100%|████████████████████████████████████████| 10.6k/10.6k [00:00<00:00, 3.50MB/s]\n",
      "Downloading (…)0bca8e1d/config.json: 100%|█████████████████████████████████████████████| 571/571 [00:00<00:00, 191kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|████████████████████████████████████████████| 116/116 [00:00<00:00, 14.5kB/s]\n",
      "Downloading (…)e1d/data_config.json: 100%|████████████████████████████████████████| 39.3k/39.3k [00:00<00:00, 2.12MB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████| 438M/438M [00:38<00:00, 11.4MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 17.8kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████| 239/239 [00:00<00:00, 30.0kB/s]\n",
      "Downloading (…)a8e1d/tokenizer.json: 100%|██████████████████████████████████████████| 466k/466k [00:00<00:00, 3.13MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████| 363/363 [00:00<00:00, 40.5kB/s]\n",
      "Downloading (…)8e1d/train_script.py: 100%|████████████████████████████████████████| 13.1k/13.1k [00:00<00:00, 1.87MB/s]\n",
      "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████████████████████████████████████| 232k/232k [00:00<00:00, 14.3MB/s]\n",
      "Downloading (…)bca8e1d/modules.json: 100%|████████████████████████████████████████████| 349/349 [00:00<00:00, 43.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383ba5a-1259-4bae-b0c9-8922418bf556",
   "metadata": {},
   "source": [
    "## Cohere embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd4b4c92-2b3d-4295-9b20-8841dac89a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bdd1d036-73ce-4789-8cab-0ca2af4fa790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello from Cohere!\n",
      "Embedding: [0.23449707, 0.50097656, -0.04876709, 0.14001465, -0.1796875]\n",
      "Text: مرحبًا من كوهير!\n",
      "Embedding: [0.25341797, 0.30004883, 0.01083374, 0.12573242, -0.1821289]\n",
      "Text: Hallo von Cohere!\n",
      "Embedding: [0.10205078, 0.28320312, -0.0496521, 0.2364502, -0.0715332]\n",
      "Text: Bonjour de Cohere!\n",
      "Embedding: [0.15161133, 0.28222656, -0.057281494, 0.11743164, -0.044189453]\n",
      "Text: ¡Hola desde Cohere!\n",
      "Embedding: [0.25146484, 0.43139648, -0.08642578, 0.24682617, -0.117004395]\n",
      "Text: Olá do Cohere!\n",
      "Embedding: [0.18676758, 0.390625, -0.04550171, 0.14562988, -0.11230469]\n",
      "Text: Ciao da Cohere!\n",
      "Embedding: [0.11590576, 0.4333496, -0.025772095, 0.14538574, 0.0703125]\n",
      "Text: 您好，来自 Cohere！\n",
      "Embedding: [0.24645996, 0.3083496, -0.111816406, 0.26586914, -0.05102539]\n",
      "Text: कोहेरे से नमस्ते!\n",
      "Embedding: [0.19274902, 0.6352539, 0.031951904, 0.117370605, -0.26098633]\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "# Initialize the CohereEmbeddings object\n",
    "cohere = CohereEmbeddings(\n",
    "\tmodel=\"embed-multilingual-v2.0\",\n",
    "\tcohere_api_key=os.environ['COHERE_API_KEY']\n",
    ")\n",
    "\n",
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e8f9e-c393-4f8e-afaf-993598932b5b",
   "metadata": {},
   "source": [
    "## Deep Lake Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "051d5812-a95c-4f42-981f-31c4057767d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c18b7eed-6a4e-4591-9b72-dbcb9a9f7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8c033fc-3f34-4062-b2d7-a806c476dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://bettermaxfeng/langchain_course_embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0004ace2-4083-11ee-b8f9-d89c6787905c',\n",
       " '0004ace3-4083-11ee-9aea-d89c6787905c',\n",
       " '0004ace4-4083-11ee-8297-d89c6787905c',\n",
       " '0004ace5-4083-11ee-9092-d89c6787905c']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = os.environ[\"ACTIVELOOP-ORG-ID\"]\n",
    "my_activeloop_dataset_name = \"langchain_course_embeddings\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e6e1386-932e-435f-8420-f91fe41026b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8513b396-7590-461a-8e33-53b385790f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Jordan was born on 17 February 1963.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# istantiate the llm wrapper\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "# create the question-answering chain\n",
    "qa_chain = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# ask a question to the chain\n",
    "qa_chain.run(\"When was Michael Jordan born?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97cf9c-30e1-4698-9e24-09d4658b6835",
   "metadata": {},
   "source": [
    "1. OpenAI and LangChain Integration: LangChain, a library built for chaining NLP models, is designed to work seamlessly with OpenAI's GPT-3.5-turbo model for language understanding and generation. You've initialized OpenAI embeddings using OpenAIEmbeddings(), and these embeddings are later used to transform the text into a high-dimensional vector representation. This vector representation captures the semantic essence of the text and is essential for information retrieval tasks.\n",
    "2. Deep Lake: Deep Lake is a Vector Store for creating, storing, and querying vector representations (also known as embeddings) of data.\n",
    "3. Text Retrieval: Using the db.as_retriever() function, you've transformed the Deep Lake dataset into a retriever object. This object is designed to fetch the most relevant pieces of text from the dataset based on the semantic similarity of their embeddings.\n",
    "4. Question Answering: The final step involves setting up a RetrievalQA chain from LangChain. This chain is designed to accept a natural language question, transform it into an embedding, retrieve the most relevant document chunks from the Deep Lake dataset, and generate a natural language answer. The ChatOpenAI model, which is the underlying model of this chain, is responsible for both the question embedding and the answer generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb4619-72ff-4652-9bfa-d69ec691066c",
   "metadata": {},
   "source": [
    "# Build a Customer Support Question Answering Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f7b72-48ff-4cb8-9016-36cda96fd2c9",
   "metadata": {},
   "source": [
    "## Having a Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2e782-93b2-44b6-a6d2-05e12724d9b2",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297c375-b649-4e59-9dea-f65d887e958c",
   "metadata": {},
   "source": [
    "![图片描述](pics/chatbot_workflow.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "864f9892-4df4-4b75-a060-957df01b24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unstructured selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd58af0-c521-4f7e-a28f-473e67feadec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.19) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcba0562-7386-43b7-b6fe-b409ec5d8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use information from the following articles\n",
    "urls = ['https://beebom.com/what-is-nft-explained/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-download-gif-twitter/',\n",
    "        'https://beebom.com/how-use-chatgpt-linux-terminal/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-save-instagram-story-with-music/',\n",
    "        'https://beebom.com/how-install-pip-windows/',\n",
    "        'https://beebom.com/how-check-disk-usage-linux/']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d395247-ed44-4c14-a8ce-e0e636ff3b4d",
   "metadata": {},
   "source": [
    "### 1: Split the documents into chunks and compute their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "974763f2-38b9-4223-9280-b35b22d38495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1226, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# use the selenium scraper to load the documents\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "docs_not_splitted = loader.load()\n",
    "\n",
    "# we split the documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs_not_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e861a5-c6aa-4b1c-a2ca-4ea96bd75ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://bettermaxfeng/langchain_course_customer_support', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (127, 1536)  float32   None   \n",
      "    id        text      (127, 1)      str     None   \n",
      " metadata     json      (127, 1)      str     None   \n",
      "   text       text      (127, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "['cff08f7c-420e-11ee-b60b-d89c6787905c',\n",
       " 'cff08f7d-420e-11ee-98a7-d89c6787905c',\n",
       " 'cff08f7e-420e-11ee-b082-d89c6787905c',\n",
       " 'cff08f7f-420e-11ee-b8f9-d89c6787905c',\n",
       " 'cff08f80-420e-11ee-9aea-d89c6787905c',\n",
       " 'cff0b66d-420e-11ee-8297-d89c6787905c',\n",
       " 'cff0b66e-420e-11ee-9092-d89c6787905c',\n",
       " 'cff0b66f-420e-11ee-bdca-d89c6787905c',\n",
       " 'cff0b670-420e-11ee-a0b8-d89c6787905c',\n",
       " 'cff0b671-420e-11ee-9f19-d89c6787905c',\n",
       " 'cff0b672-420e-11ee-99ea-d89c6787905c',\n",
       " 'cff0b673-420e-11ee-bac4-d89c6787905c',\n",
       " 'cff0b674-420e-11ee-b229-d89c6787905c',\n",
       " 'cff0b675-420e-11ee-b51c-d89c6787905c',\n",
       " 'cff0b676-420e-11ee-9369-d89c6787905c',\n",
       " 'cff0b677-420e-11ee-bdf0-d89c6787905c',\n",
       " 'cff0b678-420e-11ee-9e80-d89c6787905c',\n",
       " 'cff0b679-420e-11ee-96ea-d89c6787905c',\n",
       " 'cff0b67a-420e-11ee-a556-d89c6787905c',\n",
       " 'cff0b67b-420e-11ee-b910-d89c6787905c',\n",
       " 'cff0b67c-420e-11ee-ba1e-d89c6787905c',\n",
       " 'cff0b67d-420e-11ee-8dfa-d89c6787905c',\n",
       " 'cff0b67e-420e-11ee-a04c-d89c6787905c',\n",
       " 'cff0b67f-420e-11ee-88e9-d89c6787905c',\n",
       " 'cff0b680-420e-11ee-9209-d89c6787905c',\n",
       " 'cff0b681-420e-11ee-88f1-d89c6787905c',\n",
       " 'cff0b682-420e-11ee-b05f-d89c6787905c',\n",
       " 'cff0b683-420e-11ee-8611-d89c6787905c',\n",
       " 'cff0b684-420e-11ee-a793-d89c6787905c',\n",
       " 'cff0b685-420e-11ee-b329-d89c6787905c',\n",
       " 'cff0b686-420e-11ee-9008-d89c6787905c',\n",
       " 'cff0b687-420e-11ee-bf2f-d89c6787905c',\n",
       " 'cff0b688-420e-11ee-ba39-d89c6787905c',\n",
       " 'cff0b689-420e-11ee-a215-d89c6787905c',\n",
       " 'cff0b68a-420e-11ee-bee5-d89c6787905c',\n",
       " 'cff0b68b-420e-11ee-ad21-d89c6787905c',\n",
       " 'cff0b68c-420e-11ee-b3da-d89c6787905c',\n",
       " 'cff0b68d-420e-11ee-a685-d89c6787905c',\n",
       " 'cff0b68e-420e-11ee-b9bd-d89c6787905c',\n",
       " 'cff0b68f-420e-11ee-8967-d89c6787905c',\n",
       " 'cff0b690-420e-11ee-93d9-d89c6787905c',\n",
       " 'cff0b691-420e-11ee-8652-d89c6787905c',\n",
       " 'cff0b692-420e-11ee-aeb5-d89c6787905c',\n",
       " 'cff0b693-420e-11ee-84b8-d89c6787905c',\n",
       " 'cff0b694-420e-11ee-b986-d89c6787905c',\n",
       " 'cff0b695-420e-11ee-b66e-d89c6787905c',\n",
       " 'cff0b696-420e-11ee-abc6-d89c6787905c',\n",
       " 'cff0b697-420e-11ee-9521-d89c6787905c',\n",
       " 'cff0b698-420e-11ee-9e37-d89c6787905c',\n",
       " 'cff0b699-420e-11ee-a3d3-d89c6787905c',\n",
       " 'cff0b69a-420e-11ee-8671-d89c6787905c',\n",
       " 'cff0b69b-420e-11ee-96a4-d89c6787905c',\n",
       " 'cff0b69c-420e-11ee-9bc9-d89c6787905c',\n",
       " 'cff0b69d-420e-11ee-943c-d89c6787905c',\n",
       " 'cff0b69e-420e-11ee-a718-d89c6787905c',\n",
       " 'cff0b69f-420e-11ee-a8fc-d89c6787905c',\n",
       " 'cff0b6a0-420e-11ee-ba6e-d89c6787905c',\n",
       " 'cff0b6a1-420e-11ee-8d16-d89c6787905c',\n",
       " 'cff0b6a2-420e-11ee-bddc-d89c6787905c',\n",
       " 'cff0b6a3-420e-11ee-a35c-d89c6787905c',\n",
       " 'cff0b6a4-420e-11ee-9e87-d89c6787905c',\n",
       " 'cff0b6a5-420e-11ee-9c55-d89c6787905c',\n",
       " 'cff0b6a6-420e-11ee-b761-d89c6787905c',\n",
       " 'cff0b6a7-420e-11ee-a15d-d89c6787905c',\n",
       " 'cff0b6a8-420e-11ee-90ab-d89c6787905c',\n",
       " 'cff0b6a9-420e-11ee-83fc-d89c6787905c',\n",
       " 'cff0b6aa-420e-11ee-b385-d89c6787905c',\n",
       " 'cff0b6ab-420e-11ee-bac8-d89c6787905c',\n",
       " 'cff0b6ac-420e-11ee-a31d-d89c6787905c',\n",
       " 'cff0b6ad-420e-11ee-ba9f-d89c6787905c',\n",
       " 'cff0b6ae-420e-11ee-80e6-d89c6787905c',\n",
       " 'cff0b6af-420e-11ee-85f8-d89c6787905c',\n",
       " 'cff0b6b0-420e-11ee-ae0f-d89c6787905c',\n",
       " 'cff0b6b1-420e-11ee-b5c4-d89c6787905c',\n",
       " 'cff0b6b2-420e-11ee-9986-d89c6787905c',\n",
       " 'cff0b6b3-420e-11ee-ad74-d89c6787905c',\n",
       " 'cff0b6b4-420e-11ee-b4ca-d89c6787905c',\n",
       " 'cff0b6b5-420e-11ee-b23e-d89c6787905c',\n",
       " 'cff0b6b6-420e-11ee-aac3-d89c6787905c',\n",
       " 'cff0b6b7-420e-11ee-a804-d89c6787905c',\n",
       " 'cff0b6b8-420e-11ee-8012-d89c6787905c',\n",
       " 'cff0b6b9-420e-11ee-a729-d89c6787905c',\n",
       " 'cff0b6ba-420e-11ee-9f96-d89c6787905c',\n",
       " 'cff0b6bb-420e-11ee-b4fe-d89c6787905c',\n",
       " 'cff0b6bc-420e-11ee-b786-d89c6787905c',\n",
       " 'cff0b6bd-420e-11ee-9552-d89c6787905c',\n",
       " 'cff0b6be-420e-11ee-8f9c-d89c6787905c',\n",
       " 'cff0b6bf-420e-11ee-aebc-d89c6787905c',\n",
       " 'cff0b6c0-420e-11ee-94d0-d89c6787905c',\n",
       " 'cff0b6c1-420e-11ee-ad08-d89c6787905c',\n",
       " 'cff0b6c2-420e-11ee-b7b5-d89c6787905c',\n",
       " 'cff0b6c3-420e-11ee-8407-d89c6787905c',\n",
       " 'cff0b6c4-420e-11ee-8c3a-d89c6787905c',\n",
       " 'cff0b6c5-420e-11ee-bab0-d89c6787905c',\n",
       " 'cff0b6c6-420e-11ee-a452-d89c6787905c',\n",
       " 'cff0b6c7-420e-11ee-8e30-d89c6787905c',\n",
       " 'cff0b6c8-420e-11ee-8f45-d89c6787905c',\n",
       " 'cff0b6c9-420e-11ee-b36a-d89c6787905c',\n",
       " 'cff0b6ca-420e-11ee-bdec-d89c6787905c',\n",
       " 'cff0b6cb-420e-11ee-891e-d89c6787905c',\n",
       " 'cff0b6cc-420e-11ee-b367-d89c6787905c',\n",
       " 'cff0b6cd-420e-11ee-a2c0-d89c6787905c',\n",
       " 'cff0b6ce-420e-11ee-9cab-d89c6787905c',\n",
       " 'cff0b6cf-420e-11ee-85d6-d89c6787905c',\n",
       " 'cff0b6d0-420e-11ee-8526-d89c6787905c',\n",
       " 'cff0b6d1-420e-11ee-bf90-d89c6787905c',\n",
       " 'cff0b6d2-420e-11ee-947b-d89c6787905c',\n",
       " 'cff0b6d3-420e-11ee-b801-d89c6787905c',\n",
       " 'cff0b6d4-420e-11ee-a082-d89c6787905c',\n",
       " 'cff0b6d5-420e-11ee-bfde-d89c6787905c',\n",
       " 'cff0b6d6-420e-11ee-bbb3-d89c6787905c',\n",
       " 'cff0b6d7-420e-11ee-9f50-d89c6787905c',\n",
       " 'cff0b6d8-420e-11ee-86fa-d89c6787905c',\n",
       " 'cff0b6d9-420e-11ee-934a-d89c6787905c',\n",
       " 'cff0b6da-420e-11ee-a347-d89c6787905c',\n",
       " 'cff0b6db-420e-11ee-92a1-d89c6787905c',\n",
       " 'cff0b6dc-420e-11ee-ad38-d89c6787905c',\n",
       " 'cff0b6dd-420e-11ee-87fc-d89c6787905c',\n",
       " 'cff0b6de-420e-11ee-a309-d89c6787905c',\n",
       " 'cff0b6df-420e-11ee-954b-d89c6787905c',\n",
       " 'cff0b6e0-420e-11ee-b420-d89c6787905c',\n",
       " 'cff0b6e1-420e-11ee-bb06-d89c6787905c',\n",
       " 'cff0b6e2-420e-11ee-a294-d89c6787905c',\n",
       " 'cff0b6e3-420e-11ee-8d01-d89c6787905c',\n",
       " 'cff0b6e4-420e-11ee-bdaf-d89c6787905c',\n",
       " 'cff0b6e5-420e-11ee-b329-d89c6787905c',\n",
       " 'cff0b6e6-420e-11ee-a69a-d89c6787905c']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = os.environ[\"ACTIVELOOP-ORG-ID\"]\n",
    "my_activeloop_dataset_name = \"langchain_course_customer_support\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3e9ac3-f277-424e-9be6-8c6bc62f7b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home  Tech  How to Check Disk Usage in Linux (4 Methods)\n",
      "\n",
      "How to Check Disk Usage in Linux (4 Methods)\n",
      "\n",
      "Beebom Staff\n",
      "\n",
      "Last Updated: June 19, 2023 5:14 pm\n",
      "\n",
      "There may be times when you need to download some important files or transfer some photos to your Linux system, but face a problem of insufficient disk space. You head over to your file manager to delete the large files which you no longer require, but you have no clue which of them are occupying most of your disk space. In this article, we will show some easy methods to check disk usage in Linux from both the terminal and the GUI application.\n",
      "\n",
      "Monitor Disk Usage in Linux (2023)\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Check Disk Space Using the df Command\n",
      "\t\t\n",
      "Display Disk Usage in Human Readable FormatDisplay Disk Occupancy of a Particular Type\n",
      "\n",
      "Check Disk Usage using the du Command\n",
      "\t\t\n",
      "Display Disk Usage in Human Readable FormatDisplay Disk Usage for a Particular DirectoryCompare Disk Usage of Two Directories\n"
     ]
    }
   ],
   "source": [
    "# let's see the top relevant documents to a specific query\n",
    "query = \"how to check disk usage in linux?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5704357-2d2a-45f0-9662-11a81dc5464b",
   "metadata": {},
   "source": [
    "### 2: Craft a prompt for GPT-3 using the suggested strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f3992d-0437-4eb2-99f1-9218927bd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a prompt for a customer support chatbot that\n",
    "# answer questions using information extracted from our db\n",
    "template = \"\"\"You are an exceptional customer support chatbot that gently answer questions.\n",
    "\n",
    "You know the following context information.\n",
    "\n",
    "{chunks_formatted}\n",
    "\n",
    "Answer to the following question from a customer. Use only information from the previous context information. Do not invent stuff.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chunks_formatted\", \"query\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a9b95-cf6a-4367-96c1-0f1320e569ad",
   "metadata": {},
   "source": [
    "### 3: Utilize the GPT3 model with a temperature of 0 for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10245c50-0f95-4298-a7c2-1290d2a88036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can check disk usage in Linux using the df command or by using a GUI tool such as the GDU Disk Usage Analyzer or the Gnome Disks Tool. The df command is used to check the current disk usage and the available disk space in Linux. The syntax for the df command is: df <options> <file_system>. The options to use with the df command are: a, h, t, and x. To install the GDU Disk Usage Analyzer, use the command: sudo snap install gdu-disk-usage-analyzer. To install the Gnome Disks Tool, use the command: sudo apt-get -y install gnome-disk-utility.\n"
     ]
    }
   ],
   "source": [
    "# the full pipeline\n",
    "\n",
    "# user question\n",
    "query = \"How to check disk usage in linux?\"\n",
    "\n",
    "# retrieve relevant chunks\n",
    "docs = db.similarity_search(query)\n",
    "retrieved_chunks = [doc.page_content for doc in docs]\n",
    "\n",
    "# format the prompt\n",
    "chunks_formatted = \"\\n\\n\".join(retrieved_chunks)\n",
    "prompt_formatted = prompt.format(chunks_formatted=chunks_formatted, query=query)\n",
    "\n",
    "# generate answer\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "answer = llm(prompt_formatted)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37336ae-2240-4a82-bbca-8b23698de5e3",
   "metadata": {},
   "source": [
    "## Issues with Generating Answers using GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73ec0e-6960-49d1-9858-a622598be560",
   "metadata": {},
   "source": [
    "Suppose we ask, \"Is the Linux distribution free?\" and provide GPT-3 with a document about kernel features as context. It might generate an answer like \"Yes, the Linux distribution is free to download and use,\" even if such information is not present in the context document. Producing false information is highly undesirable for customer service chatbots!\n",
    "\n",
    "GPT-3 is less likely to generate false information when the answer to the user's question is contained within the context. Since user questions are often brief and ambiguous, we cannot always rely on the semantic search step to retrieve the correct document. Thus, there is always a risk of generating false information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d039387-df74-455f-b526-654b9e1f206e",
   "metadata": {},
   "source": [
    "# Conversation Intelligence: Gong.io Open-Source Alternative AI Sales Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e1ce3-7160-406d-b1de-7a0398233b49",
   "metadata": {},
   "source": [
    "## Didn't Work: Naively Splitting the Custom Knowledge Base\n",
    "> Objection: \"There's no money.\"\n",
    "It could be that your prospect's business simply isn't big enough or generating enough cash right now to afford a product like yours. Track their growth and see how you can help your prospect get to a place where your offering would fit into their business.\n",
    "\n",
    "> Objection: \"We don't have any budget left this year.\"\n",
    "A variation of the \"no money\" objection, what your prospect's telling you here is that they're having cash flow issues. But if there's a pressing problem, it needs to get solved eventually. Either help your prospect secure a budget from executives to buy now or arrange a follow-up call for when they expect funding to return.\n",
    "\n",
    "> Objection: \"We need to use that budget somewhere else.\"\n",
    "Prospects sometimes try to earmark resources for other uses. It's your job to make your product/service a priority that deserves budget allocation now. Share case studies of similar companies that have saved money, increased efficiency, or had a massive ROI with you.\n",
    "If we naively split this text, we might end up with individual sections that look like this:\n",
    "\n",
    "> Objection: \"We need to use that budget somewhere else.\"\n",
    "Here, we see that the advice does not match the objection. When we try to retrieve the most relevant chunk for the objection \"We need to use that budget somewhere else\", this will likely be our top result, which isn't what we want. When we pass it to the LLM, it might be confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b6cd6-8902-4622-a042-578f1bd8eb9b",
   "metadata": {},
   "source": [
    "## Did Work: Intelligent Splitting\n",
    "In our example text, there is a set structure to each individual objection and its recommended response. Rather than split the text based on size, why don't we split the text based on its structure? We want each chunk to begin with the objection, and end before the \"Objection\" of the next chunk. Here's how we could do it:\n",
    "\n",
    "> text = \"\"\"\n",
    "Objection: \"There's no money.\"\n",
    "It could be that your prospect's business simply isn't big enough or generating enough cash right now to afford a product like yours. Track their growth and see how you can help your prospect get to a place where your offering would fit into their business.\n",
    "\n",
    "> Objection: \"We don't have any budget left this year.\"\n",
    "A variation of the \"no money\" objection, what your prospect's telling you here is that they're having cash flow issues. But if there's a pressing problem, it needs to get solved eventually. Either help your prospect secure a budget from executives to buy now or arrange a follow-up call for when they expect funding to return.\n",
    "\n",
    "> Objection: \"We need to use that budget somewhere else.\"\n",
    "Prospects sometimes try to earmark resources for other uses. It's your job to make your product/service a priority that deserves budget allocation now. Share case studies of similar companies that have saved money, increased efficiency, or had a massive ROI with you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaad600-40f6-4125-b9db-881953ca8725",
   "metadata": {},
   "source": [
    "## Split the text into a list using the keyword \"Objection: \"\n",
    "objections_list = text.split(\"Objection: \")[1:]  # We ignore the first split as it is empty\n",
    "\n",
    "## Now, prepend \"Objection: \" to each item as splitting removed it\n",
    "objections_list = [\"Objection: \" + objection for objection in objections_list]\n",
    "This gave us the best results. Nailing the way we split and embed our knowledge base means more relevant documents are retrieved and the LLM gets the best possible context to generate a response from. Now let's see how we integrated this solution with Deep Lake and SalesCopilot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73920608-851e-481f-a191-49813dc9cd68",
   "metadata": {},
   "source": [
    "# FableForge: Creating Picture Books with OpenAI, Replicate, and Deep Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa9547-3970-4f56-8d0b-f018adf5eb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f32c92-73c7-43f7-8d9b-224bd70ee7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ac6fb-4135-457c-8c33-2f625a73908b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fd4a3-a1a4-45cd-818e-e57ba614b7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64a497-b893-4539-942f-4524c71b90b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0ea16-d854-49a8-8b75-b20fdbf71d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8590889-f6ea-4ada-b0b6-84aef4b8030e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16062ff1-e4ba-4074-9e2c-04837f0d3f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db01ba1-705b-4255-b9ce-ea6eec53294b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77129f9-6730-4c02-9b0b-b485400c7912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba8452-5d67-4e54-bfac-4895bd62bf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4bd25-b746-4ab8-99e5-1c70b1e80017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25174490-b6df-4fc9-9a01-18127baf9fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b99ec8-ffb8-40d1-b220-a60b5dd687a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f769c0-259f-4675-8aaa-876d0afde711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba75140-40cc-48b5-b620-35c825a3430e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95114fd7-0dcb-4056-b694-eea7c0fc6dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805cb0d-bb0f-4834-9d41-1469239a8686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624edf6-6aa9-4f3f-a31d-8032243ade24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892612de-c182-476c-a6d8-75c8104dfd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419e500-f25a-47cd-8973-fb7e860f7828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a690a-1cb8-420d-9580-ad6800685e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae948d9-1708-4d81-8bb6-d08fa35f0c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ea90e-1cda-4934-9d74-21a1cf3ba482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1df89e-074b-452b-aa64-916f2b79aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661b1c1-c833-4b1a-b90d-7b769a575d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3f7cb-0faf-442e-b81f-5fe1afbd964b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba982fe4-c0c1-4350-b447-0eb38e0b72d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95c3c5-8f19-41f6-bc80-10b817b46a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
