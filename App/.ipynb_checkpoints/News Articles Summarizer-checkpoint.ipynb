{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02819df-9d1c-4c56-8fc0-9ecaac410f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcc0d86-7b59-4efe-958a-6c3040bdaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前windows\n",
    "# 获取当前的 Conda 环境路径\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "\n",
    "# \".env\" 文件的绝对路径\n",
    "dotenv_path = os.path.join(conda_env_path, '.env')\n",
    "\n",
    "# 加载 \".env\" 文件\n",
    "_ = load_dotenv(dotenv_path, verbose=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399910a0-9794-4a22-83c9-7ccaf75732ea",
   "metadata": {},
   "source": [
    "# News Articles Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2006e3-458e-4b98-aabf-452940915d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "Dumping model to file cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.0418918132781982 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: “11岁男孩留遗书跳楼案”一审宣判，班主任被认定无罪-大象网\n",
      "Text: 8月9日，江西11岁男孩跳楼自杀，留下遗书指认班主任用暴力手段，其父母刑事自诉孩子班主任侮辱罪、虐待被看护人罪一案，在江西省九江市濂溪区人民法院一审开庭宣判，法院认定被告邹某不构成侮辱罪、虐待被看护人罪，宣判被告邹某无罪。男孩父亲张定杰当庭提出上诉。\n",
      "\n",
      "据此前报道，2021年11月9日，江西九江市11岁男孩宽宽在放学后，在自家小区高楼跳楼自杀，后来其父母在宽宽的身上找到一封遗书，内容为指认班主任邹某对其使用暴力。在事发后宽宽父母在查看教室当天及前半个月监控发现，邹某几次对宽宽进行言语讥讽，甚至还揪起学生的脸。\n",
      "\n",
      "2021年12月，张定杰夫妇向九江市濂溪区法院提起刑事自诉，请求判定邹某犯侮辱罪，追究其刑事责任，当月中旬，法院立案。2022年7月10日当地教育局通报了对邹某的处理意见。通报指出邹某存在“违规收受服务对象红包礼金，接受服务对象宴请和接受庐山住宿安排等问题”、“多次对多名学生有讥讽、歧视行为等违反师德师风的问题”，给予邹某党内严重警告处分、降低两个岗位等级处分和调离教学岗位处理。\n",
      "\n",
      "2023年4月29日法院一审开庭审理此案，男孩父亲张定杰在接受扬子晚报/紫牛新闻记者采访时说，“被告邹某出庭了，看到她态度依旧很顽固，没有丝毫认罪认罚的态度，一直在逃避责任，对于此前教育局通报的她讽刺挖苦、侮辱歧视学生的行为拒不承认，并称她这是幽默式教学，对此我感到非常气愤。”\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_url = \"https://www.hntv.tv/50rd/article/1/1689125963426394113?v=1.0\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "try:\n",
    "    response = session.get(article_url, headers=headers, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Text: {article.text}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to fetch article at {article_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b979b7e6-3016-4ba6-8dd8-5963fd66e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江西一名11岁男孩跳楼自杀，留下遗书指控班主任使用暴力手段。男孩的父母提起刑事自诉，指控班主任侮辱罪和虐待被看护人罪。然而，江西省九江市濂溪区人民法院一审宣判认定被告邹某无罪。男孩的父亲提出上诉。此前，教育局通报了对邹某的处理意见，指出她存在多次对学生进行讥讽、歧视等违反师德师风的问题。男孩的父亲对邹某的态度感到气愤，认为她一直在逃避责任。\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in Chinese.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# generate summary\n",
    "summary = chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a6ffdc-a0df-4fdb-bde6-55d200a9ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(article_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e130d42-25a9-468a-835f-08ab5df525de",
   "metadata": {},
   "source": [
    "# WeChat Article Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e40e007-c52a-415f-954e-08481f329693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07e770f1-a8fe-4f18-af70-938b4af9ed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H100 供需分析：芯片战争将持续多久？\n"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "\n",
    "session = HTMLSession()\n",
    "url = 'https://mp.weixin.qq.com/s/arwikTbqsGv8hBb4aqaXMA'\n",
    "r = session.get(url)\n",
    "a = r.html.text.split('\\n')\n",
    "\n",
    "# 提取标题\n",
    "title = r.html.find('h1', first=True).text\n",
    "\n",
    "flag = False\n",
    "wechat_article_text = \"\"  # 创建一个空字符串变量用于保存文章内容\n",
    "\n",
    "for num in range(len(a)):\n",
    "    if '功能介绍' in a[num]:\n",
    "        flag = True\n",
    "        continue\n",
    "    elif 'var first_sceen__time' in a[num]:\n",
    "        flag = False\n",
    "    elif flag:\n",
    "        wechat_article_text += a[num] + '\\n'  # 将每行内容添加到字符串变量中\n",
    "\n",
    "# 清除连续两个以上的换行\n",
    "import re\n",
    "wechat_article_text = re.sub('\\n{2,}', '\\n', wechat_article_text)\n",
    "# print(title)\n",
    "# print(wechat_article_text)  # 输出或随后您可以对wechat_article_text进行其他操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0330928-802f-436e-b583-d19f4beb5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in Chinese.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# generate summary\n",
    "summary = chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb88cc-77da-43ee-9c0f-239e794aa7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21eec9ea-165b-48d8-b30f-be1c8788f79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 9\n",
      "length of segment: 2364\n",
      "发表于 收录于合集 #AI/ML 53个 作者：Clay Pascal 编译：wenli， Lavida， yunhao 推荐人：Cage， Huaiwei 排版：Scout 大模型的突破是以硬件算力和云计算能力的提升作为土壤的，被视为 GPU “核弹”的 NVIDIA H100 正面临有史以来最严重的缺货。Sam Altman 就直接表示，GPU 的短缺限制了 OpenAI 在微调、专用容量、32K 上下文窗口、多模态等方面的技术升级速度。 本文编译自 GPU Utils，作者主要从供需两个视角的分析探讨了 GPU（尤其是 NVIDIA H100）会持续多久。 需求视角下，NVIDIA H100 毋庸置疑是训练大模型的刚需，根据估算，目前市场上对 H100 的需求在 43.2 万张左右，若按每张约 3.5 万美元计算，这是相当于总价值约为 150 亿美元的 GPU，这 43.2 万的数字尚未包括像字节跳动（TikTok）、百度和腾讯这样需要大量 H800 的企业。 站在供给侧，H100 的短缺直接受限于台积电产能，且短期内， NVIDIA 并没有其他可选择的芯片工厂。因为出货量有限， NVIDIA 对于如何分配这些 GPU 也有自己的策略，对于 NVIDIA 来说，如何保证这些有限的 GPU 流向 AI 黑马而非 Google、微软、AWS 这些潜在竞争者相当重要。 这场围绕着 H100 的 AI 军火竞赛要持续多久？答案尚不明朗。虽然 NVIDIA 表示下半年会增加供给，但目前看来 GPU 的短缺可能会持续到 2024 年。 围绕着 H100 的短缺，接下来市场上或许会进入一种“恶性循环”：稀缺性导致 GPU 容量被视为 AI 公司的护城河，从而导致了更多的 GPU 囤积，而这又进一步加剧了 GPU 的稀缺。 以下为本文目录，建议结合要点进行针对性阅读。 👇 01 背景 02 H100 的需求分析 03 H100 供给侧分析 04 如何获得 H100 05 总结 01. 背景 直到 2023 年 8 月，人工智能领域的发展一直受到 GPU 供应瓶颈的制约。 “人工智能热潮被低估的原因之一是 GPU/TPU 短缺。GPU 和 TPU 的短缺限制了产品推广速度和模型训练进度，但这些限制很隐蔽。我们看到的主要是 NVIDIA 的股价飙升，而非研发进度受限。当供需达到平衡时，情况会有所好转。 —— Adam D'Angelo，Quora、Poe.com 首席执行官，前 Facebook 首席技术官 这些是对 GPU 供需和人工智能最重要的 CEO 和公司 Sam Altman 则表示，GPU 的短缺限制了 OpenAI 的项目进展，例如微调、专用容量、32K 上下文窗口、多模态等。 小型和大型云提供商的大规模 H100 集群容量即将消耗殆尽。 “每个人都希望 NVIDIA 能生产更多的 A/H100”。 —— 来自云提供商执行人员的信息 “由于当下 GPU 短缺的情况，对 OpenAI 来说，使用我们产品的人越少， 对我们反而越好”； “如果大家因为我们缺乏足够 GPU 而减少对 OpenAI 产品的使用，我们实际上会很高兴”。 —— Sam Altman，OpenAI 首席执行官 Sam Altman 这句话一方面巧妙地展现了 OpenAI 的产品已经深受全球用户的喜欢，但同时也说明了另外一个事实，即 OpenAI 确实需要更多的 GPU 来进一步推广、升级其功能。 Azure 和微软方面也面临类似情况，有匿名人士提到： • 公司内部正在限制员工使用 GPU，大家必须像 20 世纪 70 年代的大学生为了使用电脑那样排队申请算力。从我的角度来看，OpenAI 当下正在吸走所有 GPU 资源； • 今年 6 月，微软和 CoreWeave 的合作本质上是为了增强微软的 GPU/算力供给。 CoreWeave ： 云算力服务供应商，据 CoreWeave 官网宣传，他们的服务比传统云计算厂商便宜 80%。2023 年 4 月，CoreWeave 获得 NVIDIA 的 B 轮投资，并获得了大量 H100 新卡，6 月，微软也与 CoreWeave 签订协议，微软会在未来数年内投资数十亿美元，用于云计算基础设施建设。 7 月，CoreWeave 推出了与 NVIDIA 合作打造的世界上最快的 AI 超级计算机项目，以及 Inflection AI 使用支持 MLPerf 提交的基础设施在 CoreWeave Cloud 上创建世界上最复杂的大型语言模型之一。此外，CoreWeave 利用手中的 NVIDIA H100 加速卡作为抵押，于 8 月宣布完成 23 亿美元的债务融资。 总结来说， H100 GPU 的供应已经相当短缺。甚至有传言说，Azure 和 GCP 的容量实际上已经用完，AWS 的容量也快用尽。 而之所以短缺，是因为 NVIDIA 给到这些云供应商的 H100 GPU 供给也就这么多，随着 NVIDIA 的 H100 GPU 产量无法满足需求，这些云供应商可以提供的算力自然也开始出现短缺。 如果要理解算力瓶颈，可以围绕以下几个问题展开： • 造成这种情况的具体原因有哪些？: - 需求量有多大？如哪些领域的人工智能需求量增加相对迅速； - 供应量有多大？NVIDIA 等 GPU 生产商的产能是否足够满足需求； • 这种短缺情况会持续多久？GPU 的供需何时会逐渐达到平衡点？ • 有哪些方式可以有效缓解这种短缺局面？ 欢迎关注海外独角兽视频号 获取最前沿的科技行业资讯 02. H100 的需求分析 从需求端分析算力瓶颈的关键问题： 1.\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 17232 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m     89\u001b[0m article \u001b[38;5;241m=\u001b[39m Article(title, wechat_article_text)\n\u001b[1;32m---> 90\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_summary)\n",
      "Cell \u001b[1;32mIn[23], line 71\u001b[0m, in \u001b[0;36mget_final_summary\u001b[1;34m(article_text, template, token_limit)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength of segment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(segment)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(segment)\n\u001b[1;32m---> 71\u001b[0m     segment_summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     intermediate_summaries\u001b[38;5;241m.\u001b[39mappend(segment_summary)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Merge intermediate summaries\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 58\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[1;34m(text, template)\u001b[0m\n\u001b[0;32m     56\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt)]\n\u001b[0;32m     57\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:208\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    203\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 208\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    209\u001b[0m         [messages], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    210\u001b[0m     )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:102\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    103\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[0;32m    104\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:94\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     99\u001b[0m     ]\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     99\u001b[0m     ]\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\openai.py:359\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m     message \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    352\u001b[0m         {\n\u001b[0;32m    353\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: inner_completion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    356\u001b[0m         }\n\u001b[0;32m    357\u001b[0m     )\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)])\n\u001b[1;32m--> 359\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\openai.py:307\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\openai.py:305\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 17232 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Mocking a function to calculate tokens. \n",
    "# NOTE: Replace this with OpenAI's actual tokenizer for real-world usage.\n",
    "def count_tokens(text, token_ratio={\"english\": 1.5, \"chinese\": 1}):\n",
    "    # Check if the provided ratios are valid numbers\n",
    "    if not all(isinstance(value, (int, float)) for value in token_ratio.values()):\n",
    "        raise ValueError(\"Provided token ratios are not valid numbers.\")\n",
    "    \n",
    "    english_words = len([word for word in text.split() if any(char.isalpha() for char in word) and any(char.isascii() for char in word)])\n",
    "    chinese_chars = len([char for char in text if '\\u4e00' <= char <= '\\u9fff'])\n",
    "    \n",
    "    return english_words * token_ratio.get(\"english\", 1.5) + chinese_chars * token_ratio.get(\"chinese\", 1)\n",
    "\n",
    "# Function to split text into segments based on token limit\n",
    "def split_text_into_segments(text, token_limit):\n",
    "    segments = []\n",
    "    words = text.split()\n",
    "    current_segment = \"\"\n",
    "    current_token_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_token_count = count_tokens(word)\n",
    "        if current_token_count + word_token_count <= token_limit:\n",
    "            current_segment += \" \" + word\n",
    "            current_token_count += word_token_count\n",
    "        else:\n",
    "            segments.append(current_segment.strip())\n",
    "            current_segment = word\n",
    "            current_token_count = word_token_count\n",
    "    \n",
    "    if current_segment:\n",
    "        segments.append(current_segment.strip())\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in Chinese.\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate summary using OpenAI model\n",
    "def generate_summary(text, template):\n",
    "    # format prompt\n",
    "    prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    summary = chat(messages)\n",
    "    return summary.content\n",
    "\n",
    "def get_final_summary(article_text, template, token_limit=1500):\n",
    "    # Split article text into segments\n",
    "    text_segments = split_text_into_segments(article_text, token_limit)\n",
    "    print(f'Number of segments: {len(text_segments)}')\n",
    "\n",
    "    # Generate intermediate summaries for each segment\n",
    "    intermediate_summaries = []\n",
    "    for segment in text_segments:\n",
    "        print(f'length of segment: {len(segment)}')\n",
    "        print(segment)\n",
    "        segment_summary = generate_summary(segment, template)\n",
    "        intermediate_summaries.append(segment_summary)\n",
    "\n",
    "    # Merge intermediate summaries\n",
    "    combined_text = \" \".join(intermediate_summaries)\n",
    "    \n",
    "    # Generate the final summary from the combined intermediate summaries\n",
    "    final_summary = generate_summary(combined_text)\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Example usage:\n",
    "# Define article data\n",
    "class Article:\n",
    "    def __init__(self, title, text):\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "\n",
    "article = Article(title, wechat_article_text)\n",
    "final_summary = get_final_summary(article.text, template)\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dec13e9a-7afe-4b1b-86a7-6f6d64200d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Downloading (…)olve/main/vocab.json: 100%|████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 4.05MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████████████████████████████████████| 456k/456k [00:00<00:00, 2.97MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████| 718/718 [00:00<00:00, 240kB/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3597 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "text = \"发表于 收录于合集 #AI/ML 53个 作者：Clay Pascal 编译：wenli， Lavida， yunhao 推荐人：Cage， Huaiwei 排版：Scout 大模型的突破是以硬件算力和云计算能力的提升作为土壤的，被视为 GPU “核弹”的 NVIDIA H100 正面临有史以来最严重的缺货。Sam Altman 就直接表示，GPU 的短缺限制了 OpenAI 在微调、专用容量、32K 上下文窗口、多模态等方面的技术升级速度。 本文编译自 GPU Utils，作者主要从供需两个视角的分析探讨了 GPU（尤其是 NVIDIA H100）会持续多久。 需求视角下，NVIDIA H100 毋庸置疑是训练大模型的刚需，根据估算，目前市场上对 H100 的需求在 43.2 万张左右，若按每张约 3.5 万美元计算，这是相当于总价值约为 150 亿美元的 GPU，这 43.2 万的数字尚未包括像字节跳动（TikTok）、百度和腾讯这样需要大量 H800 的企业。 站在供给侧，H100 的短缺直接受限于台积电产能，且短期内， NVIDIA 并没有其他可选择的芯片工厂。因为出货量有限， NVIDIA 对于如何分配这些 GPU 也有自己的策略，对于 NVIDIA 来说，如何保证这些有限的 GPU 流向 AI 黑马而非 Google、微软、AWS 这些潜在竞争者相当重要。 这场围绕着 H100 的 AI 军火竞赛要持续多久？答案尚不明朗。虽然 NVIDIA 表示下半年会增加供给，但目前看来 GPU 的短缺可能会持续到 2024 年。 围绕着 H100 的短缺，接下来市场上或许会进入一种“恶性循环”：稀缺性导致 GPU 容量被视为 AI 公司的护城河，从而导致了更多的 GPU 囤积，而这又进一步加剧了 GPU 的稀缺。 以下为本文目录，建议结合要点进行针对性阅读。 👇 01 背景 02 H100 的需求分析 03 H100 供给侧分析 04 如何获得 H100 05 总结 01. 背景 直到 2023 年 8 月，人工智能领域的发展一直受到 GPU 供应瓶颈的制约。 “人工智能热潮被低估的原因之一是 GPU/TPU 短缺。GPU 和 TPU 的短缺限制了产品推广速度和模型训练进度，但这些限制很隐蔽。我们看到的主要是 NVIDIA 的股价飙升，而非研发进度受限。当供需达到平衡时，情况会有所好转。 —— Adam D'Angelo，Quora、Poe.com 首席执行官，前 Facebook 首席技术官 这些是对 GPU 供需和人工智能最重要的 CEO 和公司 Sam Altman 则表示，GPU 的短缺限制了 OpenAI 的项目进展，例如微调、专用容量、32K 上下文窗口、多模态等。 小型和大型云提供商的大规模 H100 集群容量即将消耗殆尽。 “每个人都希望 NVIDIA 能生产更多的 A/H100”。 —— 来自云提供商执行人员的信息 “由于当下 GPU 短缺的情况，对 OpenAI 来说，使用我们产品的人越少， 对我们反而越好”； “如果大家因为我们缺乏足够 GPU 而减少对 OpenAI 产品的使用，我们实际上会很高兴”。 —— Sam Altman，OpenAI 首席执行官 Sam Altman 这句话一方面巧妙地展现了 OpenAI 的产品已经深受全球用户的喜欢，但同时也说明了另外一个事实，即 OpenAI 确实需要更多的 GPU 来进一步推广、升级其功能。 Azure 和微软方面也面临类似情况，有匿名人士提到： • 公司内部正在限制员工使用 GPU，大家必须像 20 世纪 70 年代的大学生为了使用电脑那样排队申请算力。从我的角度来看，OpenAI 当下正在吸走所有 GPU 资源； • 今年 6 月，微软和 CoreWeave 的合作本质上是为了增强微软的 GPU/算力供给。 CoreWeave ： 云算力服务供应商，据 CoreWeave 官网宣传，他们的服务比传统云计算厂商便宜 80%。2023 年 4 月，CoreWeave 获得 NVIDIA 的 B 轮投资，并获得了大量 H100 新卡，6 月，微软也与 CoreWeave 签订协议，微软会在未来数年内投资数十亿美元，用于云计算基础设施建设。 7 月，CoreWeave 推出了与 NVIDIA 合作打造的世界上最快的 AI 超级计算机项目，以及 Inflection AI 使用支持 MLPerf 提交的基础设施在 CoreWeave Cloud 上创建世界上最复杂的大型语言模型之一。此外，CoreWeave 利用手中的 NVIDIA H100 加速卡作为抵押，于 8 月宣布完成 23 亿美元的债务融资。 总结来说， H100 GPU 的供应已经相当短缺。甚至有传言说，Azure 和 GCP 的容量实际上已经用完，AWS 的容量也快用尽。 而之所以短缺，是因为 NVIDIA 给到这些云供应商的 H100 GPU 供给也就这么多，随着 NVIDIA 的 H100 GPU 产量无法满足需求，这些云供应商可以提供的算力自然也开始出现短缺。 如果要理解算力瓶颈，可以围绕以下几个问题展开： • 造成这种情况的具体原因有哪些？: - 需求量有多大？如哪些领域的人工智能需求量增加相对迅速； - 供应量有多大？NVIDIA 等 GPU 生产商的产能是否足够满足需求； • 这种短缺情况会持续多久？GPU 的供需何时会逐渐达到平衡点？ • 有哪些方式可以有效缓解这种短缺局面？ 欢迎关注海外独角兽视频号 获取最前沿的科技行业资讯 02. H100 的需求分析 从需求端分析算力瓶颈的关键问题： 1.\"\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "token_count = len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50381a55-4c04-46dd-a78c-8de37af97160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
