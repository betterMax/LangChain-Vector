{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02819df-9d1c-4c56-8fc0-9ecaac410f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcc0d86-7b59-4efe-958a-6c3040bdaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å½“å‰windows\n",
    "# è·å–å½“å‰çš„ Conda ç¯å¢ƒè·¯å¾„\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "\n",
    "# \".env\" æ–‡ä»¶çš„ç»å¯¹è·¯å¾„\n",
    "dotenv_path = os.path.join(conda_env_path, '.env')\n",
    "\n",
    "# åŠ è½½ \".env\" æ–‡ä»¶\n",
    "_ = load_dotenv(dotenv_path, verbose=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399910a0-9794-4a22-83c9-7ccaf75732ea",
   "metadata": {},
   "source": [
    "# News Articles Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2006e3-458e-4b98-aabf-452940915d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "Dumping model to file cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.0418918132781982 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: â€œ11å²ç”·å­©ç•™é—ä¹¦è·³æ¥¼æ¡ˆâ€ä¸€å®¡å®£åˆ¤ï¼Œç­ä¸»ä»»è¢«è®¤å®šæ— ç½ª-å¤§è±¡ç½‘\n",
      "Text: 8æœˆ9æ—¥ï¼Œæ±Ÿè¥¿11å²ç”·å­©è·³æ¥¼è‡ªæ€ï¼Œç•™ä¸‹é—ä¹¦æŒ‡è®¤ç­ä¸»ä»»ç”¨æš´åŠ›æ‰‹æ®µï¼Œå…¶çˆ¶æ¯åˆ‘äº‹è‡ªè¯‰å­©å­ç­ä¸»ä»»ä¾®è¾±ç½ªã€è™å¾…è¢«çœ‹æŠ¤äººç½ªä¸€æ¡ˆï¼Œåœ¨æ±Ÿè¥¿çœä¹æ±Ÿå¸‚æ¿‚æºªåŒºäººæ°‘æ³•é™¢ä¸€å®¡å¼€åº­å®£åˆ¤ï¼Œæ³•é™¢è®¤å®šè¢«å‘Šé‚¹æŸä¸æ„æˆä¾®è¾±ç½ªã€è™å¾…è¢«çœ‹æŠ¤äººç½ªï¼Œå®£åˆ¤è¢«å‘Šé‚¹æŸæ— ç½ªã€‚ç”·å­©çˆ¶äº²å¼ å®šæ°å½“åº­æå‡ºä¸Šè¯‰ã€‚\n",
      "\n",
      "æ®æ­¤å‰æŠ¥é“ï¼Œ2021å¹´11æœˆ9æ—¥ï¼Œæ±Ÿè¥¿ä¹æ±Ÿå¸‚11å²ç”·å­©å®½å®½åœ¨æ”¾å­¦åï¼Œåœ¨è‡ªå®¶å°åŒºé«˜æ¥¼è·³æ¥¼è‡ªæ€ï¼Œåæ¥å…¶çˆ¶æ¯åœ¨å®½å®½çš„èº«ä¸Šæ‰¾åˆ°ä¸€å°é—ä¹¦ï¼Œå†…å®¹ä¸ºæŒ‡è®¤ç­ä¸»ä»»é‚¹æŸå¯¹å…¶ä½¿ç”¨æš´åŠ›ã€‚åœ¨äº‹å‘åå®½å®½çˆ¶æ¯åœ¨æŸ¥çœ‹æ•™å®¤å½“å¤©åŠå‰åŠä¸ªæœˆç›‘æ§å‘ç°ï¼Œé‚¹æŸå‡ æ¬¡å¯¹å®½å®½è¿›è¡Œè¨€è¯­è®¥è®½ï¼Œç”šè‡³è¿˜æªèµ·å­¦ç”Ÿçš„è„¸ã€‚\n",
      "\n",
      "2021å¹´12æœˆï¼Œå¼ å®šæ°å¤«å¦‡å‘ä¹æ±Ÿå¸‚æ¿‚æºªåŒºæ³•é™¢æèµ·åˆ‘äº‹è‡ªè¯‰ï¼Œè¯·æ±‚åˆ¤å®šé‚¹æŸçŠ¯ä¾®è¾±ç½ªï¼Œè¿½ç©¶å…¶åˆ‘äº‹è´£ä»»ï¼Œå½“æœˆä¸­æ—¬ï¼Œæ³•é™¢ç«‹æ¡ˆã€‚2022å¹´7æœˆ10æ—¥å½“åœ°æ•™è‚²å±€é€šæŠ¥äº†å¯¹é‚¹æŸçš„å¤„ç†æ„è§ã€‚é€šæŠ¥æŒ‡å‡ºé‚¹æŸå­˜åœ¨â€œè¿è§„æ”¶å—æœåŠ¡å¯¹è±¡çº¢åŒ…ç¤¼é‡‘ï¼Œæ¥å—æœåŠ¡å¯¹è±¡å®´è¯·å’Œæ¥å—åºå±±ä½å®¿å®‰æ’ç­‰é—®é¢˜â€ã€â€œå¤šæ¬¡å¯¹å¤šåå­¦ç”Ÿæœ‰è®¥è®½ã€æ­§è§†è¡Œä¸ºç­‰è¿åå¸ˆå¾·å¸ˆé£çš„é—®é¢˜â€ï¼Œç»™äºˆé‚¹æŸå…šå†…ä¸¥é‡è­¦å‘Šå¤„åˆ†ã€é™ä½ä¸¤ä¸ªå²—ä½ç­‰çº§å¤„åˆ†å’Œè°ƒç¦»æ•™å­¦å²—ä½å¤„ç†ã€‚\n",
      "\n",
      "2023å¹´4æœˆ29æ—¥æ³•é™¢ä¸€å®¡å¼€åº­å®¡ç†æ­¤æ¡ˆï¼Œç”·å­©çˆ¶äº²å¼ å®šæ°åœ¨æ¥å—æ‰¬å­æ™šæŠ¥/ç´«ç‰›æ–°é—»è®°è€…é‡‡è®¿æ—¶è¯´ï¼Œâ€œè¢«å‘Šé‚¹æŸå‡ºåº­äº†ï¼Œçœ‹åˆ°å¥¹æ€åº¦ä¾æ—§å¾ˆé¡½å›ºï¼Œæ²¡æœ‰ä¸æ¯«è®¤ç½ªè®¤ç½šçš„æ€åº¦ï¼Œä¸€ç›´åœ¨é€ƒé¿è´£ä»»ï¼Œå¯¹äºæ­¤å‰æ•™è‚²å±€é€šæŠ¥çš„å¥¹è®½åˆºæŒ–è‹¦ã€ä¾®è¾±æ­§è§†å­¦ç”Ÿçš„è¡Œä¸ºæ‹’ä¸æ‰¿è®¤ï¼Œå¹¶ç§°å¥¹è¿™æ˜¯å¹½é»˜å¼æ•™å­¦ï¼Œå¯¹æ­¤æˆ‘æ„Ÿåˆ°éå¸¸æ°”æ„¤ã€‚â€\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_url = \"https://www.hntv.tv/50rd/article/1/1689125963426394113?v=1.0\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "try:\n",
    "    response = session.get(article_url, headers=headers, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Text: {article.text}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to fetch article at {article_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b979b7e6-3016-4ba6-8dd8-5963fd66e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ±Ÿè¥¿ä¸€å11å²ç”·å­©è·³æ¥¼è‡ªæ€ï¼Œç•™ä¸‹é—ä¹¦æŒ‡æ§ç­ä¸»ä»»ä½¿ç”¨æš´åŠ›æ‰‹æ®µã€‚ç”·å­©çš„çˆ¶æ¯æèµ·åˆ‘äº‹è‡ªè¯‰ï¼ŒæŒ‡æ§ç­ä¸»ä»»ä¾®è¾±ç½ªå’Œè™å¾…è¢«çœ‹æŠ¤äººç½ªã€‚ç„¶è€Œï¼Œæ±Ÿè¥¿çœä¹æ±Ÿå¸‚æ¿‚æºªåŒºäººæ°‘æ³•é™¢ä¸€å®¡å®£åˆ¤è®¤å®šè¢«å‘Šé‚¹æŸæ— ç½ªã€‚ç”·å­©çš„çˆ¶äº²æå‡ºä¸Šè¯‰ã€‚æ­¤å‰ï¼Œæ•™è‚²å±€é€šæŠ¥äº†å¯¹é‚¹æŸçš„å¤„ç†æ„è§ï¼ŒæŒ‡å‡ºå¥¹å­˜åœ¨å¤šæ¬¡å¯¹å­¦ç”Ÿè¿›è¡Œè®¥è®½ã€æ­§è§†ç­‰è¿åå¸ˆå¾·å¸ˆé£çš„é—®é¢˜ã€‚ç”·å­©çš„çˆ¶äº²å¯¹é‚¹æŸçš„æ€åº¦æ„Ÿåˆ°æ°”æ„¤ï¼Œè®¤ä¸ºå¥¹ä¸€ç›´åœ¨é€ƒé¿è´£ä»»ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in Chinese.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# generate summary\n",
    "summary = chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a6ffdc-a0df-4fdb-bde6-55d200a9ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(article_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e130d42-25a9-468a-835f-08ab5df525de",
   "metadata": {},
   "source": [
    "# WeChat Article Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e40e007-c52a-415f-954e-08481f329693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07e770f1-a8fe-4f18-af70-938b4af9ed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H100 ä¾›éœ€åˆ†æï¼šèŠ¯ç‰‡æˆ˜äº‰å°†æŒç»­å¤šä¹…ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "\n",
    "session = HTMLSession()\n",
    "url = 'https://mp.weixin.qq.com/s/arwikTbqsGv8hBb4aqaXMA'\n",
    "r = session.get(url)\n",
    "a = r.html.text.split('\\n')\n",
    "\n",
    "# æå–æ ‡é¢˜\n",
    "title = r.html.find('h1', first=True).text\n",
    "\n",
    "flag = False\n",
    "wechat_article_text = \"\"  # åˆ›å»ºä¸€ä¸ªç©ºå­—ç¬¦ä¸²å˜é‡ç”¨äºä¿å­˜æ–‡ç« å†…å®¹\n",
    "\n",
    "for num in range(len(a)):\n",
    "    if 'åŠŸèƒ½ä»‹ç»' in a[num]:\n",
    "        flag = True\n",
    "        continue\n",
    "    elif 'var first_sceen__time' in a[num]:\n",
    "        flag = False\n",
    "    elif flag:\n",
    "        wechat_article_text += a[num] + '\\n'  # å°†æ¯è¡Œå†…å®¹æ·»åŠ åˆ°å­—ç¬¦ä¸²å˜é‡ä¸­\n",
    "\n",
    "# æ¸…é™¤è¿ç»­ä¸¤ä¸ªä»¥ä¸Šçš„æ¢è¡Œ\n",
    "import re\n",
    "wechat_article_text = re.sub('\\n{2,}', '\\n', wechat_article_text)\n",
    "# print(title)\n",
    "# print(wechat_article_text)  # è¾“å‡ºæˆ–éšåæ‚¨å¯ä»¥å¯¹wechat_article_textè¿›è¡Œå…¶ä»–æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0330928-802f-436e-b583-d19f4beb5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in Chinese.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# generate summary\n",
    "summary = chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb88cc-77da-43ee-9c0f-239e794aa7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21eec9ea-165b-48d8-b30f-be1c8788f79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 9\n",
      "length of segment: 2364\n",
      "å‘è¡¨äº æ”¶å½•äºåˆé›† #AI/ML 53ä¸ª ä½œè€…ï¼šClay Pascal ç¼–è¯‘ï¼šwenliï¼Œ Lavidaï¼Œ yunhao æ¨èäººï¼šCageï¼Œ Huaiwei æ’ç‰ˆï¼šScout å¤§æ¨¡å‹çš„çªç ´æ˜¯ä»¥ç¡¬ä»¶ç®—åŠ›å’Œäº‘è®¡ç®—èƒ½åŠ›çš„æå‡ä½œä¸ºåœŸå£¤çš„ï¼Œè¢«è§†ä¸º GPU â€œæ ¸å¼¹â€çš„ NVIDIA H100 æ­£é¢ä¸´æœ‰å²ä»¥æ¥æœ€ä¸¥é‡çš„ç¼ºè´§ã€‚Sam Altman å°±ç›´æ¥è¡¨ç¤ºï¼ŒGPU çš„çŸ­ç¼ºé™åˆ¶äº† OpenAI åœ¨å¾®è°ƒã€ä¸“ç”¨å®¹é‡ã€32K ä¸Šä¸‹æ–‡çª—å£ã€å¤šæ¨¡æ€ç­‰æ–¹é¢çš„æŠ€æœ¯å‡çº§é€Ÿåº¦ã€‚ æœ¬æ–‡ç¼–è¯‘è‡ª GPU Utilsï¼Œä½œè€…ä¸»è¦ä»ä¾›éœ€ä¸¤ä¸ªè§†è§’çš„åˆ†ææ¢è®¨äº† GPUï¼ˆå°¤å…¶æ˜¯ NVIDIA H100ï¼‰ä¼šæŒç»­å¤šä¹…ã€‚ éœ€æ±‚è§†è§’ä¸‹ï¼ŒNVIDIA H100 æ¯‹åº¸ç½®ç–‘æ˜¯è®­ç»ƒå¤§æ¨¡å‹çš„åˆšéœ€ï¼Œæ ¹æ®ä¼°ç®—ï¼Œç›®å‰å¸‚åœºä¸Šå¯¹ H100 çš„éœ€æ±‚åœ¨ 43.2 ä¸‡å¼ å·¦å³ï¼Œè‹¥æŒ‰æ¯å¼ çº¦ 3.5 ä¸‡ç¾å…ƒè®¡ç®—ï¼Œè¿™æ˜¯ç›¸å½“äºæ€»ä»·å€¼çº¦ä¸º 150 äº¿ç¾å…ƒçš„ GPUï¼Œè¿™ 43.2 ä¸‡çš„æ•°å­—å°šæœªåŒ…æ‹¬åƒå­—èŠ‚è·³åŠ¨ï¼ˆTikTokï¼‰ã€ç™¾åº¦å’Œè…¾è®¯è¿™æ ·éœ€è¦å¤§é‡ H800 çš„ä¼ä¸šã€‚ ç«™åœ¨ä¾›ç»™ä¾§ï¼ŒH100 çš„çŸ­ç¼ºç›´æ¥å—é™äºå°ç§¯ç”µäº§èƒ½ï¼Œä¸”çŸ­æœŸå†…ï¼Œ NVIDIA å¹¶æ²¡æœ‰å…¶ä»–å¯é€‰æ‹©çš„èŠ¯ç‰‡å·¥å‚ã€‚å› ä¸ºå‡ºè´§é‡æœ‰é™ï¼Œ NVIDIA å¯¹äºå¦‚ä½•åˆ†é…è¿™äº› GPU ä¹Ÿæœ‰è‡ªå·±çš„ç­–ç•¥ï¼Œå¯¹äº NVIDIA æ¥è¯´ï¼Œå¦‚ä½•ä¿è¯è¿™äº›æœ‰é™çš„ GPU æµå‘ AI é»‘é©¬è€Œé Googleã€å¾®è½¯ã€AWS è¿™äº›æ½œåœ¨ç«äº‰è€…ç›¸å½“é‡è¦ã€‚ è¿™åœºå›´ç»•ç€ H100 çš„ AI å†›ç«ç«èµ›è¦æŒç»­å¤šä¹…ï¼Ÿç­”æ¡ˆå°šä¸æ˜æœ—ã€‚è™½ç„¶ NVIDIA è¡¨ç¤ºä¸‹åŠå¹´ä¼šå¢åŠ ä¾›ç»™ï¼Œä½†ç›®å‰çœ‹æ¥ GPU çš„çŸ­ç¼ºå¯èƒ½ä¼šæŒç»­åˆ° 2024 å¹´ã€‚ å›´ç»•ç€ H100 çš„çŸ­ç¼ºï¼Œæ¥ä¸‹æ¥å¸‚åœºä¸Šæˆ–è®¸ä¼šè¿›å…¥ä¸€ç§â€œæ¶æ€§å¾ªç¯â€ï¼šç¨€ç¼ºæ€§å¯¼è‡´ GPU å®¹é‡è¢«è§†ä¸º AI å…¬å¸çš„æŠ¤åŸæ²³ï¼Œä»è€Œå¯¼è‡´äº†æ›´å¤šçš„ GPU å›¤ç§¯ï¼Œè€Œè¿™åˆè¿›ä¸€æ­¥åŠ å‰§äº† GPU çš„ç¨€ç¼ºã€‚ ä»¥ä¸‹ä¸ºæœ¬æ–‡ç›®å½•ï¼Œå»ºè®®ç»“åˆè¦ç‚¹è¿›è¡Œé’ˆå¯¹æ€§é˜…è¯»ã€‚ ğŸ‘‡ 01 èƒŒæ™¯ 02 H100 çš„éœ€æ±‚åˆ†æ 03 H100 ä¾›ç»™ä¾§åˆ†æ 04 å¦‚ä½•è·å¾— H100 05 æ€»ç»“ 01. èƒŒæ™¯ ç›´åˆ° 2023 å¹´ 8 æœˆï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ä¸€ç›´å—åˆ° GPU ä¾›åº”ç“¶é¢ˆçš„åˆ¶çº¦ã€‚ â€œäººå·¥æ™ºèƒ½çƒ­æ½®è¢«ä½ä¼°çš„åŸå› ä¹‹ä¸€æ˜¯ GPU/TPU çŸ­ç¼ºã€‚GPU å’Œ TPU çš„çŸ­ç¼ºé™åˆ¶äº†äº§å“æ¨å¹¿é€Ÿåº¦å’Œæ¨¡å‹è®­ç»ƒè¿›åº¦ï¼Œä½†è¿™äº›é™åˆ¶å¾ˆéšè”½ã€‚æˆ‘ä»¬çœ‹åˆ°çš„ä¸»è¦æ˜¯ NVIDIA çš„è‚¡ä»·é£™å‡ï¼Œè€Œéç ”å‘è¿›åº¦å—é™ã€‚å½“ä¾›éœ€è¾¾åˆ°å¹³è¡¡æ—¶ï¼Œæƒ…å†µä¼šæœ‰æ‰€å¥½è½¬ã€‚ â€”â€” Adam D'Angeloï¼ŒQuoraã€Poe.com é¦–å¸­æ‰§è¡Œå®˜ï¼Œå‰ Facebook é¦–å¸­æŠ€æœ¯å®˜ è¿™äº›æ˜¯å¯¹ GPU ä¾›éœ€å’Œäººå·¥æ™ºèƒ½æœ€é‡è¦çš„ CEO å’Œå…¬å¸ Sam Altman åˆ™è¡¨ç¤ºï¼ŒGPU çš„çŸ­ç¼ºé™åˆ¶äº† OpenAI çš„é¡¹ç›®è¿›å±•ï¼Œä¾‹å¦‚å¾®è°ƒã€ä¸“ç”¨å®¹é‡ã€32K ä¸Šä¸‹æ–‡çª—å£ã€å¤šæ¨¡æ€ç­‰ã€‚ å°å‹å’Œå¤§å‹äº‘æä¾›å•†çš„å¤§è§„æ¨¡ H100 é›†ç¾¤å®¹é‡å³å°†æ¶ˆè€—æ®†å°½ã€‚ â€œæ¯ä¸ªäººéƒ½å¸Œæœ› NVIDIA èƒ½ç”Ÿäº§æ›´å¤šçš„ A/H100â€ã€‚ â€”â€” æ¥è‡ªäº‘æä¾›å•†æ‰§è¡Œäººå‘˜çš„ä¿¡æ¯ â€œç”±äºå½“ä¸‹ GPU çŸ­ç¼ºçš„æƒ…å†µï¼Œå¯¹ OpenAI æ¥è¯´ï¼Œä½¿ç”¨æˆ‘ä»¬äº§å“çš„äººè¶Šå°‘ï¼Œ å¯¹æˆ‘ä»¬åè€Œè¶Šå¥½â€ï¼› â€œå¦‚æœå¤§å®¶å› ä¸ºæˆ‘ä»¬ç¼ºä¹è¶³å¤Ÿ GPU è€Œå‡å°‘å¯¹ OpenAI äº§å“çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¼šå¾ˆé«˜å…´â€ã€‚ â€”â€” Sam Altmanï¼ŒOpenAI é¦–å¸­æ‰§è¡Œå®˜ Sam Altman è¿™å¥è¯ä¸€æ–¹é¢å·§å¦™åœ°å±•ç°äº† OpenAI çš„äº§å“å·²ç»æ·±å—å…¨çƒç”¨æˆ·çš„å–œæ¬¢ï¼Œä½†åŒæ—¶ä¹Ÿè¯´æ˜äº†å¦å¤–ä¸€ä¸ªäº‹å®ï¼Œå³ OpenAI ç¡®å®éœ€è¦æ›´å¤šçš„ GPU æ¥è¿›ä¸€æ­¥æ¨å¹¿ã€å‡çº§å…¶åŠŸèƒ½ã€‚ Azure å’Œå¾®è½¯æ–¹é¢ä¹Ÿé¢ä¸´ç±»ä¼¼æƒ…å†µï¼Œæœ‰åŒ¿åäººå£«æåˆ°ï¼š â€¢ å…¬å¸å†…éƒ¨æ­£åœ¨é™åˆ¶å‘˜å·¥ä½¿ç”¨ GPUï¼Œå¤§å®¶å¿…é¡»åƒ 20 ä¸–çºª 70 å¹´ä»£çš„å¤§å­¦ç”Ÿä¸ºäº†ä½¿ç”¨ç”µè„‘é‚£æ ·æ’é˜Ÿç”³è¯·ç®—åŠ›ã€‚ä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼ŒOpenAI å½“ä¸‹æ­£åœ¨å¸èµ°æ‰€æœ‰ GPU èµ„æºï¼› â€¢ ä»Šå¹´ 6 æœˆï¼Œå¾®è½¯å’Œ CoreWeave çš„åˆä½œæœ¬è´¨ä¸Šæ˜¯ä¸ºäº†å¢å¼ºå¾®è½¯çš„ GPU/ç®—åŠ›ä¾›ç»™ã€‚ CoreWeave ï¼š äº‘ç®—åŠ›æœåŠ¡ä¾›åº”å•†ï¼Œæ® CoreWeave å®˜ç½‘å®£ä¼ ï¼Œä»–ä»¬çš„æœåŠ¡æ¯”ä¼ ç»Ÿäº‘è®¡ç®—å‚å•†ä¾¿å®œ 80%ã€‚2023 å¹´ 4 æœˆï¼ŒCoreWeave è·å¾— NVIDIA çš„ B è½®æŠ•èµ„ï¼Œå¹¶è·å¾—äº†å¤§é‡ H100 æ–°å¡ï¼Œ6 æœˆï¼Œå¾®è½¯ä¹Ÿä¸ CoreWeave ç­¾è®¢åè®®ï¼Œå¾®è½¯ä¼šåœ¨æœªæ¥æ•°å¹´å†…æŠ•èµ„æ•°åäº¿ç¾å…ƒï¼Œç”¨äºäº‘è®¡ç®—åŸºç¡€è®¾æ–½å»ºè®¾ã€‚ 7 æœˆï¼ŒCoreWeave æ¨å‡ºäº†ä¸ NVIDIA åˆä½œæ‰“é€ çš„ä¸–ç•Œä¸Šæœ€å¿«çš„ AI è¶…çº§è®¡ç®—æœºé¡¹ç›®ï¼Œä»¥åŠ Inflection AI ä½¿ç”¨æ”¯æŒ MLPerf æäº¤çš„åŸºç¡€è®¾æ–½åœ¨ CoreWeave Cloud ä¸Šåˆ›å»ºä¸–ç•Œä¸Šæœ€å¤æ‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸€ã€‚æ­¤å¤–ï¼ŒCoreWeave åˆ©ç”¨æ‰‹ä¸­çš„ NVIDIA H100 åŠ é€Ÿå¡ä½œä¸ºæŠµæŠ¼ï¼Œäº 8 æœˆå®£å¸ƒå®Œæˆ 23 äº¿ç¾å…ƒçš„å€ºåŠ¡èèµ„ã€‚ æ€»ç»“æ¥è¯´ï¼Œ H100 GPU çš„ä¾›åº”å·²ç»ç›¸å½“çŸ­ç¼ºã€‚ç”šè‡³æœ‰ä¼ è¨€è¯´ï¼ŒAzure å’Œ GCP çš„å®¹é‡å®é™…ä¸Šå·²ç»ç”¨å®Œï¼ŒAWS çš„å®¹é‡ä¹Ÿå¿«ç”¨å°½ã€‚ è€Œä¹‹æ‰€ä»¥çŸ­ç¼ºï¼Œæ˜¯å› ä¸º NVIDIA ç»™åˆ°è¿™äº›äº‘ä¾›åº”å•†çš„ H100 GPU ä¾›ç»™ä¹Ÿå°±è¿™ä¹ˆå¤šï¼Œéšç€ NVIDIA çš„ H100 GPU äº§é‡æ— æ³•æ»¡è¶³éœ€æ±‚ï¼Œè¿™äº›äº‘ä¾›åº”å•†å¯ä»¥æä¾›çš„ç®—åŠ›è‡ªç„¶ä¹Ÿå¼€å§‹å‡ºç°çŸ­ç¼ºã€‚ å¦‚æœè¦ç†è§£ç®—åŠ›ç“¶é¢ˆï¼Œå¯ä»¥å›´ç»•ä»¥ä¸‹å‡ ä¸ªé—®é¢˜å±•å¼€ï¼š â€¢ é€ æˆè¿™ç§æƒ…å†µçš„å…·ä½“åŸå› æœ‰å“ªäº›ï¼Ÿ: - éœ€æ±‚é‡æœ‰å¤šå¤§ï¼Ÿå¦‚å“ªäº›é¢†åŸŸçš„äººå·¥æ™ºèƒ½éœ€æ±‚é‡å¢åŠ ç›¸å¯¹è¿…é€Ÿï¼› - ä¾›åº”é‡æœ‰å¤šå¤§ï¼ŸNVIDIA ç­‰ GPU ç”Ÿäº§å•†çš„äº§èƒ½æ˜¯å¦è¶³å¤Ÿæ»¡è¶³éœ€æ±‚ï¼› â€¢ è¿™ç§çŸ­ç¼ºæƒ…å†µä¼šæŒç»­å¤šä¹…ï¼ŸGPU çš„ä¾›éœ€ä½•æ—¶ä¼šé€æ¸è¾¾åˆ°å¹³è¡¡ç‚¹ï¼Ÿ â€¢ æœ‰å“ªäº›æ–¹å¼å¯ä»¥æœ‰æ•ˆç¼“è§£è¿™ç§çŸ­ç¼ºå±€é¢ï¼Ÿ æ¬¢è¿å…³æ³¨æµ·å¤–ç‹¬è§’å…½è§†é¢‘å· è·å–æœ€å‰æ²¿çš„ç§‘æŠ€è¡Œä¸šèµ„è®¯ 02. H100 çš„éœ€æ±‚åˆ†æ ä»éœ€æ±‚ç«¯åˆ†æç®—åŠ›ç“¶é¢ˆçš„å…³é”®é—®é¢˜ï¼š 1.\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 17232 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m     89\u001b[0m article \u001b[38;5;241m=\u001b[39m Article(title, wechat_article_text)\n\u001b[1;32m---> 90\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_summary)\n",
      "Cell \u001b[1;32mIn[23], line 71\u001b[0m, in \u001b[0;36mget_final_summary\u001b[1;34m(article_text, template, token_limit)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength of segment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(segment)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(segment)\n\u001b[1;32m---> 71\u001b[0m     segment_summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     intermediate_summaries\u001b[38;5;241m.\u001b[39mappend(segment_summary)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Merge intermediate summaries\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 58\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[1;34m(text, template)\u001b[0m\n\u001b[0;32m     56\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt)]\n\u001b[0;32m     57\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:208\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    203\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 208\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    209\u001b[0m         [messages], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    210\u001b[0m     )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:102\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    103\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[0;32m    104\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:94\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     99\u001b[0m     ]\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\base.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     99\u001b[0m     ]\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\openai.py:359\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m     message \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    352\u001b[0m         {\n\u001b[0;32m    353\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: inner_completion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    356\u001b[0m         }\n\u001b[0;32m    357\u001b[0m     )\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)])\n\u001b[1;32m--> 359\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\openai.py:307\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\langchain\\chat_models\\openai.py:305\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 17232 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Mocking a function to calculate tokens. \n",
    "# NOTE: Replace this with OpenAI's actual tokenizer for real-world usage.\n",
    "def count_tokens(text, token_ratio={\"english\": 1.5, \"chinese\": 1}):\n",
    "    # Check if the provided ratios are valid numbers\n",
    "    if not all(isinstance(value, (int, float)) for value in token_ratio.values()):\n",
    "        raise ValueError(\"Provided token ratios are not valid numbers.\")\n",
    "    \n",
    "    english_words = len([word for word in text.split() if any(char.isalpha() for char in word) and any(char.isascii() for char in word)])\n",
    "    chinese_chars = len([char for char in text if '\\u4e00' <= char <= '\\u9fff'])\n",
    "    \n",
    "    return english_words * token_ratio.get(\"english\", 1.5) + chinese_chars * token_ratio.get(\"chinese\", 1)\n",
    "\n",
    "# Function to split text into segments based on token limit\n",
    "def split_text_into_segments(text, token_limit):\n",
    "    segments = []\n",
    "    words = text.split()\n",
    "    current_segment = \"\"\n",
    "    current_token_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_token_count = count_tokens(word)\n",
    "        if current_token_count + word_token_count <= token_limit:\n",
    "            current_segment += \" \" + word\n",
    "            current_token_count += word_token_count\n",
    "        else:\n",
    "            segments.append(current_segment.strip())\n",
    "            current_segment = word\n",
    "            current_token_count = word_token_count\n",
    "    \n",
    "    if current_segment:\n",
    "        segments.append(current_segment.strip())\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in Chinese.\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate summary using OpenAI model\n",
    "def generate_summary(text, template):\n",
    "    # format prompt\n",
    "    prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    summary = chat(messages)\n",
    "    return summary.content\n",
    "\n",
    "def get_final_summary(article_text, template, token_limit=1500):\n",
    "    # Split article text into segments\n",
    "    text_segments = split_text_into_segments(article_text, token_limit)\n",
    "    print(f'Number of segments: {len(text_segments)}')\n",
    "\n",
    "    # Generate intermediate summaries for each segment\n",
    "    intermediate_summaries = []\n",
    "    for segment in text_segments:\n",
    "        print(f'length of segment: {len(segment)}')\n",
    "        print(segment)\n",
    "        segment_summary = generate_summary(segment, template)\n",
    "        intermediate_summaries.append(segment_summary)\n",
    "\n",
    "    # Merge intermediate summaries\n",
    "    combined_text = \" \".join(intermediate_summaries)\n",
    "    \n",
    "    # Generate the final summary from the combined intermediate summaries\n",
    "    final_summary = generate_summary(combined_text)\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Example usage:\n",
    "# Define article data\n",
    "class Article:\n",
    "    def __init__(self, title, text):\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "\n",
    "article = Article(title, wechat_article_text)\n",
    "final_summary = get_final_summary(article.text, template)\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dec13e9a-7afe-4b1b-86a7-6f6d64200d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 4.05MB/s]\n",
      "Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 2.97MB/s]\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 718/718 [00:00<00:00, 240kB/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3597 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "text = \"å‘è¡¨äº æ”¶å½•äºåˆé›† #AI/ML 53ä¸ª ä½œè€…ï¼šClay Pascal ç¼–è¯‘ï¼šwenliï¼Œ Lavidaï¼Œ yunhao æ¨èäººï¼šCageï¼Œ Huaiwei æ’ç‰ˆï¼šScout å¤§æ¨¡å‹çš„çªç ´æ˜¯ä»¥ç¡¬ä»¶ç®—åŠ›å’Œäº‘è®¡ç®—èƒ½åŠ›çš„æå‡ä½œä¸ºåœŸå£¤çš„ï¼Œè¢«è§†ä¸º GPU â€œæ ¸å¼¹â€çš„ NVIDIA H100 æ­£é¢ä¸´æœ‰å²ä»¥æ¥æœ€ä¸¥é‡çš„ç¼ºè´§ã€‚Sam Altman å°±ç›´æ¥è¡¨ç¤ºï¼ŒGPU çš„çŸ­ç¼ºé™åˆ¶äº† OpenAI åœ¨å¾®è°ƒã€ä¸“ç”¨å®¹é‡ã€32K ä¸Šä¸‹æ–‡çª—å£ã€å¤šæ¨¡æ€ç­‰æ–¹é¢çš„æŠ€æœ¯å‡çº§é€Ÿåº¦ã€‚ æœ¬æ–‡ç¼–è¯‘è‡ª GPU Utilsï¼Œä½œè€…ä¸»è¦ä»ä¾›éœ€ä¸¤ä¸ªè§†è§’çš„åˆ†ææ¢è®¨äº† GPUï¼ˆå°¤å…¶æ˜¯ NVIDIA H100ï¼‰ä¼šæŒç»­å¤šä¹…ã€‚ éœ€æ±‚è§†è§’ä¸‹ï¼ŒNVIDIA H100 æ¯‹åº¸ç½®ç–‘æ˜¯è®­ç»ƒå¤§æ¨¡å‹çš„åˆšéœ€ï¼Œæ ¹æ®ä¼°ç®—ï¼Œç›®å‰å¸‚åœºä¸Šå¯¹ H100 çš„éœ€æ±‚åœ¨ 43.2 ä¸‡å¼ å·¦å³ï¼Œè‹¥æŒ‰æ¯å¼ çº¦ 3.5 ä¸‡ç¾å…ƒè®¡ç®—ï¼Œè¿™æ˜¯ç›¸å½“äºæ€»ä»·å€¼çº¦ä¸º 150 äº¿ç¾å…ƒçš„ GPUï¼Œè¿™ 43.2 ä¸‡çš„æ•°å­—å°šæœªåŒ…æ‹¬åƒå­—èŠ‚è·³åŠ¨ï¼ˆTikTokï¼‰ã€ç™¾åº¦å’Œè…¾è®¯è¿™æ ·éœ€è¦å¤§é‡ H800 çš„ä¼ä¸šã€‚ ç«™åœ¨ä¾›ç»™ä¾§ï¼ŒH100 çš„çŸ­ç¼ºç›´æ¥å—é™äºå°ç§¯ç”µäº§èƒ½ï¼Œä¸”çŸ­æœŸå†…ï¼Œ NVIDIA å¹¶æ²¡æœ‰å…¶ä»–å¯é€‰æ‹©çš„èŠ¯ç‰‡å·¥å‚ã€‚å› ä¸ºå‡ºè´§é‡æœ‰é™ï¼Œ NVIDIA å¯¹äºå¦‚ä½•åˆ†é…è¿™äº› GPU ä¹Ÿæœ‰è‡ªå·±çš„ç­–ç•¥ï¼Œå¯¹äº NVIDIA æ¥è¯´ï¼Œå¦‚ä½•ä¿è¯è¿™äº›æœ‰é™çš„ GPU æµå‘ AI é»‘é©¬è€Œé Googleã€å¾®è½¯ã€AWS è¿™äº›æ½œåœ¨ç«äº‰è€…ç›¸å½“é‡è¦ã€‚ è¿™åœºå›´ç»•ç€ H100 çš„ AI å†›ç«ç«èµ›è¦æŒç»­å¤šä¹…ï¼Ÿç­”æ¡ˆå°šä¸æ˜æœ—ã€‚è™½ç„¶ NVIDIA è¡¨ç¤ºä¸‹åŠå¹´ä¼šå¢åŠ ä¾›ç»™ï¼Œä½†ç›®å‰çœ‹æ¥ GPU çš„çŸ­ç¼ºå¯èƒ½ä¼šæŒç»­åˆ° 2024 å¹´ã€‚ å›´ç»•ç€ H100 çš„çŸ­ç¼ºï¼Œæ¥ä¸‹æ¥å¸‚åœºä¸Šæˆ–è®¸ä¼šè¿›å…¥ä¸€ç§â€œæ¶æ€§å¾ªç¯â€ï¼šç¨€ç¼ºæ€§å¯¼è‡´ GPU å®¹é‡è¢«è§†ä¸º AI å…¬å¸çš„æŠ¤åŸæ²³ï¼Œä»è€Œå¯¼è‡´äº†æ›´å¤šçš„ GPU å›¤ç§¯ï¼Œè€Œè¿™åˆè¿›ä¸€æ­¥åŠ å‰§äº† GPU çš„ç¨€ç¼ºã€‚ ä»¥ä¸‹ä¸ºæœ¬æ–‡ç›®å½•ï¼Œå»ºè®®ç»“åˆè¦ç‚¹è¿›è¡Œé’ˆå¯¹æ€§é˜…è¯»ã€‚ ğŸ‘‡ 01 èƒŒæ™¯ 02 H100 çš„éœ€æ±‚åˆ†æ 03 H100 ä¾›ç»™ä¾§åˆ†æ 04 å¦‚ä½•è·å¾— H100 05 æ€»ç»“ 01. èƒŒæ™¯ ç›´åˆ° 2023 å¹´ 8 æœˆï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ä¸€ç›´å—åˆ° GPU ä¾›åº”ç“¶é¢ˆçš„åˆ¶çº¦ã€‚ â€œäººå·¥æ™ºèƒ½çƒ­æ½®è¢«ä½ä¼°çš„åŸå› ä¹‹ä¸€æ˜¯ GPU/TPU çŸ­ç¼ºã€‚GPU å’Œ TPU çš„çŸ­ç¼ºé™åˆ¶äº†äº§å“æ¨å¹¿é€Ÿåº¦å’Œæ¨¡å‹è®­ç»ƒè¿›åº¦ï¼Œä½†è¿™äº›é™åˆ¶å¾ˆéšè”½ã€‚æˆ‘ä»¬çœ‹åˆ°çš„ä¸»è¦æ˜¯ NVIDIA çš„è‚¡ä»·é£™å‡ï¼Œè€Œéç ”å‘è¿›åº¦å—é™ã€‚å½“ä¾›éœ€è¾¾åˆ°å¹³è¡¡æ—¶ï¼Œæƒ…å†µä¼šæœ‰æ‰€å¥½è½¬ã€‚ â€”â€” Adam D'Angeloï¼ŒQuoraã€Poe.com é¦–å¸­æ‰§è¡Œå®˜ï¼Œå‰ Facebook é¦–å¸­æŠ€æœ¯å®˜ è¿™äº›æ˜¯å¯¹ GPU ä¾›éœ€å’Œäººå·¥æ™ºèƒ½æœ€é‡è¦çš„ CEO å’Œå…¬å¸ Sam Altman åˆ™è¡¨ç¤ºï¼ŒGPU çš„çŸ­ç¼ºé™åˆ¶äº† OpenAI çš„é¡¹ç›®è¿›å±•ï¼Œä¾‹å¦‚å¾®è°ƒã€ä¸“ç”¨å®¹é‡ã€32K ä¸Šä¸‹æ–‡çª—å£ã€å¤šæ¨¡æ€ç­‰ã€‚ å°å‹å’Œå¤§å‹äº‘æä¾›å•†çš„å¤§è§„æ¨¡ H100 é›†ç¾¤å®¹é‡å³å°†æ¶ˆè€—æ®†å°½ã€‚ â€œæ¯ä¸ªäººéƒ½å¸Œæœ› NVIDIA èƒ½ç”Ÿäº§æ›´å¤šçš„ A/H100â€ã€‚ â€”â€” æ¥è‡ªäº‘æä¾›å•†æ‰§è¡Œäººå‘˜çš„ä¿¡æ¯ â€œç”±äºå½“ä¸‹ GPU çŸ­ç¼ºçš„æƒ…å†µï¼Œå¯¹ OpenAI æ¥è¯´ï¼Œä½¿ç”¨æˆ‘ä»¬äº§å“çš„äººè¶Šå°‘ï¼Œ å¯¹æˆ‘ä»¬åè€Œè¶Šå¥½â€ï¼› â€œå¦‚æœå¤§å®¶å› ä¸ºæˆ‘ä»¬ç¼ºä¹è¶³å¤Ÿ GPU è€Œå‡å°‘å¯¹ OpenAI äº§å“çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¼šå¾ˆé«˜å…´â€ã€‚ â€”â€” Sam Altmanï¼ŒOpenAI é¦–å¸­æ‰§è¡Œå®˜ Sam Altman è¿™å¥è¯ä¸€æ–¹é¢å·§å¦™åœ°å±•ç°äº† OpenAI çš„äº§å“å·²ç»æ·±å—å…¨çƒç”¨æˆ·çš„å–œæ¬¢ï¼Œä½†åŒæ—¶ä¹Ÿè¯´æ˜äº†å¦å¤–ä¸€ä¸ªäº‹å®ï¼Œå³ OpenAI ç¡®å®éœ€è¦æ›´å¤šçš„ GPU æ¥è¿›ä¸€æ­¥æ¨å¹¿ã€å‡çº§å…¶åŠŸèƒ½ã€‚ Azure å’Œå¾®è½¯æ–¹é¢ä¹Ÿé¢ä¸´ç±»ä¼¼æƒ…å†µï¼Œæœ‰åŒ¿åäººå£«æåˆ°ï¼š â€¢ å…¬å¸å†…éƒ¨æ­£åœ¨é™åˆ¶å‘˜å·¥ä½¿ç”¨ GPUï¼Œå¤§å®¶å¿…é¡»åƒ 20 ä¸–çºª 70 å¹´ä»£çš„å¤§å­¦ç”Ÿä¸ºäº†ä½¿ç”¨ç”µè„‘é‚£æ ·æ’é˜Ÿç”³è¯·ç®—åŠ›ã€‚ä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼ŒOpenAI å½“ä¸‹æ­£åœ¨å¸èµ°æ‰€æœ‰ GPU èµ„æºï¼› â€¢ ä»Šå¹´ 6 æœˆï¼Œå¾®è½¯å’Œ CoreWeave çš„åˆä½œæœ¬è´¨ä¸Šæ˜¯ä¸ºäº†å¢å¼ºå¾®è½¯çš„ GPU/ç®—åŠ›ä¾›ç»™ã€‚ CoreWeave ï¼š äº‘ç®—åŠ›æœåŠ¡ä¾›åº”å•†ï¼Œæ® CoreWeave å®˜ç½‘å®£ä¼ ï¼Œä»–ä»¬çš„æœåŠ¡æ¯”ä¼ ç»Ÿäº‘è®¡ç®—å‚å•†ä¾¿å®œ 80%ã€‚2023 å¹´ 4 æœˆï¼ŒCoreWeave è·å¾— NVIDIA çš„ B è½®æŠ•èµ„ï¼Œå¹¶è·å¾—äº†å¤§é‡ H100 æ–°å¡ï¼Œ6 æœˆï¼Œå¾®è½¯ä¹Ÿä¸ CoreWeave ç­¾è®¢åè®®ï¼Œå¾®è½¯ä¼šåœ¨æœªæ¥æ•°å¹´å†…æŠ•èµ„æ•°åäº¿ç¾å…ƒï¼Œç”¨äºäº‘è®¡ç®—åŸºç¡€è®¾æ–½å»ºè®¾ã€‚ 7 æœˆï¼ŒCoreWeave æ¨å‡ºäº†ä¸ NVIDIA åˆä½œæ‰“é€ çš„ä¸–ç•Œä¸Šæœ€å¿«çš„ AI è¶…çº§è®¡ç®—æœºé¡¹ç›®ï¼Œä»¥åŠ Inflection AI ä½¿ç”¨æ”¯æŒ MLPerf æäº¤çš„åŸºç¡€è®¾æ–½åœ¨ CoreWeave Cloud ä¸Šåˆ›å»ºä¸–ç•Œä¸Šæœ€å¤æ‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸€ã€‚æ­¤å¤–ï¼ŒCoreWeave åˆ©ç”¨æ‰‹ä¸­çš„ NVIDIA H100 åŠ é€Ÿå¡ä½œä¸ºæŠµæŠ¼ï¼Œäº 8 æœˆå®£å¸ƒå®Œæˆ 23 äº¿ç¾å…ƒçš„å€ºåŠ¡èèµ„ã€‚ æ€»ç»“æ¥è¯´ï¼Œ H100 GPU çš„ä¾›åº”å·²ç»ç›¸å½“çŸ­ç¼ºã€‚ç”šè‡³æœ‰ä¼ è¨€è¯´ï¼ŒAzure å’Œ GCP çš„å®¹é‡å®é™…ä¸Šå·²ç»ç”¨å®Œï¼ŒAWS çš„å®¹é‡ä¹Ÿå¿«ç”¨å°½ã€‚ è€Œä¹‹æ‰€ä»¥çŸ­ç¼ºï¼Œæ˜¯å› ä¸º NVIDIA ç»™åˆ°è¿™äº›äº‘ä¾›åº”å•†çš„ H100 GPU ä¾›ç»™ä¹Ÿå°±è¿™ä¹ˆå¤šï¼Œéšç€ NVIDIA çš„ H100 GPU äº§é‡æ— æ³•æ»¡è¶³éœ€æ±‚ï¼Œè¿™äº›äº‘ä¾›åº”å•†å¯ä»¥æä¾›çš„ç®—åŠ›è‡ªç„¶ä¹Ÿå¼€å§‹å‡ºç°çŸ­ç¼ºã€‚ å¦‚æœè¦ç†è§£ç®—åŠ›ç“¶é¢ˆï¼Œå¯ä»¥å›´ç»•ä»¥ä¸‹å‡ ä¸ªé—®é¢˜å±•å¼€ï¼š â€¢ é€ æˆè¿™ç§æƒ…å†µçš„å…·ä½“åŸå› æœ‰å“ªäº›ï¼Ÿ: - éœ€æ±‚é‡æœ‰å¤šå¤§ï¼Ÿå¦‚å“ªäº›é¢†åŸŸçš„äººå·¥æ™ºèƒ½éœ€æ±‚é‡å¢åŠ ç›¸å¯¹è¿…é€Ÿï¼› - ä¾›åº”é‡æœ‰å¤šå¤§ï¼ŸNVIDIA ç­‰ GPU ç”Ÿäº§å•†çš„äº§èƒ½æ˜¯å¦è¶³å¤Ÿæ»¡è¶³éœ€æ±‚ï¼› â€¢ è¿™ç§çŸ­ç¼ºæƒ…å†µä¼šæŒç»­å¤šä¹…ï¼ŸGPU çš„ä¾›éœ€ä½•æ—¶ä¼šé€æ¸è¾¾åˆ°å¹³è¡¡ç‚¹ï¼Ÿ â€¢ æœ‰å“ªäº›æ–¹å¼å¯ä»¥æœ‰æ•ˆç¼“è§£è¿™ç§çŸ­ç¼ºå±€é¢ï¼Ÿ æ¬¢è¿å…³æ³¨æµ·å¤–ç‹¬è§’å…½è§†é¢‘å· è·å–æœ€å‰æ²¿çš„ç§‘æŠ€è¡Œä¸šèµ„è®¯ 02. H100 çš„éœ€æ±‚åˆ†æ ä»éœ€æ±‚ç«¯åˆ†æç®—åŠ›ç“¶é¢ˆçš„å…³é”®é—®é¢˜ï¼š 1.\"\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "token_count = len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50381a55-4c04-46dd-a78c-8de37af97160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
