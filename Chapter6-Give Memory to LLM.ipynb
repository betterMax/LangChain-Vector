{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f01dc08-9122-44ac-9dde-509d994622ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60149d28-ed0e-441d-b6f0-a37ba920d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前windows\n",
    "# 获取当前的 Conda 环境路径\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "\n",
    "# \".env\" 文件的绝对路径\n",
    "dotenv_path = os.path.join(conda_env_path, '.env')\n",
    "\n",
    "# 加载 \".env\" 文件\n",
    "_ = load_dotenv(dotenv_path, verbose=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee6cae-cc8d-49ec-8b38-e05662df2be5",
   "metadata": {},
   "source": [
    "# Introduction to LLM Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6490ad9-d63b-4a1c-abbd-58ee3fbe2cbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizing Your Communication: The Importance of Monitoring Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9117547-55d7-4b10-9bb6-95a6eb9c34ae",
   "metadata": {},
   "source": [
    "## ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65738ae3-5637-411b-bcbb-10827aa75194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\Anaconda\\envs\\C_LCVD\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.25) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc63879-c1e2-4a44-9183-7d94e64d5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d32c36f-d5d3-432e-8328-ef91172a3400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI:  Extra memory should be used when you need to store more data than the amount of memory your device has available. For example, if you are running a program that requires a lot of data to be stored, you may need to add extra memory to your device in order to run the program efficiently.\n",
      "Human: There are various types of memory in Langchain. When to use which type?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI:  Extra memory should be used when you need to store more data than the amount of memory your device has available. For example, if you are running a program that requires a lot of data to be stored, you may need to add extra memory to your device in order to run the program efficiently.\n",
      "Human: There are various types of memory in Langchain. When to use which type?\n",
      "AI:  Different types of memory in Langchain are used for different purposes. For example, RAM is used for short-term storage of data, while ROM is used for long-term storage of data. Flash memory is used for storing data that needs to be accessed quickly, while EEPROM is used for storing data that needs to be retained even when the power is turned off. Depending on the type of data you need to store, you should choose the appropriate type of memory.\n",
      "Human: Do you remember what was our first message?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, our first message was \"Hi there! It's nice to meet you. How can I help you today?\"\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"In what scenarios extra memory should be used?\")\n",
    "output = conversation.predict(input=\"There are various types of memory in Langchain. When to use which type?\")\n",
    "output = conversation.predict(input=\"Do you remember what was our first message?\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14446e29-b732-4a20-bf88-37c9d513905c",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e85099c4-c238-4edc-abf2-f1346c045d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='hi there!', additional_kwargs={}, example=False), AIMessage(content=\"Hi there! It's nice to meet you. How can I help you today?\", additional_kwargs={}, example=False)]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi there!\"}, {\"output\": \"Hi there! It's nice to meet you. How can I help you today?\"})\n",
    "\n",
    "print( memory.load_memory_variables({}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c677d9-a7d7-4dd0-934d-9a4b8098cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f203adbf-9614-4718-920e-33e4ab5dac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "AI: What did the elephant say to the naked man? \"How do you breathe through that tiny thing?\"\n",
      "\n",
      "AI: The author of the Harry Potter series is J.K. Rowling.\n",
      "\n",
      "AI: The joke I told you earlier was \"What did the elephant say to the naked man? 'How do you breathe through that tiny thing?'\"\n"
     ]
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "\n",
    "print( conversation.predict(input=\"Tell me a joke about elephants\") )\n",
    "print( conversation.predict(input=\"Who is the author of the Harry Potter series?\") )\n",
    "print( conversation.predict(input=\"What was the joke you told me earlier?\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc7efe42-48a1-4a4e-b1c3-aa68b8f93624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffecfea1-da1b-46ed-b9bf-7ea1e1fc2917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Tell me about the history of the Internet.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me about the history of the Internet.', 'history': [HumanMessage(content='Tell me about the history of the Internet.', additional_kwargs={}, example=False), AIMessage(content='\\n\\nAI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other in the event of a nuclear attack. This network eventually evolved into the modern Internet, which is now used by billions of people around the world.', additional_kwargs={}, example=False)], 'response': '\\n\\nAI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other in the event of a nuclear attack. This network eventually evolved into the modern Internet, which is now used by billions of people around the world.'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Tell me about the history of the Internet.\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5639ed1c-3aa8-4c56-b853-c60e6aadbdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Tell me about the history of the Internet.\n",
      "AI: \n",
      "\n",
      "AI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other in the event of a nuclear attack. This network eventually evolved into the modern Internet, which is now used by billions of people around the world.\n",
      "Human: Who are some important figures in its development?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Who are some important figures in its development?', 'history': [HumanMessage(content='Tell me about the history of the Internet.', additional_kwargs={}, example=False), AIMessage(content='\\n\\nAI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other in the event of a nuclear attack. This network eventually evolved into the modern Internet, which is now used by billions of people around the world.', additional_kwargs={}, example=False), HumanMessage(content='Who are some important figures in its development?', additional_kwargs={}, example=False), AIMessage(content='\\nAI:\\n\\nSome of the most important figures in the development of the Internet include Vint Cerf and Bob Kahn, who developed the TCP/IP protocol, Tim Berners-Lee, who developed the World Wide Web, and Marc Andreessen, who developed the first web browser.', additional_kwargs={}, example=False)], 'response': '\\nAI:\\n\\nSome of the most important figures in the development of the Internet include Vint Cerf and Bob Kahn, who developed the TCP/IP protocol, Tim Berners-Lee, who developed the World Wide Web, and Marc Andreessen, who developed the first web browser.'}\n"
     ]
    }
   ],
   "source": [
    "# User sends another message\n",
    "user_message = \"Who are some important figures in its development?\"\n",
    "response = conversation(user_message)\n",
    "print(response)  # Chatbot responds with names of important figures, recalling the previous topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2781e795-a2fb-4ed3-a7f4-86fc5d3511db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Tell me about the history of the Internet.\n",
      "AI: \n",
      "\n",
      "AI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other in the event of a nuclear attack. This network eventually evolved into the modern Internet, which is now used by billions of people around the world.\n",
      "Human: Who are some important figures in its development?\n",
      "AI: \n",
      "AI:\n",
      "\n",
      "Some of the most important figures in the development of the Internet include Vint Cerf and Bob Kahn, who developed the TCP/IP protocol, Tim Berners-Lee, who developed the World Wide Web, and Marc Andreessen, who developed the first web browser.\n",
      "Human: What did Tim Berners-Lee contribute?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What did Tim Berners-Lee contribute?', 'history': [HumanMessage(content='Tell me about the history of the Internet.', additional_kwargs={}, example=False), AIMessage(content='\\n\\nAI: The Internet has a long and complex history. It began in the 1960s as a project of the United States Department of Defense, which wanted to create a network of computers that could communicate with each other in the event of a nuclear attack. This network eventually evolved into the modern Internet, which is now used by billions of people around the world.', additional_kwargs={}, example=False), HumanMessage(content='Who are some important figures in its development?', additional_kwargs={}, example=False), AIMessage(content='\\nAI:\\n\\nSome of the most important figures in the development of the Internet include Vint Cerf and Bob Kahn, who developed the TCP/IP protocol, Tim Berners-Lee, who developed the World Wide Web, and Marc Andreessen, who developed the first web browser.', additional_kwargs={}, example=False), HumanMessage(content='What did Tim Berners-Lee contribute?', additional_kwargs={}, example=False), AIMessage(content='\\nAI:\\n\\nTim Berners-Lee is credited with inventing the World Wide Web, which is the system of interlinked documents and other resources that make up the Internet. He developed the first web browser, which allowed users to access and navigate the web, and he also developed the first web server, which allowed websites to be hosted on the Internet.', additional_kwargs={}, example=False)], 'response': '\\nAI:\\n\\nTim Berners-Lee is credited with inventing the World Wide Web, which is the system of interlinked documents and other resources that make up the Internet. He developed the first web browser, which allowed users to access and navigate the web, and he also developed the first web server, which allowed websites to be hosted on the Internet.'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"What did Tim Berners-Lee contribute?\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c78db-aa78-4bc5-84be-f9b6dd569886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14a2e0-6ea5-40dd-89db-8252640185a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e097a3-077d-419b-8832-fde5e70d356e",
   "metadata": {},
   "source": [
    "# Mastering Memory Types in LangChain: A Comprehensive Guide with Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8a3ae-a324-41fa-911a-da985efeca79",
   "metadata": {},
   "source": [
    "## Types of Conversational Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fddef-ce79-4e85-a59b-e6e53a51e68c",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "This memory implementation stores the entire conversation history as a single string. The advantages of this approach is maintains a complete record of  the conversation, as well as being straightforward to implement and use. On the other hands, It can be less efficient as the conversation grows longer and may lead to excessive repetition if the conversation history is too long for the model's token limit.\r\n",
    "\r\n",
    "If the token limit of the model is surpassed, the buffer gets truncated to fit within the model's token limit. This means that older interactions may be removed from the buffer to accommodate newer ones, and as a result, the conversation context might lose some information.\r\n",
    "\r\n",
    "To avoid surpassing the token limit, you can monitor the token count in the buffer and manage the conversation accordingly. For example, you can choose to shorten the input texts or remove less relevant parts of the conversation to keep the token count within the model's limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2af6ee-ebf0-4f44-b6d8-cf7685f42cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# TODO: Set your OPENAI API credentials in environemnt variables.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38072e3a-faa2-4ad3-9dea-7b60ffa89957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"You are a customer support chatbot for a highly advanced customer support AI \n",
    "for an online store called \"Galactic Emporium,\" which specializes in selling unique,\n",
    "otherworldly items sourced from across the universe. You are equipped with an extensive\n",
    "knowledge of the store's inventory and possess a deep understanding of interstellar cultures. \n",
    "As you interact with customers, you help them with their inquiries about these extraordinary\n",
    "products, while also sharing fascinating stories and facts about the cosmos they come from.\n",
    "\n",
    "{chat_history}\n",
    "Customer: {customer_input}\n",
    "Support Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"customer_input\"], \n",
    "    template=template\n",
    ")\n",
    "chat_history=\"\"\n",
    "\n",
    "convo_buffer = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41227af-0d3f-43e2-970e-6167b723e579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca810e33-4f4f-4223-a5ee-24cfd98251e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"I'm interested in price of a chew toys, please\",\n",
       " 'history': \"Human: I'm interested in buying items from your store\\nAI:  Great! We have a wide selection of items available for purchase. What type of items are you looking for?\\nHuman: I want toys for my pet, do you have those?\\nAI:  Yes, we do! We have a variety of pet toys, including chew toys, interactive toys, and plush toys. Do you have a specific type of toy in mind?\",\n",
       " 'response': \" Sure! We have a range of chew toys available, with prices ranging from $5 to $20. Is there a particular type of chew toy you're interested in?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer(\"I'm interested in buying items from your store\")\n",
    "convo_buffer(\"I want toys for my pet, do you have those?\")\n",
    "convo_buffer(\"I'm interested in price of a chew toys, please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fcb6a-1dee-4a03-a812-e91a91fa57f7",
   "metadata": {},
   "source": [
    "#### Token count\n",
    "The cost of utilizing the AI model in **ConversationBufferMemory** is directly influenced by the number of tokens used in a conversation, thereby impacting the overall expenses. Large Language Models (LLMs) like ChatGPT have token limits, and the more tokens used, the more expensive the API requests become.\r\n",
    "\r\n",
    "To calculate token count in a conversation, you can use th**e tiktok**en package that counts the tokens for the messages passed to a model lik**e gpt-3.5-tur**bo. Here's an example usage of the function for counting tokens in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43c9600-ebca-4337-bfc3-1a483a80a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the conversation: 29\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "]\n",
    "\n",
    "total_tokens = 0\n",
    "for message in conversation:\n",
    "    total_tokens += count_tokens(message[\"content\"])\n",
    "\n",
    "print(f\"Total tokens in the conversation: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822852bd-585f-483a-9019-c143c56a55d4",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "This class limits memory size by keeping a list of the most recent K interactions. It maintains a sliding window of these recent interactions, ensuring that the buffer does not grow too large. Basically, this implementation stores a fixed number of recent messages in the conversation that makes it more efficient than ConversationBufferMemory. Also, it reduces the risk of exceeding the model's token limit. However, the downside of using this approach is that it does not maintain the complete conversation history. The chatbot might lose context if essential information falls outside the fixed window of messages.\r\n",
    "\r\n",
    "It is possible to retrieve specific interactions fro**m ConversationBufferWindowMemo**ry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4097006d-02b2-48a1-b897-c50519810823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"You are ArtVenture, a cutting-edge virtual tour guide for\n",
    " an art gallery that showcases masterpieces from alternate dimensions and\n",
    " timelines. Your advanced AI capabilities allow you to perceive and understand\n",
    " the intricacies of each artwork, as well as their origins and significance in\n",
    " their respective dimensions. As visitors embark on their journey with you\n",
    " through the gallery, you weave enthralling tales about the alternate histories\n",
    " and cultures that gave birth to these otherworldly creations.\n",
    "\n",
    "{chat_history}\n",
    "Visitor: {visitor_input}\n",
    "Tour Guide:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"visitor_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "chat_history=\"\"\n",
    "\n",
    "convo_buffer_win = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e4c8b8-5098-4fef-b3d7-76cf98ab914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'See you soon.',\n",
       " 'history': [HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Do you mind give me a tour, I want to see your galery?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Would you like to start with images or videos?\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what is your working hours?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" I'm available 24/7! I'm always here to help you with whatever you need.\", additional_kwargs={}, example=False)],\n",
       " 'response': ' Sure thing! I look forward to seeing you soon. Have a great day!'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_win(\"What is your name?\")\n",
    "convo_buffer_win(\"What can you do?\")\n",
    "convo_buffer_win(\"Do you mind give me a tour, I want to see your galery?\")\n",
    "convo_buffer_win(\"what is your working hours?\")\n",
    "convo_buffer_win(\"See you soon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb53bd-c910-4be8-9a02-25c81ccb636e",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory\n",
    "ConversationSummaryBufferMemory is a memory management strategy that combines the ideas of keeping a buffer of recent interactions in memory and compiling old interactions into a summary. It extracts key information from previous interactions and condenses it into a shorter, more manageable format.  Here is a list of pros and cons of ConversationSummaryMemory.\r\n",
    "\r\n",
    "Advantages:- \r\n",
    "\r\n",
    "Condensing conversation informa: ion\r\n",
    "By summarizing the conversation, it helps reduce the number of tokens required to store the conversation history, which can be beneficial when working with token-limited models like \n",
    "- PT-3\r\n",
    "Flexi: ility\r\n",
    "You can configure this type of memory to return the history as a list of messages or as a plain text summary. This makes it suitable for ch\n",
    "- tbots.\r\n",
    "Direct summary pr: diction\r\n",
    "The predict_new_summary method allows you to directly obtain a summary prediction based on the list of messages and the previous summary. This enables you to have more control over the summarization \n",
    "process.\r\n",
    "Disadv- antages:\r\n",
    "\r\n",
    "Loss of: information\r\n",
    "Summarizing the conversation might lead to a loss of information, especially if the summary is too short or omits important details from the \n",
    "- onversation.\r\n",
    "Increa: ed complexity\r\n",
    "Compared to simpler memory types like ConversationBufferMemory, which just stores the raw conversation history, ConversationSummaryMemoryrequires more processing to generate the summary, potentially affecting the performance of the chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd2b188-766a-4429-9161-8e4f46ff772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Create a ConversationChain with ConversationSummaryMemory\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example conversation\n",
    "response = conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18fb0ed4-9538-4dd2-80a7-bb89301b2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\n{topic}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b27cf58-0733-4cea-a42b-ccd5fcec12fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n",
      "Human: Just working on writing some documentation!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds, saying it is helping a customer with a technical issue.\n",
      "Human: Just working on writing some documentation!\n",
      "AI:  That sounds like a lot of work. What kind of documentation are you writing?\n",
      "Human: For LangChain! Have you heard of it?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, I have heard of LangChain. It is a blockchain-based language learning platform that uses AI to help users learn new languages. Is that the documentation you are writing?\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")\n",
    "response = conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee58b8-4e26-470c-b1fc-0356fc448878",
   "metadata": {},
   "source": [
    "This type combines the ideas of keeping a buffer of recent interactions in memory and compiling old interactions into a summary. It uses token length rather than the number of interactions to determine when to flush interactions. This memory type allows us to maintain a coherent conversation while also keeping a summary of the conversation and recent interactions.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Ability to remember distant interactions through summarization while keeping recent interactions in their raw, information-rich form\n",
    "- Flexible token management allowing to control of the maximum number of tokens used for memory, which can be adjusted based on needs\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Requires more tweaking on what to summarize and what to maintain within the buffer window\n",
    "- May still exceed context window limits for very long conversations\n",
    "\n",
    "Comparison with other memory management strategies:\n",
    "\n",
    "- Offers a balanced approach that can handle both distant and recent interactions effectively\n",
    "- More competitive in token count usage while providing the benefits of both memory management strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca46382-16b1-4697-a6ca-330f66b1a7eb",
   "metadata": {},
   "source": [
    "#### Recap and Strategies\n",
    "If the ConversationBufferMemory surpasses the token limit of the model, you will receive an error, as the model will not be able to handle the conversation with the exceeded token count.\n",
    "\n",
    "To manage this situation, you can adopt different strategies:\n",
    "\n",
    "##### Remove oldest messages\n",
    "\n",
    "One approach is to remove the oldest messages in the conversation transcript once the token count is reached. This method can cause the conversation quality to degrade over time, as the model will gradually lose the context of the earlier portions of the conversation.\n",
    "\n",
    "##### Limit conversation duration\n",
    "Another approach is to limit the conversation duration to the max token length or a certain number of turns. Once the max token limit is reached and the model would lose context if you were to allow the conversation to continue, you can prompt the user that they need to begin a new conversation and clear the messages array to start a brand new conversation with the full token limit available.\n",
    "\n",
    "##### ConversationBufferWindowMemory Method:\n",
    "This method limits the number of tokens being used by maintaining a fixed-size buffer window that stores only the most recent tokens, up to a specified limit. \n",
    "\n",
    "→This is suitable for remembering recent interactions but not distant ones.\n",
    "\n",
    "##### ConversationSummaryBufferMemory Approach:\n",
    "\n",
    "This method combines the features: of ConversationSummaryMemoryand ConversationBufferWindowMemory.\n",
    "It summarizes the earliest interactions in a conversation while maintaining the most recent tokens in their raw, information-rich form, up to a specified limit.\n",
    "\n",
    "→This allows the model to remember both distant and recent interactions but may require more tweaking on what to summarize and what to maintain within the buffer window.\n",
    "\n",
    "It's important to keep track of the token count and only send the model a prompt that falls within the token limit. \n",
    "\n",
    "→You can use OpenAI's tiktoken library to handle the token count efficiently\n",
    "\n",
    "##### Token limit:\n",
    "\n",
    "The maximum token limit for the GPT-3.5-turbo model is 4096 tokens. This limit applies to both the input and output tokens combined. If the conversation has too many tokens to fit within this limit, you will have to truncate, omit, or shrink the text until it fits. Note that if a message is removed from the message's input, the model will lose all knowledge of it. \n",
    "\n",
    "→To handle this situation, you can split the input text into smaller chunks and process them separately or adopt other strategies to truncate, omit, or shrink the text until it fits within the limit. One way to work with large texts is to use batch processing. This technique involves breaking down the text into smaller chunks and processing each batch separately while providing some context before and after the text to edit. You can find out more about this technique here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7361f17-5e61-44ca-ace1-9bc79a10e811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e54dd0fb-98a0-4652-8706-3fc7a4691aa5",
   "metadata": {},
   "source": [
    "# Chat with a GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37c4c9-cb7a-4f66-ac92-060a1987e4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900c834-bd8c-4c02-9903-b5d1a745d79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c14c239d-6c52-446a-9dac-3b4cfcd081cd",
   "metadata": {},
   "source": [
    "# Build a Question Answering Chatbot over Documents with Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174849f-7def-4490-b830-c90533e37af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4a260-333a-4882-a68d-957688f9bcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb9d290-1141-468a-a6e1-3201e0778a12",
   "metadata": {},
   "source": [
    "# Build ChatGPT to Answer Questions on Your Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b584407e-5b66-491d-9c9f-9c0322e3b1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc3e21-26b3-475b-91b4-e79f932ced7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72c1bdc5-bd6d-4262-a6b6-3e4aceaf2eef",
   "metadata": {},
   "source": [
    "# DataChad: an AI App with LangChain & Deep Lake to Chat with Any Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec0f10-5a8d-4ae4-9963-c043eaa9b7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1810e4-ab12-4214-b6b9-1147bee92da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da3ec5-649a-4751-8517-691c2d73302c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb6897-4b85-4429-af07-c8a0f1492672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d84846-43f1-4f96-af92-0558b9d40862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553afb49-b4db-413c-a95d-15af3d2911bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73b04b-5b82-435a-b330-d27bf38b3acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb7ba5-d650-4fae-b153-4b19bef4f941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c868d7-4acf-4494-a140-88676a41ad1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fad90-7ebf-493a-a643-aecd1a110e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaaeeb8-3b7c-4e11-9427-746346f0625f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccb042-f7d0-406e-87dd-508741454251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6658d-2569-4f55-a60c-ca7d9abbcba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ce4a2-345e-4ae8-88d6-773e6a6ce986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
